<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>深层神经网络编程作业2</title>
      <link href="/2020/02/13/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A2/"/>
      <url>/2020/02/13/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A2/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>深层神经网络编程作业1</title>
      <link href="/2020/02/13/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A1/"/>
      <url>/2020/02/13/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A1/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><h1 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v4 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>)</span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"> </span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="浅层神经网络参数初始化"><a href="#浅层神经网络参数初始化" class="headerlink" title="浅层神经网络参数初始化"></a>浅层神经网络参数初始化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                 <span class="string">"b1"</span>: b1,</span><br><span class="line">                 <span class="string">"W2"</span>: W2,</span><br><span class="line">                 <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="L层神经网络参数初始化"><a href="#L层神经网络参数初始化" class="headerlink" title="L层神经网络参数初始化"></a>L层神经网络参数初始化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L): <span class="comment">#从1到L-1，不包括L</span></span><br><span class="line">        parameters[<span class="string">'W'</span>+str(l)] = np.random.randn(layer_dims[l], layer_dims[l<span class="number">-1</span>])*<span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span>+str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span>+str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span>+str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="实现一个单层正向传播"><a href="#实现一个单层正向传播" class="headerlink" title="实现一个单层正向传播"></a>实现一个单层正向传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></span><br><span class="line">    Z = np.dot(W, A) +b</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><h1 id="对一层激活并向前传播（activation-cache存Z，linear-cache存A、W、b）"><a href="#对一层激活并向前传播（activation-cache存Z，linear-cache存A、W、b）" class="headerlink" title="对一层激活并向前传播（activation_cache存Z，linear_cache存A、W、b）"></a>对一层激活并向前传播（activation_cache存Z，linear_cache存A、W、b）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> activation==<span class="string">"sigmoid"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">    <span class="keyword">elif</span> activation==<span class="string">"relu"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><h1 id="L层正向传播"><a href="#L层正向传播" class="headerlink" title="L层正向传播"></a>L层正向传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters)//<span class="number">2</span> <span class="comment">#整数除法，返回一个不大于结果的整数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L): <span class="comment">#前L-1层都用relu，只有最后一层用sigmoid</span></span><br><span class="line">        A_prev = A</span><br><span class="line">        A,cache = linear_activation_forward(A_prev, parameters[<span class="string">'W'</span>+str(l)], parameters[<span class="string">'b'</span>+str(l)], <span class="string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">'W'</span>+str(L)], parameters[<span class="string">'b'</span>+str(L)], <span class="string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    cost = <span class="number">-1</span>/m*np.sum(Y*np.log(AL)+(<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-AL))</span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200213155632392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/2020021315582530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="单层向后传播"><a href="#单层向后传播" class="headerlink" title="单层向后传播"></a>单层向后传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    dW = <span class="number">1</span>/m * np.dot(dZ, A_prev.T)</span><br><span class="line">    db = <span class="number">1</span>/m * np.sum(dZ, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.shape == b.shape)</span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><h1 id="对单层激活并向后传播"><a href="#对单层激活并向后传播" class="headerlink" title="对单层激活并向后传播"></a>对单层激活并向后传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation==<span class="string">"relu"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    <span class="keyword">elif</span> activation==<span class="string">"sigmoid"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200213160009273.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="L层向后传播"><a href="#L层向后传播" class="headerlink" title="L层向后传播"></a>L层向后传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    grads=&#123;&#125;</span><br><span class="line">    L = len(caches)</span><br><span class="line">    m = AL.shape[<span class="number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape)</span><br><span class="line">    </span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</span><br><span class="line">    </span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]</span><br><span class="line">    grads[<span class="string">"dA"</span>+str(L<span class="number">-1</span>)], grads[<span class="string">"dW"</span>+str(L)], grads[<span class="string">"db"</span>+str(L)] = linear_activation_backward(dAL, current_cache, <span class="string">"sigmoid"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</span><br><span class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class="line">        <span class="comment"># Inputs: "grads["dA" + str(l + 1)], current_cache". Outputs: "grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)], current_cache, <span class="string">"relu"</span>)</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><h1 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    L = len(parameters) // <span class="number">2</span>  <span class="comment"># 神经网络的层数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L): <span class="comment">#从0到L-1</span></span><br><span class="line">        parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)]-learning_rate*grads[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)]-learning_rate*grads[<span class="string">"db"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"W1 = "</span>+ str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b1 = "</span>+ str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"W2 = "</span>+ str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b2 = "</span>+ str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008] [-1.76569676 -0.80627147  0.51115557 -1.18258802] [-1.0535704  -0.86128581  0.68284052  2.20374577]]b1 = [[-0.04659241] [-1.28888275] [ 0.53405496]]W2 = [[-0.55569196  0.0354055   1.32964895]]b2 = [[-0.84610769]]</code></pre>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅层神经网络编程作业</title>
      <link href="/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><h1 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X, Y = load_planar_dataset()</span><br></pre></td></tr></table></figure><h1 id="观察数据"><a href="#观察数据" class="headerlink" title="观察数据"></a>观察数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(Y), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">    关于np.squeeze()</span></span><br><span class="line"><span class="string">    删除Y中为空的维度</span></span><br><span class="line"><span class="string">    例如：</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; a = e.reshape(1,1,10)</span></span><br><span class="line"><span class="string">    array([[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; np.squeeze(a)</span></span><br><span class="line"><span class="string">    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><pre><code>&apos;\n    关于np.squeeze()\n    删除Y中为空的维度\n    例如：\n    &gt;&gt;&gt; a = e.reshape(1,1,10)\n    array([[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]])\n    &gt;&gt;&gt; np.squeeze(a)\n    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n&apos;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200211224816770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="神经网络每层神经元数"><a href="#神经网络每层神经元数" class="headerlink" title="神经网络每层神经元数"></a>神经网络每层神经元数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment">#输入层</span></span><br><span class="line">    n_h = <span class="number">4</span> <span class="comment">#隐藏层</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment">#输出层</span></span><br><span class="line">    <span class="keyword">return</span>(n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><h1 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros(shape=(n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros(shape=(n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                 <span class="string">"b1"</span>: b1,</span><br><span class="line">                 <span class="string">"W2"</span>: W2,</span><br><span class="line">                 <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="向前传播"><a href="#向前传播" class="headerlink" title="向前传播"></a>向前传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    Z1 = np.dot(W1,X)+b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2,A1)+b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">            <span class="string">"A1"</span>: A1,</span><br><span class="line">            <span class="string">"Z2"</span>: Z2,</span><br><span class="line">            <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    <span class="keyword">return</span> A2,cache</span><br></pre></td></tr></table></figure><h1 id="计算损失函数"><a href="#计算损失函数" class="headerlink" title="计算损失函数"></a>计算损失函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    cost = <span class="number">-1</span>/m * np.sum(Y*np.log(A2)+(<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A2))</span><br><span class="line">    </span><br><span class="line">    cost = float(np.squeeze(cost))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost, float))</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h1 id="向后传播"><a href="#向后传播" class="headerlink" title="向后传播"></a>向后传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">    </span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) <span class="comment">#按行相加并保持二维特性</span></span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2) *(<span class="number">1</span>-np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">            <span class="string">"db1"</span>: db1,</span><br><span class="line">            <span class="string">"dW2"</span>: dW2,</span><br><span class="line">            <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><h1 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line">    </span><br><span class="line">    W1 = W1-learning_rate*dW1</span><br><span class="line">    b1 = b1-learning_rate*db1</span><br><span class="line">    W2 = W2-learning_rate*dW2</span><br><span class="line">    b2 = b2-learning_rate*db2</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="构造神经网络模型"><a href="#构造神经网络模型" class="headerlink" title="构造神经网络模型"></a>构造神经网络模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=<span class="number">0.5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = np.round(A2)</span><br><span class="line">    <span class="comment">#np.round()函数的作用：对给定的数组进行四舍五入，可以指定精度</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><h1 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画边界</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d'</span> % float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.693048Cost after iteration 1000: 0.309802Cost after iteration 2000: 0.292433Cost after iteration 3000: 0.283349Cost after iteration 4000: 0.276781Cost after iteration 5000: 0.263472Cost after iteration 6000: 0.242044Cost after iteration 7000: 0.235525Cost after iteration 8000: 0.231410Cost after iteration 9000: 0.228464Accuracy: 90%</code></pre><p><img src="https://img-blog.csdnimg.cn/20200211224850147.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="隐藏层数量大小的影响"><a href="#隐藏层数量大小的影响" class="headerlink" title="隐藏层数量大小的影响"></a>隐藏层数量大小的影响</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]</span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>)</span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</span><br></pre></td></tr></table></figure><pre><code>Accuracy for 1 hidden units: 67.25 %Accuracy for 2 hidden units: 66.5 %Accuracy for 3 hidden units: 89.25 %Accuracy for 4 hidden units: 90.0 %Accuracy for 5 hidden units: 89.75 %Accuracy for 20 hidden units: 90.0 %Accuracy for 50 hidden units: 89.75 %</code></pre><p><img src="https://img-blog.csdnimg.cn/20200211224940314.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""><br><img src="https://img-blog.csdnimg.cn/20200211225018244.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="测试其他数据集"><a href="#测试其他数据集" class="headerlink" title="测试其他数据集"></a>测试其他数据集</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Datasets</span></span><br><span class="line">noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()</span><br><span class="line"></span><br><span class="line">datasets = &#123;<span class="string">"noisy_circles"</span>: noisy_circles,</span><br><span class="line">            <span class="string">"noisy_moons"</span>: noisy_moons,</span><br><span class="line">            <span class="string">"blobs"</span>: blobs,</span><br><span class="line">            <span class="string">"gaussian_quantiles"</span>: gaussian_quantiles&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE ### (choose your dataset)</span></span><br><span class="line">dataset = <span class="string">"noisy_moons"</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">X, Y = datasets[dataset]</span><br><span class="line">X, Y = X.T, Y.reshape(<span class="number">1</span>, Y.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># make blobs binary</span></span><br><span class="line"><span class="keyword">if</span> dataset == <span class="string">"blobs"</span>:</span><br><span class="line">    Y = Y%<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the data</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(Y), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200211225052519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200211225220649.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""><br><img src="https://img-blog.csdnimg.cn/20200211225246980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="">)<img src="https://img-blog.csdnimg.cn/20200211225306910.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="">)<img src="https://img-blog.csdnimg.cn/20200211225325983.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络基础编程作业</title>
      <link href="/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><h1 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = test_set_x_orig.shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h1 id="转换数据的形状，转换为：每一列代表一个图片的RGB，行代表所有的图片"><a href="#转换数据的形状，转换为：每一列代表一个图片的RGB，行代表所有的图片" class="headerlink" title="转换数据的形状，转换为：每一列代表一个图片的RGB，行代表所有的图片"></a>转换数据的形状，转换为：每一列代表一个图片的RGB，行代表所有的图片</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">print(train_set_x_flatten.shape)</span><br><span class="line">print(test_set_x_flatten.shape)</span><br></pre></td></tr></table></figure><pre><code>(12288, 209)(12288, 50)</code></pre><h1 id="将数据集标准化（颜色值都是0-255，要把颜色值转化成0-1）"><a href="#将数据集标准化（颜色值都是0-255，要把颜色值转化成0-1）" class="headerlink" title="将数据集标准化（颜色值都是0-255，要把颜色值转化成0-1）"></a>将数据集标准化（颜色值都是0-255，要把颜色值转化成0-1）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_set_x = train_set_x_flatten/<span class="number">255</span></span><br><span class="line">test_set_x = test_set_x_flatten/<span class="number">255</span></span><br></pre></td></tr></table></figure><h1 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="sigmoid 函数"></a>sigmoid 函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><h1 id="初始化w、b-这里是logistics-regression"><a href="#初始化w、b-这里是logistics-regression" class="headerlink" title="初始化w、b(这里是logistics regression)"></a>初始化w、b(这里是logistics regression)</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    w = np.zeros(shape=(dim,<span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>)) <span class="comment">#断言确保维度正确</span></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> w,b</span><br></pre></td></tr></table></figure><h1 id="向前向后传播"><a href="#向前向后传播" class="headerlink" title="向前向后传播"></a>向前向后传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="comment">#向前传播</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>] <span class="comment">#样本个数</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X)+b)</span><br><span class="line">    cost = (<span class="number">-1</span>/m)*np.sum(Y*np.log(A)+(<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#向后传播</span></span><br><span class="line">    dw = <span class="number">1</span>/m * np.dot(X , (A - Y).T)</span><br><span class="line">    db = <span class="number">1</span>/m * np.sum(A - Y)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,  <span class="comment">#将dw、db存入字典</span></span><br><span class="line">            <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads,cost</span><br></pre></td></tr></table></figure><h1 id="优化w、b"><a href="#优化w、b" class="headerlink" title="优化w、b"></a>优化w、b</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    num_iterations  - 优化循环的迭代次数</span></span><br><span class="line"><span class="string">        learning_rate  - 梯度下降更新规则的学习率</span></span><br><span class="line"><span class="string">        print_cost  - 每100步打印一次损失值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    costs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        grads, cost = propagate(w,b,X,Y)</span><br><span class="line">        </span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line">        </span><br><span class="line">        w = w-learning_rate*dw</span><br><span class="line">        b = b-learning_rate*db</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">             <span class="string">"b"</span>: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">            <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> params,grads,costs</span><br></pre></td></tr></table></figure><h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m)) <span class="comment">#每一列对应一张图片</span></span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>) <span class="comment">#X.shape[0]代表一章图片的每一个颜色值，要和w权值相乘</span></span><br><span class="line">    </span><br><span class="line">    A = sigmoid(np.dot(w.T, X)+b) <span class="comment">#激活函数将值对应在0-1（logistics regression）</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span>(A[<span class="number">0</span>,i]&gt;<span class="number">0.5</span>):</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i]=<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i]=<span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line">    <span class="keyword">return</span> Y_prediction,A</span><br></pre></td></tr></table></figure><h1 id="制作模型"><a href="#制作模型" class="headerlink" title="制作模型"></a>制作模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    num_iterations  - 优化循环的迭代次数</span></span><br><span class="line"><span class="string">        learning_rate  - 梯度下降更新规则的学习率</span></span><br><span class="line"><span class="string">        print_cost  - 每100步打印一次损失值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line">    </span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    Y_prediction_test, A_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train, A_train = predict(w, b, X_train)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#打印错误预测</span></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line">    </span><br><span class="line">    d = &#123;<span class="string">"costs"</span>: costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </span><br><span class="line">         <span class="string">"w"</span> : w, </span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.004</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.693147Cost after iteration 100: 0.506765Cost after iteration 200: 0.442269Cost after iteration 300: 0.397201Cost after iteration 400: 0.362439Cost after iteration 500: 0.334271Cost after iteration 600: 0.310725Cost after iteration 700: 0.290608Cost after iteration 800: 0.273138Cost after iteration 900: 0.257771Cost after iteration 1000: 0.244114Cost after iteration 1100: 0.231873Cost after iteration 1200: 0.220823Cost after iteration 1300: 0.210787Cost after iteration 1400: 0.201623Cost after iteration 1500: 0.193217Cost after iteration 1600: 0.185474Cost after iteration 1700: 0.178317Cost after iteration 1800: 0.171678Cost after iteration 1900: 0.165504train accuracy: 98.08612440191388 %test accuracy: 70.0 %</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 学习速率对损失函数影响的曲线</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>])</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200211174752993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="测试自己的图片"><a href="#测试自己的图片" class="headerlink" title="测试自己的图片"></a>测试自己的图片</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">my_image = <span class="string">"cat2.jpg"</span></span><br><span class="line"></span><br><span class="line">fname = <span class="string">"C:\\Users\\董润泽\\Desktop\\cat_picture\\"</span> + my_image</span><br><span class="line">image = np.array(ndimage.imread(fname, flatten=<span class="literal">False</span>))</span><br><span class="line">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((<span class="number">1</span>, num_px*num_px*<span class="number">3</span>)).T</span><br><span class="line">my_predicted_image,A = predict(d[<span class="string">"w"</span>], d[<span class="string">"b"</span>], my_image)</span><br><span class="line"></span><br><span class="line">plt.imshow(image)</span><br><span class="line">print(<span class="string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="string">", your algorithm predicts a \""</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br><span class="line">print(A[<span class="number">0</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>y = 1.0, your algorithm predicts a &quot;cat&quot; picture.1.0C:\ProgramData\Anaconda3\lib\site-packages\ipykernel_launcher.py:4: DeprecationWarning: `imread` is deprecated!`imread` is deprecated in SciPy 1.0.0.Use ``matplotlib.pyplot.imread`` instead.  after removing the cwd from sys.path.C:\ProgramData\Anaconda3\lib\site-packages\ipykernel_launcher.py:5: DeprecationWarning: `imresize` is deprecated!`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.  &quot;&quot;&quot;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200211174807684.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>03深层神经网络</title>
      <link href="/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/29738823" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29738823</a></p><p>深层神经网络模型：<br><img src="https://img-blog.csdnimg.cn/20200212111650297.png" alt=""></p><p>看完前面的两个部分，这个深层神经网络就比较简单，原理相同。</p><h1 id="1、参数矩阵维度"><a href="#1、参数矩阵维度" class="headerlink" title="1、参数矩阵维度"></a><strong>1、参数矩阵维度</strong></h1><p><img src="https://img-blog.csdnimg.cn/20200212111930249.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="2、前向和反向传播"><a href="#2、前向和反向传播" class="headerlink" title="2、前向和反向传播"></a><strong>2、前向和反向传播</strong></h1><p>给定参数：</p><p><img src="https://img-blog.csdnimg.cn/20200212112503726.png" alt=""></p><p><strong>- 前向传播</strong></p><p><img src="https://img-blog.csdnimg.cn/20200212112614553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><strong>- 反向传播</strong></p><p><img src="https://img-blog.csdnimg.cn/2020021211271436.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200212112740739.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="3、为什么要用深层表示"><a href="#3、为什么要用深层表示" class="headerlink" title="3、为什么要用深层表示"></a><strong>3、为什么要用深层表示</strong></h1><p><img src="https://pic2.zhimg.com/v2-07b30fc02b2b2decaa66edcbff69f6b5_r.jpg" alt=""></p><p>对于人脸识别，神经网络的第一层从原始图片中提取人脸的轮廓和边缘，每个神经元学习到不同边缘的信息；网络的第二层将第一层学得的边缘信息组合起来，形成人脸的一些局部的特征，例如眼睛、嘴巴等；后面的几层逐步将上一层的特征组合起来，形成人脸的模样。随着神经网络层数的增加，特征也从原来的边缘逐步扩展为人脸的整体，由整体到局部，由简单到复杂。层数越多，那么模型学习的效果也就越精确。</p><p>对于语音识别，第一层神经网络可以学习到语言发音的一些音调，后面更深层次的网络可以检测到基本的音素，再到单词信息，逐渐加深可以学到短语、句子。</p><p>所以从上面的两个例子可以看出随着神经网络的深度加深，模型能学习到更加复杂的问题，功能也更加强大。</p><p><strong>对于逻辑电路</strong><br><img src="https://pic1.zhimg.com/80/v2-d912f368562a4abdb0cf7b69589fdc64_hd.jpg" alt=""></p><p>假定计算异或逻辑输出：</p><p><img src="https://www.zhihu.com/equation?tex=y%3Dx_%7B1%7D%5Coplus+x_%7B2%7D%5Coplus+x_%7B3%7D%5Coplus+%5Ccdots%5Coplus+x_%7Bn%7D" alt=""></p><p>对于该运算，若果使用深度神经网络，每层将前一层的相邻的两单元进行异或，最后到一个输出，此时整个网络的层数为一个树形的形状，网络的深度为 [公式] ，共使用的神经元的个数为：</p><p><img src="https://www.zhihu.com/equation?tex=1%2B2%2B%5Ccdot%2B2%5E%7B%5Clog_%7B2%7D%28n%29-1%7D%3D1%5Ccdot+%5Cdfrac%7B1-2%5E%7B%5Clog_%7B2%7D%28n%29%7D%7D%7B1-2%7D%3D2%5E%7B%5Clog_%7B2%7D%28n%29%7D-1%3Dn-1" alt=""></p><p>即输入个数为n，输出个数为n-1。</p><p>但是如果不适用深层网络，仅仅使用单隐层的网络（如右图所示），需要的神经元个数为 <img src="https://www.zhihu.com/equation?tex=2%5E%7Bn-1%7D" alt=""> 个 。同样的问题，但是深层网络要比浅层网络所需要的神经元个数要少得多。</p><h1 id="4、参数和超参数"><a href="#4、参数和超参数" class="headerlink" title="4、参数和超参数"></a><strong>4、参数和超参数</strong></h1><p><img src="https://img-blog.csdnimg.cn/20200212113727546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>02浅层神经网络</title>
      <link href="/2020/02/11/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/02/11/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/29706138" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29706138</a></p><h1 id="1、神经网络表示"><a href="#1、神经网络表示" class="headerlink" title="1、神经网络表示"></a><strong>1、神经网络表示</strong></h1><p><img src="https://img-blog.csdnimg.cn/2020021109545357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>看着这张浅层神经网络的示意图</p><p>第一列为输入层，x1,x2,x3；第二列为隐藏层，第三列为输出层。我们把这个神经网络成为两层神经网络（输入层不计）</p><p>我们要注意的是每两层之间的计算和每一层的参数矩阵大小</p><p><img src="https://pic2.zhimg.com/80/v2-57166f7b5bb26904e62433ffffd01e61_hd.jpg" alt=""></p><ul><li><p>输入层和隐藏层</p><ul><li>w–&gt;(4,3),4代表隐藏层神经元个数，3代表输入层神经元个数</li><li>b–&gt;(4,1)，4指隐藏层神经元数</li></ul></li><li><p>隐藏层和输出层</p><ul><li>w–&gt;(1,4),1指输出层，4指隐藏层</li><li>b–&gt;(1,1),1指输出层</li></ul></li></ul><p>在神经网络中，前一层是输入，后一层作为输出，两层之间，w参数矩阵大小为<strong>（N[out], N[in]）</strong>, b参数矩阵大小为<strong>（N[out], 1）</strong>,这里 <strong>z = wX + b</strong>,神经网络中w[i] = w.T</p><p>在logistic regression 中，用<strong>（N[in], N[out]）</strong>表示参数矩阵大小，公式是 <strong>z = w.T***</strong>X + b**</p><h1 id="2、神经网络的输出"><a href="#2、神经网络的输出" class="headerlink" title="2、神经网络的输出"></a><strong>2、神经网络的输出</strong></h1><p><img src="https://pic4.zhimg.com/80/v2-722991018136c95bff18be8e2932d987_hd.jpg" alt=""></p><p>只需要四个公式来控制这个神经网络的输出</p><p><img src="https://pic3.zhimg.com/80/v2-6eb2bc22d2d7953ab4fb4115a2a33a96_hd.jpg" alt=""></p><h1 id="3、向量化"><a href="#3、向量化" class="headerlink" title="3、向量化"></a><strong>3、向量化</strong></h1><p>对于每一个神经元x，都需要计算上面的四个公式，如果用for循环，速度十分慢。所以向量化就是使用矩阵来代替for循环，提高效率</p><p><img src="https://pic1.zhimg.com/80/v2-1615f9d56a1561b3aef3bc4deb895110_hd.jpg" alt=""></p><p>对于m个样本，它的参数矩阵为： （n_x,m）</p><h1 id="4、激活函数"><a href="#4、激活函数" class="headerlink" title="4、激活函数"></a><strong>4、激活函数</strong></h1><p><img src="https://pic4.zhimg.com/80/v2-58168196b7618a2dfe6b2f02a714fabb_hd.jpg" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200211104536645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>sigmoid函数和tanh函数比较：</p><ul><li><p>隐藏层：tanh函数的表现要好于sigmoid函数，因为tanh取值范围为 【-1，1】 ，输出分布在0值的附近，均值为0，从隐藏层到输出层数据起到了归一化（均值为0）的效果。<br>输出层：对于二分类任务的输出取值为 【0，1】 ，故一般会选择sigmoid函数。<br>然而sigmoid和tanh函数在当 |Z| 很大的时候，梯度会很小，在依据梯度的算法中，更新在后期会变得很慢。在实际应用中，要使 |Z| 尽可能的落在0值附近。</p></li><li><p>ReLU弥补了前两者的缺陷，当 Z&gt;0 时，梯度始终为1，从而提高神经网络基于梯度算法的运算速度。然而当 Z&lt;0 时，梯度一直为0，但是实际的运用中，该缺陷的影响不是很大。</p></li><li><p>Leaky ReLU保证在 Z&lt;0 的时候，梯度仍然不为0。</p></li></ul><p>在选择激活函数的时候，如果在不知道该选什么的时候就选择ReLU，当然也没有固定答案，要依据实际问题在交叉验证集合中进行验证分析。</p><h1 id="5、神经网络的梯度下降"><a href="#5、神经网络的梯度下降" class="headerlink" title="5、神经网络的梯度下降"></a><strong>5、神经网络的梯度下降</strong></h1><p><img src="https://img-blog.csdnimg.cn/2020021110591783.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><img src="https://pic3.zhimg.com/80/v2-88e08b98b1d6ee1acba5f79a326fbb82_hd.jpg" alt=""></p><h1 id="6、随机初始化"><a href="#6、随机初始化" class="headerlink" title="6、随机初始化"></a><strong>6、随机初始化</strong></h1><p><img src="https://img-blog.csdnimg.cn/2020021111032085.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01神经网络基础</title>
      <link href="/2020/02/06/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
      <url>/2020/02/06/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/29688927" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29688927</a></p><h1 id="整体处理流程："><a href="#整体处理流程：" class="headerlink" title="整体处理流程："></a>整体处理流程：</h1><ul><li>初始化参数w、b</li><li>正向传播，计算损失函数J</li><li>反向传播，计算dw、db</li><li>梯度下降优化w、b</li><li>预测y</li></ul><h1 id="1-二分类问题"><a href="#1-二分类问题" class="headerlink" title="1.二分类问题"></a><strong>1.二分类问题</strong></h1><p>这一节主要讲了定义数据格式</p><p>例：</p><p>对于下面这张图片，把它看作64*64个像素点，那么对每个像素点，又可以分为红黄蓝三色：</p><p><img src="https://img-blog.csdnimg.cn/2020020610562129.png" alt=" "></p><p>我们可以得出需要的</p><p><img src="https://img-blog.csdnimg.cn/20200206110505637.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><p>而y=1/0 可以代表图片中有无猫</p><p><img src="https://img-blog.csdnimg.cn/20200206110730236.png" alt=" "></p><p><img src="https://img-blog.csdnimg.cn/20200206110931586.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><h1 id="2-logistic-regression"><a href="#2-logistic-regression" class="headerlink" title="2.logistic regression"></a><strong>2.logistic regression</strong></h1><p>上面的 Y^ = [0,1]表示的是一种概率。</p><p><img src="https://img-blog.csdnimg.cn/202002061618162.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><p>z趋于无穷大，y趋于1；z变小，y变小</p><p>控制z，即要获得w、b的值</p><p><img src="https://img-blog.csdnimg.cn/20200206162902528.png" alt=" "></p><p>符号惯例：θo 代表 b；其他的代表w（不同神经）</p><h1 id="3-logistic-regression损失函数-Loss-Function"><a href="#3-logistic-regression损失函数-Loss-Function" class="headerlink" title="3.logistic regression损失函数 Loss Function"></a><strong>3.logistic regression损失函数 Loss Function</strong></h1><p>一般经验来说是用平方错误衡量损失函数</p><p><img src="https://img-blog.csdnimg.cn/20200206171116302.png" alt=" "></p><p>但是，对于logistic regression 来说，一般不适用平方错误来作为Loss Function，这是因为上面的平方错误损失函数一般是非凸函数（non-convex），其在使用梯度下降算法的时候，容易得到局部最优解，而不是全局最优解。因此要选择凸函数。</p><p><img src="https://img-blog.csdnimg.cn/20200206171229963.png" alt=" "></p><p><img src="https://img-blog.csdnimg.cn/20200206171332982.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><h1 id="4-梯度下降法"><a href="#4-梯度下降法" class="headerlink" title="4.梯度下降法"></a><strong>4.梯度下降法</strong></h1><p>即以梯度下降的方式最小化代价函数cost function,最终得出w、b</p><p>每次迭代更新的修正表达式：</p><p><img src="https://img-blog.csdnimg.cn/20200206192504836.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><p>程序中分别用dw、db分别表示上面的偏导部分</p><h1 id="5-逻辑回归中的梯度下降法"><a href="#5-逻辑回归中的梯度下降法" class="headerlink" title="5.逻辑回归中的梯度下降法"></a><strong>5.逻辑回归中的梯度下降法</strong></h1><p><img src="https://img-blog.csdnimg.cn/20200207085458345.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><p>最后是通过梯度下降法中的修正表达式，得出w1、w2、b的值</p><h1 id="6-m个样本的梯度下降"><a href="#6-m个样本的梯度下降" class="headerlink" title="6.m个样本的梯度下降"></a><strong>6.m个样本的梯度下降</strong></h1><p><img src="https://img-blog.csdnimg.cn/20200207092426643.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><p>$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}$$</p><p>$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}$$</p><h1 id="7-向量化"><a href="#7-向量化" class="headerlink" title="7.向量化"></a><strong>7.向量化</strong></h1><p>向量化就是把原始的for循环用矩阵来代替，使用numpy中的矩阵运算直接算出来。</p><p>单次迭代梯度下降算法流程：</p><pre><code>Z = np.dot(w.T,X) + bA = sigmoid(Z)dZ = A-Ydw = 1/m*np.dot(X,dZ.T)db = 1/m*np.sum(dZ)w = w - alpha*dwb = b - alpha*db</code></pre><h1 id="8-关于numpy说明"><a href="#8-关于numpy说明" class="headerlink" title="8.关于numpy说明"></a><strong>8.关于numpy说明</strong></h1><p>当不确定矩阵的维度时，可以使用assert保证安全</p><pre><code>assert(a.shape == (5,1))</code></pre>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA静态代理和动态代理</title>
      <link href="/2020/02/04/JVM/JAVA%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"/>
      <url>/2020/02/04/JVM/JAVA%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>参考文章：<a href="https://www.jianshu.com/p/9bcac608c714" target="_blank" rel="noopener">https://www.jianshu.com/p/9bcac608c714</a></p><h1 id="代理模式"><a href="#代理模式" class="headerlink" title="代理模式"></a>代理模式</h1><p>为其他对象提供一个代理以控制对某个对象的访问。代理类主要负责为委托了（真实对象）预处理消息、过滤消息、传递消息给委托类，代理类不现实具体服务，而是利用委托类来完成服务，并将执行结果封装处理。</p><p>代理类为被代理类预处理消息、过滤消息并在此之后将消息转发给被代理类，之后还能进行消息的后置处理。代理类和被代理类通常会存在关联关系(即上面提到的持有的被代理对象的引用)，代理类本身不实现服务，而是通过调用被代理类中的方法来提供服务。</p><h1 id="静态代理"><a href="#静态代理" class="headerlink" title="静态代理"></a>静态代理</h1><p>静态代理的模式：首先创建一个接口，然后被代理类实现这个接口，再创建一个代理类，代理类中创建一个被代理类对象的引用，然后代理类中完成被代理类的方法。</p><p>有点绕，只要记住一点：代理类主要负责预处理消息、过滤消息、传递消息，所以方法要在代理类中进行加工和调用。</p><p>下面来看代码：</p><p>1、创建接口</p><pre><code>public interface HelloInterface {    void sayHello();}</code></pre><p>2、创建被代理类实现接口：</p><pre><code>public class Hello implements HelloInterface{    public void sayHello() {        System.out.println(&quot;Hello&quot;);    }}</code></pre><p>3、创建代理类，在该类中，实现被代理类的引用以及方法调用和加工</p><pre><code>public class HelloProxy implements HelloInterface{    private HelloInterface helloInterface = (HelloInterface) new Hello();    public void sayHello() {        System.out.println(&quot;before invok&quot;);        helloInterface.sayHello();        System.out.println(&quot;after invok&quot;);    }}</code></pre><p>4、通过代理类对象，对目标方法进行调用</p><pre><code>public static void main(String[] args) {        HelloProxy hello = new HelloProxy();    }</code></pre><p>静态代理存在缺点：每个代理类只能为一个类服务，如果要代理的类很多，就需要编写大量的代理类。</p><p>例如，再来一个Bye类：</p><p>1、接口：</p><pre><code>public interface ByeInterface {    void sayBye();}</code></pre><p>2、接口实现</p><pre><code>public class Bye implements ByeInterface{    public void sayBye() {        System.out.println(&quot;GoodBye!&quot;);    }}</code></pre><p>3、代理类中引用方法</p><p>注意，这时候就会发现，原来的那个Hello代理不可以代理这个，必须要新建一个此方法的专属代理</p><pre><code>public class ByeProxy implements ByeInterface{    private ByeInterface byeInterface = (ByeInterface) new Bye();    public void sayBye(){        System.out.println(&quot;before invok&quot;);        byeInterface.sayBye();        System.out.println(&quot;after invok&quot;);    }}</code></pre><p>4、main方法也要new一个新代理的对象</p><pre><code>ByeProxy bye = new ByeProxy();bye.sayBye();</code></pre><p>结果：</p><p><img src="https://img-blog.csdnimg.cn/20200204111758538.png" alt=" "></p><h1 id="动态代理"><a href="#动态代理" class="headerlink" title="动态代理"></a>动态代理</h1><p>前两步相同：1、创建接口 ， 2、接口实现</p><p>3、代理：</p><pre><code>public class ProxyHandler implements InvocationHandler{    private Object object;    public ProxyHandler(Object object){        this.object = object;    }    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {        System.out.println(&quot;Before invok &quot;+method.getName());        method.invoke(object,args);        System.out.println(&quot;after invok &quot;+method.getName());        return null;    }}</code></pre><p>构建一个handler类来实现InvocationHandler接口</p><p>4、main</p><pre><code>public static void main(String[] args) {        HelloInterface hello = new Hello();        ByeInterface bye = new Bye();        InvocationHandler handler = new ProxyHandler(hello);        InvocationHandler handler2 = new ProxyHandler(bye);        HelloInterface proxyHello = (HelloInterface) Proxy.newProxyInstance(hello.getClass().getClassLoader(), hello.getClass().getInterfaces(), handler);        ByeInterface proxyBye = (ByeInterface) Proxy.newProxyInstance(bye.getClass().getClassLoader(), bye.getClass().getInterfaces(), handler2);        proxyHello.sayHello();        proxyBye.sayBye();    }</code></pre><p>结果：</p><p><img src="https://img-blog.csdnimg.cn/20200204113509359.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><p>动态代理的代理不需要新建。</p><p><img src="https://img-blog.csdnimg.cn/20200204141934502.png" alt=" "><br><img src="https://img-blog.csdnimg.cn/20200204141955282.png" alt=" "></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 静态、动态代理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pandas</title>
      <link href="/2020/02/01/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/pandas/"/>
      <url>/2020/02/01/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/pandas/</url>
      
        <content type="html"><![CDATA[<h1 id="数据结构简介"><a href="#数据结构简介" class="headerlink" title="数据结构简介"></a>数据结构简介</h1><h2 id="1、Series"><a href="#1、Series" class="headerlink" title="1、Series"></a>1、Series</h2><pre><code>import numpy as np,pandas as pdarr1 = np.arange(10)print(arr1)print(type(arr1))</code></pre><h3 id="通过一维数组创建"><a href="#通过一维数组创建" class="headerlink" title="- 通过一维数组创建"></a>- 通过一维数组创建</h3><pre><code>s1 = pd.Series(arr1)print(s1)print(type(s1))</code></pre><p>结果：</p><pre><code>0    01    12    23    34    45    56    67    78    89    9</code></pre><h3 id="通过字典创建"><a href="#通过字典创建" class="headerlink" title="- 通过字典创建"></a>- 通过字典创建</h3><pre><code>dic = {&apos;a&apos;:10,&apos;b&apos;:20,&apos;c&apos;:30,&apos;d&apos;:40}s2 = pd.Series(dic)print(s2)</code></pre><p>结果：</p><pre><code>a    10b    20c    30d    40</code></pre><h2 id="2、DataFrame"><a href="#2、DataFrame" class="headerlink" title="2、DataFrame"></a>2、DataFrame</h2><h3 id="二维数组创建数据框"><a href="#二维数组创建数据框" class="headerlink" title="- 二维数组创建数据框"></a>- 二维数组创建数据框</h3><pre><code>arr2 = np.array(np.arange(12)).reshape(4,3)print(arr2)df1 = pd.DataFrame(arr2)print(df1)</code></pre><p>结果：</p><pre><code>   0   1   20  0   1   21  3   4   52  6   7   83  9  10  11</code></pre><h3 id="字典创建数据框"><a href="#字典创建数据框" class="headerlink" title="- 字典创建数据框"></a>- 字典创建数据框</h3><pre><code>dic2 = {&apos;a&apos;:[1,2,3,4],&apos;b&apos;:[5,6,7,8],&apos;c&apos;:[9,0,11,21]}print(dic2)df2 = pd.DataFrame(dic2)print(df2)dic3 = {&apos;one&apos;:{&apos;a&apos;:1,&apos;b&apos;:2,&apos;c&apos;:3,&apos;d&apos;:4},&apos;two&apos;:{&apos;a&apos;:5,&apos;b&apos;:6,&apos;c&apos;:7,&apos;d&apos;:8},&apos;three&apos;:{&apos;a&apos;:9,&apos;b&apos;:10,&apos;c&apos;:11,&apos;d&apos;:12}}df3 = pd.DataFrame(dic3)print(df3)</code></pre><p>结果：</p><pre><code>{&apos;a&apos;: [1, 2, 3, 4], &apos;b&apos;: [5, 6, 7, 8], &apos;c&apos;: [9, 0, 11, 21]}   a  b   c0  1  5   91  2  6   02  3  7  113  4  8  21   one  two  threea    1    5      9b    2    6     10c    3    7     11d    4    8     12</code></pre><h3 id="通过数据框的方式创建数据框"><a href="#通过数据框的方式创建数据框" class="headerlink" title="通过数据框的方式创建数据框"></a>通过数据框的方式创建数据框</h3><pre><code>df4 = df3[[&apos;one&apos;,&apos;two&apos;]]print(df4)</code></pre><p>结果：</p><pre><code>   one  twoa    1    5b    2    6c    3    7d    4    8</code></pre><p>#数据索引 index<br>##通过索引值或索引标签获取数据</p><pre><code>s4 = pd.Series(np.array(np.arange(5)))print(s4)print(s4.index)s4.index = [&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;]print(s4)print(s4[3],s4[&apos;a&apos;])</code></pre><p>结果：</p><pre><code>0    01    12    23    34    4dtype: int32RangeIndex(start=0, stop=5, step=1)a    0b    1c    2d    3e    4dtype: int323 0</code></pre><h2 id="自动化对齐"><a href="#自动化对齐" class="headerlink" title="自动化对齐"></a>自动化对齐</h2><pre><code>s5=pd.Series(np.array([1,2,3,21,12,325]),index=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;,&apos;f&apos;])print(s5)s6=pd.Series(np.array([1,2,3,21,12,325]),index=[&apos;b&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;,&apos;f&apos;])print(s5+s6)</code></pre><p>结果：</p><pre><code>a      1b      2c      3d     21e     12f    325dtype: int32a      NaNb      3.0b      4.0c      6.0d     42.0e     24.0f    650.0dtype: float64</code></pre><h1 id="利用pandas查询数据"><a href="#利用pandas查询数据" class="headerlink" title="利用pandas查询数据"></a>利用pandas查询数据</h1><p>导入数据<br>    import pandas as pd</p><pre><code>stu_dic = {&apos;Age&apos;:[14,13,13,14,14,12,12,15,13,12,11,14,12,15,16,12,15,11,15],&apos;Height&apos;:[69,56.5,65.3,62.8,63.5,57.3,59.8,62.5,62.5,59,51.3,64.3,56.3,66.5,72,64.8,67,57.5,66.5],&apos;Name&apos;:[&apos;Alfred&apos;,&apos;Alice&apos;,&apos;Barbara&apos;,&apos;Carol&apos;,&apos;Henry&apos;,&apos;James&apos;,&apos;Jane&apos;,&apos;Janet&apos;,&apos;Jeffrey&apos;,&apos;John&apos;,&apos;Joyce&apos;,&apos;Judy&apos;,&apos;Louise&apos;,&apos;Marry&apos;,&apos;Philip&apos;,&apos;Robert&apos;,&apos;Ronald&apos;,&apos;Thomas&apos;,&apos;Willam&apos;],&apos;Sex&apos;:[&apos;M&apos;,&apos;F&apos;,&apos;F&apos;,&apos;F&apos;,&apos;M&apos;,&apos;M&apos;,&apos;F&apos;,&apos;F&apos;,&apos;M&apos;,&apos;M&apos;,&apos;F&apos;,&apos;F&apos;,&apos;F&apos;,&apos;F&apos;,&apos;M&apos;,&apos;M&apos;,&apos;M&apos;,&apos;M&apos;,&apos;M&apos;],&apos;Weight&apos;:[112.5,84,98,102.5,102.5,83,84.5,112.5,84,99.5,50.5,90,77,112,150,128,133,85,112]}student = pd.DataFrame(stu_dic)</code></pre><p>-</p><pre><code>print(student.head())print(student.tail())print(student.loc[[1,3,4,5]])print(student[[&apos;Name&apos;]].head())print(student.loc[:3,[&apos;Name&apos;]])print(student[(student[&apos;Sex&apos;]==&apos;M&apos;) &amp; (student[&apos;Age&apos;]&gt;14)])print(student[(student[&apos;Sex&apos;]==&apos;M&apos;) &amp; (student[&apos;Age&apos;]&gt;14)][[&apos;Name&apos;]])</code></pre><p>结果：</p><pre><code>  Age  Height     Name Sex  Weight0   14    69.0   Alfred   M   112.51   13    56.5    Alice   F    84.02   13    65.3  Barbara   F    98.03   14    62.8    Carol   F   102.54   14    63.5    Henry   M   102.5    Age  Height    Name Sex  Weight14   16    72.0  Philip   M   150.015   12    64.8  Robert   M   128.016   15    67.0  Ronald   M   133.017   11    57.5  Thomas   M    85.018   15    66.5  Willam   M   112.0   Age  Height   Name Sex  Weight1   13    56.5  Alice   F    84.03   14    62.8  Carol   F   102.54   14    63.5  Henry   M   102.55   12    57.3  James   M    83.0      Name0   Alfred1    Alice2  Barbara3    Carol4    Henry      Name0   Alfred1    Alice2  Barbara3    Carol    Age  Height    Name Sex  Weight14   16    72.0  Philip   M   150.016   15    67.0  Ronald   M   133.018   15    66.5  Willam   M   112.0      Name14  Philip16  Ronald18  Willam</code></pre><h1 id="利用pandas的DataFrames进行统计分析"><a href="#利用pandas的DataFrames进行统计分析" class="headerlink" title="利用pandas的DataFrames进行统计分析"></a>利用pandas的DataFrames进行统计分析</h1><pre><code>np.random.seed(1234)d1 = pd.Series(2*np.random.normal(size = 100)+3)d2 = np.random.f(2,4,size = 100)d3 = np.random.randint(1,100,size = 100)print(d1)print(d2)print(d3)print(&apos;非空元素计算: &apos;, d1.count()) #非空元素计算print(&apos;最小值: &apos;, d1.min()) #最小值print(&apos;最大值: &apos;, d1.max()) #最大值print(&apos;最小值的位置: &apos;, d1.idxmin()) #最小值的位置，类似于R中的which.min函数print(&apos;最大值的位置: &apos;, d1.idxmax()) #最大值的位置，类似于R中的which.max函数print(&apos;10%分位数: &apos;, d1.quantile(0.1)) #10%分位数print(&apos;求和: &apos;, d1.sum()) #求和print(&apos;均值: &apos;, d1.mean()) #均值print(&apos;中位数: &apos;, d1.median()) #中位数print(&apos;众数: &apos;, d1.mode()) #众数print(&apos;方差: &apos;, d1.var()) #方差print(&apos;标准差: &apos;, d1.std()) #标准差print(&apos;平均绝对偏差: &apos;, d1.mad()) #平均绝对偏差print(&apos;偏度: &apos;, d1.skew()) #偏度print(&apos;峰度: &apos;, d1.kurt()) #峰度print(&apos;描述性统计指标: &apos;, d1.describe()) #一次性输出多个描述性统计指标</code></pre><p>结果：</p><pre><code>非空元素计算:  100最小值:  -4.1270333212494705最大值:  7.781921030926066最小值的位置:  81最大值的位置:  3910%分位数:  0.6870184644069928求和:  307.0224566250873均值:  3.070224566250874中位数:  3.204555266776845众数:  0    -4.1270331    -1.8009072    -1.4853703    -1.1499554    -1.0425105    -0.6340546    -0.0938117     0.1083808     0.1960539     0.61804910    0.69468211    0.71473712    0.86202213    0.94429914    1.05152715    1.14749116    1.20568617    1.42913018    1.55882319    1.68806120    1.72695321    1.83056422    1.86710823    1.90351524    1.97623725    2.06138926    2.13980927    2.20007128    2.20432029    2.310468        ...   70    4.13147771    4.26395972    4.35110873    4.40845674    4.40944175    4.51082876    4.53473777    4.63318878    4.68201879    4.68334980    4.71917781    4.72743582    4.77432683    4.90664884    4.96984085    4.98389286    5.06760187    5.09187788    5.09515789    5.11793890    5.30007191    5.63630392    5.64221193    5.64231694    5.78397295    5.86541496    6.09131897    7.01568698    7.06120799    7.781921Length: 100, dtype: float64方差:  4.005609378535085标准差:  2.0014018533355777平均绝对偏差:  1.5112880411556109偏度:  -0.6494780760484293峰度:  1.2201094052398012描述性统计指标:  count    100.000000mean       3.070225std        2.001402min       -4.12703325%        2.04010150%        3.20455575%        4.434788max        7.781921dtype: float64</code></pre><h1 id="利用pandas实现SQL操作"><a href="#利用pandas实现SQL操作" class="headerlink" title="利用pandas实现SQL操作"></a>利用pandas实现SQL操作</h1><pre><code>dic = {&apos;Name&apos;:[&apos;LiuShunxiang&apos;,&apos;Zhangshan&apos;],&apos;Sex&apos;:[&apos;M&apos;,&apos;F&apos;],&apos;Age&apos;:[27,23],&apos;Height&apos;:[165.7,167.2],&apos;Weight&apos;:[61,63]}student2 = pd.DataFrame(dic)print(student2)</code></pre><h2 id="增"><a href="#增" class="headerlink" title="增"></a>增</h2><pre><code>student3 = pd.concat([student,student2])print(student3)#新增一列print(pd.DataFrame(student2, columns=[&apos;Age&apos;,&apos;Height&apos;,&apos;Name&apos;,&apos;Sex&apos;,&apos;Weight&apos;,&apos;Score&apos;]))</code></pre><h2 id="删"><a href="#删" class="headerlink" title="删"></a>删</h2><pre><code>del(student3)print(student3)#删除指定行print(student.drop([0,3,5,1]))#删除指定列print(student.drop([&apos;Height&apos;,&apos;Weight&apos;],axis=1).head())</code></pre><p>#利用pandas进行缺失值的处理</p><pre><code>df = pd.DataFrame([[1,1,2],[3,5,np.nan],[13,21,34],[55,np.nan,10],[np.nan,np.nan,np.nan],[np.nan,1,2]],columns=(&apos;x1&apos;,&apos;x2&apos;,&apos;x3&apos;))print(df)#直接删除print(df.dropna())#用0填充print(df.fillna(0))########print(&quot;采用前倾填充、向后填充&quot;)##采用前倾填充、向后填充print(df.fillna(method=&apos;ffill&apos;))print(df.fillna(method=&apos;bfill&apos;))#######print(&quot;#用常量填充不同的列&quot;)#用常量填充不同的列x1_median=df[&apos;x1&apos;].median()x2_mean=df[&apos;x2&apos;].mean()x3_mean=df[&apos;x3&apos;].mean()print(x1_median)print(x2_mean)print(x3_mean)print(df.fillna({&apos;x1&apos;:x1_median,&apos;x2&apos;:x2_mean,&apos;x3&apos;:x3_mean}))</code></pre><p>结果：</p><pre><code>     x1    x2    x30   1.0   1.0   2.01   3.0   5.0   NaN2  13.0  21.0  34.03  55.0   NaN  10.04   NaN   NaN   NaN5   NaN   1.0   2.0     x1    x2    x30   1.0   1.0   2.02  13.0  21.0  34.0     x1    x2    x30   1.0   1.0   2.01   3.0   5.0   0.02  13.0  21.0  34.03  55.0   0.0  10.04   0.0   0.0   0.05   0.0   1.0   2.0采用前倾填充、向后填充     x1    x2    x30   1.0   1.0   2.01   3.0   5.0   2.02  13.0  21.0  34.03  55.0  21.0  10.04  55.0  21.0  10.05  55.0   1.0   2.0     x1    x2    x30   1.0   1.0   2.01   3.0   5.0  34.02  13.0  21.0  34.03  55.0   1.0  10.04   NaN   1.0   2.05   NaN   1.0   2.0#用常量填充不同的列8.07.012.0     x1    x2    x30   1.0   1.0   2.01   3.0   5.0  12.02  13.0  21.0  34.03  55.0   7.0  10.04   8.0   7.0  12.05   8.0   1.0   2.0</code></pre><p>#利用pandas实现Excel的数据透视表功能</p><pre><code>Table2 = pd.pivot_table(student, values=[&apos;Height&apos;,&apos;Weight&apos;], columns=[&apos;Sex&apos;,&apos;Age&apos;]).unstack()print(Table2)</code></pre><p>结果：</p><pre><code>Age           11          12    13      14      15     16       Sex                                               Height F    51.3   58.050000  60.9   63.55   64.50    NaN       M    57.5   60.366667  62.5   66.25   66.75   72.0Weight F    50.5   80.750000  91.0   96.25  112.25    NaN       M    85.0  103.500000  84.0  107.50  122.50  150.0</code></pre><p>-</p><pre><code>Table2 = pd.pivot_table(student, values=[&apos;Height&apos;,&apos;Weight&apos;], columns=[&apos;Sex&apos;,&apos;Age&apos;],aggfunc=[np.mean,np.median,np.std]).unstack()print(Table2)</code></pre><p>结果：</p><pre><code>            mean                                          median               \Age           11          12    13      14      15     16     11     12    13          Sex                                                                      Height F    51.3   58.050000  60.9   63.55   64.50    NaN   51.3  58.05  60.9          M    57.5   60.366667  62.5   66.25   66.75   72.0   57.5  59.00  62.5   Weight F    50.5   80.750000  91.0   96.25  112.25    NaN   50.5  80.75  91.0          M    85.0  103.500000  84.0  107.50  122.50  150.0   85.0  99.50  84.0                                     std                                 \Age             14      15     16  11         12        13        14          Sex                                                             Height F     63.55   64.50    NaN NaN   2.474874  6.222540  1.060660          M     66.25   66.75   72.0 NaN   3.932345       NaN  3.889087   Weight F     96.25  112.25    NaN NaN   5.303301  9.899495  8.838835          M    107.50  122.50  150.0 NaN  22.765105       NaN  7.071068   Age                15  16         Sex                 Height F     2.828427 NaN         M     0.353553 NaN  Weight F     0.353553 NaN         M    14.849242 NaN  </code></pre><h1 id="多层索引的使用"><a href="#多层索引的使用" class="headerlink" title="多层索引的使用"></a>多层索引的使用</h1><pre><code>data = pd.DataFrame(np.random.randint(0,150,size=(8,12)),               columns = pd.MultiIndex.from_product([[&apos;模拟考&apos;,&apos;正式考&apos;],                                                   [&apos;数学&apos;,&apos;语文&apos;,&apos;英语&apos;,&apos;物理&apos;,&apos;化学&apos;,&apos;生物&apos;]]),               index = pd.MultiIndex.from_product([[&apos;期中&apos;,&apos;期末&apos;],                                                   [&apos;雷军&apos;,&apos;李斌&apos;],                                                  [&apos;测试一&apos;,&apos;测试二&apos;]]))</code></pre><p><img src="https://img-blog.csdnimg.cn/2020020112342850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200201123504657.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 数据挖掘 </category>
          
          <category> pandas </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NumPy</title>
      <link href="/2020/01/31/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/NumPy/"/>
      <url>/2020/01/31/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/NumPy/</url>
      
        <content type="html"><![CDATA[<h1 id="NumPy性能和python列表对比"><a href="#NumPy性能和python列表对比" class="headerlink" title="NumPy性能和python列表对比"></a>NumPy性能和python列表对比</h1><pre><code>import numpy as npmy_arr = np.arange(1000000)my_list = list(range(1000000))%time for _ in range(10): my_arr2 = my_arr*2%time for _ in range(10): my_list2 = [x*2 for x in my_list]</code></pre><p>结果</p><pre><code>Wall time: 16 msWall time: 636 ms</code></pre><h1 id="创建ndarray"><a href="#创建ndarray" class="headerlink" title="创建ndarray"></a>创建ndarray</h1><pre><code>data1 = [1,23,4.6,2]arr1 = np.array(data1)arr1data2 = [[1,2,3],        [4,5,6]]arr2 = np.array(data2)arr2print(arr2.ndim)print(arr2.shape)np.zeros((2,3))int_array = np.arange(10)calibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)int_array.astype(calibers.dtype)</code></pre><h1 id="numpy数组的运算"><a href="#numpy数组的运算" class="headerlink" title="numpy数组的运算"></a>numpy数组的运算</h1><pre><code>arr = np.array([[1., 2., 3.],               [4., 5., 6.]])print(arr*arr)print(arr*arr == arr)</code></pre><p>结果</p><pre><code>[[ 1.  4.  9.] [16. 25. 36.]][[False  True  True] [ True  True  True]]</code></pre><h1 id="基本索引和切片"><a href="#基本索引和切片" class="headerlink" title="基本索引和切片"></a>基本索引和切片</h1><pre><code>arr = np.arange(10)print(arr)print(arr[5])arr[5:8]=12print(arr[5:8])print(arr)</code></pre><p>创建一个切片后，对切片进行操作，会改变原数组中的值</p><pre><code>arr_slice = arr[5:8]print(arr_slice)arr_slice[1] = 12345print(arr)</code></pre><p>结果：</p><pre><code>[12 12 12][    0     1     2     3     4    12 12345    12     8     9]</code></pre><p>若想要得到的是一个副本而不是视图，需要明确<strong>arr[5:8].copy()</strong></p><h1 id="布尔类型索引"><a href="#布尔类型索引" class="headerlink" title="布尔类型索引"></a>布尔类型索引</h1><pre><code>names = np.array([&apos;Bob&apos;, &apos;Joe&apos;, &apos;Will&apos;, &apos;Joe&apos;, &apos;Joe&apos;])data = np.random.randn(5,4)print(data)print(names == &apos;Bob&apos;)print(data[names == &apos;Bob&apos;])print(names != &apos;Bob&apos;)print(data[~(names == &apos;Bob&apos;)])</code></pre><p>结果：</p><pre><code>array([[-0.72559713,  0.5021692 , -0.40538323,  1.03159213],       [ 0.72144716,  0.9853986 , -1.90579109, -0.42454087],       [-0.35674934,  0.12369675,  1.71308068,  0.61302904],       [ 0.86463033, -0.97788801, -0.07978174, -0.44386613],       [ 1.47681259,  1.09531974, -0.16154207,  1.92472807]])array([ True, False, False, False, False])array([[-0.72559713,  0.5021692 , -0.40538323,  1.03159213]])array([[ 0.72144716,  0.9853986 , -1.90579109, -0.42454087],       [-0.35674934,  0.12369675,  1.71308068,  0.61302904],       [ 0.86463033, -0.97788801, -0.07978174, -0.44386613],       [ 1.47681259,  1.09531974, -0.16154207,  1.92472807]])</code></pre><h1 id="数组转置和轴对换"><a href="#数组转置和轴对换" class="headerlink" title="数组转置和轴对换"></a>数组转置和轴对换</h1><pre><code>arr = np.arange(16).reshape((2,2,4))print(arr)arr.transpose((1, 0, 2))</code></pre><p>结果：</p><pre><code>[[[ 0  1  2  3]  [ 4  5  6  7]] [[ 8  9 10 11]  [12 13 14 15]]]array([[[ 0,  1,  2,  3],        [ 8,  9, 10, 11]],       [[ 4,  5,  6,  7],        [12, 13, 14, 15]]])</code></pre><p>swapaxes返回源数据的视图</p><pre><code>print(arr.swapaxes(1,2))[[[ 0  4]  [ 1  5]  [ 2  6]  [ 3  7]] [[ 8 12]  [ 9 13]  [10 14]  [11 15]]]</code></pre><p>#将条件逻辑表述为数组运算</p><pre><code>x = np.array([1,2,3,4,5])y = np.array([4,6,8,9,0])cond = np.array([True,False,True,False,True])result = np.where(cond,x,y)print(result)</code></pre><p>结果：</p><pre><code>[1 6 3 9 5]</code></pre><p>where：当cond为真则x，否则为y</p><p>#数学和统计方法</p><ul><li>mean 算数平均数</li><li>std、var 标准差、方差</li><li>argmin，argmax 最小最大元素索引</li><li>cumsum 所有元素累计和</li><li>cumprod 所有元素累计积</li></ul><p>#排序</p><pre><code>np.sort(a, axis, kind, order)</code></pre><ol><li>a:要排序的数组</li><li>axis：按照排序数组的轴</li><li>kind默认为“quicksort”</li><li>order：若数组包含字段，则为要排序的字段</li></ol><p>#唯一化以及其他的集合逻辑<br>    names = np.array([‘Bob’, ‘Joe’, ‘Will’, ‘Joe’, ‘Joe’])<br>    print(np.unique(names))<br>结果：</p><pre><code>[&apos;Bob&apos; &apos;Joe&apos; &apos;Will&apos;]</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据挖掘 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NumPy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java通信——传文件、消息</title>
      <link href="/2020/01/29/Java/Java%E9%80%9A%E4%BF%A1%E2%80%94%E2%80%94%E4%BC%A0%E6%96%87%E4%BB%B6/"/>
      <url>/2020/01/29/Java/Java%E9%80%9A%E4%BF%A1%E2%80%94%E2%80%94%E4%BC%A0%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="服务器Server端"><a href="#服务器Server端" class="headerlink" title="服务器Server端"></a>服务器Server端</h1><p>首先，创建服务器，循环等待接入客户端</p><pre><code>java.net.ServerSocket server = new java.net.ServerSocket(port);//创建服务器java.net.Socket client = server.accept();//客户端接入</code></pre><p>接入客户端之后，开始接受消息。</p><p>在这里，接受的消息不只是一个简单的内容，还含有消息头，消息头中含有有关消息的规范。</p><ol><li>消息长度——一个int（4字节）</li><li>消息类型——一个byte（1字节）</li><li>目标用户号码——一个int（4字节）</li></ol><p>消息类型这里分为两种：</p><ul><li>1代表普通消息</li><li>2代表文件</li></ul><h2 id="接收普通消息"><a href="#接收普通消息" class="headerlink" title="接收普通消息"></a>接收普通消息</h2><pre><code>byte[] data = new byte[totalLen-4-1-4];//从流中读取data.length个字节放入数组dins.readFully(data);String msg = new String(data);System.out.println(&quot;发送文本给：&quot;+destNum+&quot;内容是：&quot;+msg);</code></pre><p>这里注意readfully和read的区别：</p><p><strong>网络通信中，当发送大的数据量时，有这样一种可能：一部<br>分数据已发送到对方，有一部分数据还在本地的网卡缓存中，如果调用 read()方法，可能会提前<br>返回而没有读取到足够的数据，在传送大块数据（如一次传送一个较大文件时）可能会出错，而<br>readFully()方法会一直等待，读取到数组长度的所有数据后，才会返回。</strong></p><p>这里是个大坑，我之前传图片会出现传送之后，接受到的图片只显示一半。原因就在此。</p><p>我用read进行了测试，结果生成的图片虽然大小一模一样，但是生成图片无法显示！！！</p><h2 id="接收文件"><a href="#接收文件" class="headerlink" title="接收文件"></a>接收文件</h2><pre><code>//256个字节作为文件名大小System.out.println(&quot;发送文件给：&quot;+destNum);    //destnum为接收者byte[] data = new byte[256];dins.readFully(data);String fileName = new String (data).trim();System.out.println(&quot;读到文件名字是：&quot;+fileName);//读文件内容data = new byte[totalLen-4-1-4-256];dins.readFully(data);//保存在当前目录下FileOutputStream fous = new FileOutputStream(fileName);fous.write(data);fous.flush();    //保证可靠fous.close();</code></pre><p>关于flush：原文链接：<a href="https://blog.csdn.net/qq_38129062/article/details/87115620" target="_blank" rel="noopener">https://blog.csdn.net/qq_38129062/article/details/87115620</a></p><p><strong>设想要给鱼缸换水，所以需要一个水泵，水泵是连接鱼缸和下水道的，咱们的任务就是将鱼缸里面水全抽干，这时，我们就可以把水管当做缓冲区。如果咱们一见鱼缸里面水抽干了就立马关了水泵，这时会发现水管里还有来不及通过水泵流向下水道的残留水，我们可以把抽水当做读数据，排水当做写数据，水管当做缓冲区，这样就容易明白了。</strong></p><p><strong>那么这样一来我们如果中途调用close()方法，输出区也还是有数据的，就像水缸里有水，只是在缓冲区遗留了一部分，这时如果我们先调用flush()方法，就会强制把数据输出，缓存区就清空了，最后再关闭读写流调用close()就完成了。</strong></p><p>收文件时，用了FileOutputStream，文件输出流，它把文件直接写在了当前目录下。<br>可以直接规定文件输出的位置：</p><pre><code>FileOutputStream fous = new FileOutputStream(&quot;C:\\Users\\董润泽\\Desktop&quot;);</code></pre><h1 id="客户机client端"><a href="#客户机client端" class="headerlink" title="客户机client端"></a>客户机client端</h1><p>首先创建客户端并通过ip和主机端口连接到服务器</p><pre><code>java.net.Socket client = new java.net.Socket(ip,port);</code></pre><p>然后获取输入流和输出流：</p><pre><code>java.io.InputStream ins = client.getInputStream();OutputStream ous = client.getOutputStream();</code></pre><p>在while循环内持续发送信息</p><h2 id="发送普通消息"><a href="#发送普通消息" class="headerlink" title="发送普通消息"></a>发送普通消息</h2><pre><code>byte[] strb = msg.getBytes();//得到消息的字节数int totalLen = 4+4+1+strb.length;System.out.println(&quot;发送总长为：&quot;+totalLen);dous.writeInt(totalLen);dous.writeByte(1);//类型1代表文本dous.writeInt(destNum);//写入目标用户号dous.write(strb);dous.flush();</code></pre><p>其中dous为数据输出流</p><pre><code>private DataOutputStream dous;</code></pre><h2 id="发送文件"><a href="#发送文件" class="headerlink" title="发送文件"></a>发送文件</h2><pre><code>//根据文件名创建文件对象File file = new File(fileName);//根据文件对象构造一个输入流FileInputStream ins = new FileInputStream(file);int fileDataLen = ins.available(); //文件数据总长int totalLen = 4+1+4+256+fileDataLen; //得到了要发送数据包的总长dous.writeInt(totalLen);dous.writeByte(2);dous.writeInt(destNum);String shortFileName = file.getName();//写入文件名writeString(dous,shortFileName,256);byte[] fileData = new byte[fileDataLen];ins.read(fileData);//读入文件数据dous.write(fileData);//写到服务器的流中dous.flush();</code></pre><p>注意：发送文本和文件的正文前，首先要发出文件头！</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><h3 id="server"><a href="#server" class="headerlink" title="server"></a>server</h3><pre><code>package qq_Server_test;import java.io.DataInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.io.OutputStream;import java.net.ServerSocket;import org.omg.CORBA.portable.InputStream;public class Server {    public void setUpServer(int port) {        try {            java.net.ServerSocket server = new java.net.ServerSocket(port);            System.out.println(&quot;服务器创建完成&quot;);            while (true) {                java.net.Socket client = server.accept();                System.out.println(&quot;客户端已接入&quot;);                process(client);            }        } catch (Exception e) {            e.printStackTrace();        }    }    private void process(java.net.Socket client) {        try {            OutputStream ous = client.getOutputStream();            java.io.InputStream ins = client.getInputStream();            //将输入流包装成DataInputStream以便读取数据            DataInputStream dins = new DataInputStream(ins);            while(true){                //读取消息，消息以int开头                //1、读消息长度                int totalLen = dins.readInt();                System.out.println(&quot;发来消息长度为：&quot;+totalLen);                //2、读消息类型标识，读一个字节                byte flag = dins.readByte();                System.out.println(&quot;接收消息类型为：&quot;+flag);                //3、读取目标客户号码，一个 int                int destNum = dins.readInt();                System.out.println(&quot;目标用户号码为：&quot;+destNum);                //根据消息内容读取消息体                if(flag==1){//文本消息                    byte[] data = new byte[totalLen-4-1-4];                    //从流中读取data.length个字节放入数组                    dins.readFully(data);                    String msg = new String(data);                    System.out.println(&quot;发送文本给：&quot;+destNum+&quot;内容是：&quot;+msg);                }                else if(flag==2){//文件数据                    //256个字节作为文件名                    System.out.println(&quot;发送文件给：&quot;+destNum);                    byte[] data = new byte[256];                    dins.readFully(data);                    String fileName = new String (data).trim();                    System.out.println(&quot;读到文件名字是：&quot;+fileName);                    //读文件内容                    data = new byte[totalLen-4-1-4-256];                    dins.readFully(data);                    //保存在当前目录下                    FileOutputStream fous = new FileOutputStream(fileName+1);                    fous.write(data);                    fous.flush();                    fous.close();                }            }        } catch (Exception e) {            e.printStackTrace();        }    }    public static void main(String[] args) {        Server server = new Server();        server.setUpServer(9900);    }}</code></pre><h3 id="client"><a href="#client" class="headerlink" title="client"></a>client</h3><pre><code>package qq_client_test;import java.io.DataOutputStream;import java.io.File;import java.io.FileInputStream;import java.io.OutputStream;import java.net.Socket;import java.util.Scanner;import org.omg.CORBA.portable.InputStream;public class Client {    private DataOutputStream dous;//输出流对象    private void writeString(DataOutputStream out,String str,int len){    //向流中写入指定长度的字节，如果不足，就补0        try{            byte[] data = str.getBytes();            out.write(data);            while(len&gt;data.length){                out.writeByte(&apos;\0&apos;);                len--;            }        }catch(Exception e){            e.printStackTrace();        }    }    private void sendTextMsg(String msg,int destNum){        //发送消息        try{            byte[] strb = msg.getBytes();//得到消息的字节数            int totalLen = 4+4+1+strb.length;            System.out.println(&quot;发送总长为：&quot;+totalLen);            dous.writeInt(totalLen);            dous.writeByte(1);//类型1代表文本            dous.writeInt(destNum);//写入目标用户号            dous.write(strb);            dous.flush();                    }catch(Exception e){            e.printStackTrace();        }    }    private void sendFileMsg(String fileName, int destNum){        try{            //根据文件名创建文件对象            File file = new File(fileName);            //根据文件对象构造一个输入流            FileInputStream ins = new FileInputStream(file);            int fileDataLen = ins.available(); //文件数据总长            int totalLen = 4+1+4+256+fileDataLen; //得到了要发送数据包的总长            dous.writeInt(totalLen);            dous.writeByte(2);            dous.writeInt(destNum);            String shortFileName = file.getName();            //写入文件名            writeString(dous,shortFileName,256);            byte[] fileData = new byte[fileDataLen];            ins.read(fileData);//读入文件数据            dous.write(fileData);//写到服务器的流中            dous.flush();        }catch(Exception e){            e.printStackTrace();        }    }    public void conn2Server(String ip, int port){        try{            java.net.Socket client = new java.net.Socket(ip,port);            java.io.InputStream ins = client.getInputStream();            OutputStream ous = client.getOutputStream();            dous = new DataOutputStream(ous);            int testCount = 0;            while(true){                System.out.println(&quot;登陆服务器成功，选择要发的类型（1、聊天，2、文件）：&quot;);                java.util.Scanner sc = new Scanner(System.in);                int type = sc.nextInt();                if(type==1){                    sendTextMsg(&quot;abc聊天&quot;+testCount, 8888);                }                if(type==2){                    sendFileMsg(&quot;C:\\Users\\董润泽\\Desktop\\test1.jpg&quot;, 8888);                }                testCount++;            }        }catch(Exception e){            e.printStackTrace();        }    }    public static void main(String[] args) {        Client client = new Client();        client.conn2Server(&quot;localhost&quot;, 9900);    }}</code></pre><p>运行结果：</p><p><img src="https://img-blog.csdnimg.cn/20200129204116198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""><br><img src="https://img-blog.csdnimg.cn/20200129204122572.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
          <category> 通信 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java通信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>堆内存结构及简单性能调优</title>
      <link href="/2020/01/28/JVM/%E5%A0%86%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84%E5%8F%8A%E7%AE%80%E5%8D%95%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
      <url>/2020/01/28/JVM/%E5%A0%86%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84%E5%8F%8A%E7%AE%80%E5%8D%95%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<p>Java底层最重要的一部分就是jvm堆内存，它影响着Java的性能。</p><p>这篇博客主要介绍Java堆内存的分区及简单的Java调优。</p><h2 id="一、Java堆内存"><a href="#一、Java堆内存" class="headerlink" title="一、Java堆内存"></a>一、Java堆内存</h2><p>首先看这张图：</p><p><img src="https://img-blog.csdnimg.cn/20191121235622158.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ul><li>堆中的分区</li><li>Java堆内存分为两部分：<strong>年轻代、老年代</strong></li></ul><p>其中，年轻代分两个部分：<strong>Eden、Survivor</strong></p><ul><li>内存分配</li></ul><pre><code>老年代的内存占堆内总内存的2/3年轻代占1/3，在年轻代中，Eden分8/10，From和To都是1/10例：堆中总共300M空间，那么老年代有200M，年轻代有100M</code></pre><ul><li>运行时到底是什么个情况呢？</li></ul><p>当我们new一个对象之后，首先，这个对象就会被放入Eden区，直到Eden区内存被占满</p><ul><li>Eden满了之后：</li><li>Eden满了后，就会触发垃圾回收机制gc，但是呢，在这个地方是minor gc。minor gc会回收Eden区的垃圾对象（没有任何指针引用程序的对象）</li></ul><p><img src="https://img-blog.csdnimg.cn/201911220014486.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="JVM虚拟机内部构造图"></p><ul><li>垃圾对象？</li></ul><p>可以理解为：当main函数结束时，栈帧区域会被销毁掉，然后局部变量也就被释放掉，那么指向堆中对象的指针也就被干掉了，最后，堆中的对象就是个垃圾对象。<strong>这里我们要注意一个点：查找垃圾对象的起始是从栈帧中开始的，然后查到的时一个局部变量</strong></p><ul><li>可达性分析算法？GC Root？</li></ul><p>在垃圾对象里面说了，堆中的对象是由局部变量指出的，那么我们就可以利用它来判断是不是垃圾对象，这里的局部变量就可以理解为GC Root。可达性分析算法就是以GC Root为起点，往下搜索，找到的对象就不是垃圾对象，找不到的就是垃圾对象。</p><ul><li>接着minor gc</li></ul><p>上面已经说了，minor gc只是针对Eden区的。然后，当进行一次minor gc后，Eden中的垃圾对象全部被干掉，剩下的非垃圾对象，要进入Survivor区中的From中，这时候，这些非垃圾对象的分代年龄就会加一。</p><ul><li>分代年龄？对象头？</li></ul><p>分代年龄存放在对象的对象头中，每经历一轮的垃圾回收，分代年龄就会加一<br>对象头：（<a href="https://blog.csdn.net/lkforce/article/details/81128115#1%EF%BC%8CMark%20Word" target="_blank" rel="noopener">https://blog.csdn.net/lkforce/article/details/81128115#1%EF%BC%8CMark%20Word</a>）</p><pre><code>1. Mark Word2. 指向类的指针3. 数组长度（只有数组对象才有）</code></pre><ul><li>然后Eden区再次满的时候，又会触发垃圾回收，再次将垃圾对象（包括刚刚放入from中变成新垃圾的对象）干掉，把Eden和From中非垃圾对象放入To中，并将年龄加一。</li></ul><p>当下次Eden再满的时候，又会把Eden和To中的非垃圾对象放入From中，年龄加一，以此不断循环……</p><ul><li>所以年龄有什么卵用？</li></ul><p>当进行的次数多了，直到年龄达到了15（可以改这个参数），这时候就会被移到老年代</p><ul><li>上面已经说过，老年代也有一定的大小，那么如果老年代满的时候怎么办呢？<br>这时候就会发生full gc</li></ul><p>下面我们以代码为例，查看运行时的各区情况：</p><p>利用上篇微信红包中的main函数，使它进入死循环</p><p>（对微信红包算法感兴趣的可以看：<a href="https://blog.csdn.net/qq_44357371/article/details/103115263" target="_blank" rel="noopener">https://blog.csdn.net/qq_44357371/article/details/103115263</a>）</p><pre><code>public static void main(String[] args) {        while (true) {            test.thirdMethod(5, 20);        }    }</code></pre><p><img src="https://img-blog.csdnimg.cn/20191122011034389.gif" alt=""></p><p>old就是代表老年区，我们可以看到Eden区是最快的，然后就发生垃圾回收……，和上面所述相符。</p><p><img src="https://img-blog.csdnimg.cn/20191122011342596.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>说实话，我想看old区被塞满的情况，可惜它增长的太慢了。。。</p><ul><li>接着full gc。STW？</li></ul><p>当发生full gc时，就会产生STW（stop world，，毁掉整个世界），（哈哈哈，没这么牛逼）。这时候呢，就会暂停所有的线程，让垃圾回收机制专心的回收垃圾。这样的话，用户端会卡掉。</p><ul><li>为什么要把线程停掉呢？</li></ul><p>考虑一种情况，如果在找垃圾时，刚好已经在某个链条上面了，这时候，如果线程把这条链给干掉了，那么后面的本来应该全部是垃圾的，但是gc没有把它们给找出来，所以停掉线程。</p><h2 id="二、性能调优"><a href="#二、性能调优" class="headerlink" title="二、性能调优"></a>二、性能调优</h2><ul><li>Jvm性能调优到底调的是什么？</li></ul><p>我们上面已经知道，minor gc发生时，对性能的影响不大，但是full gc发生时，对性能的影响是巨大的。所以调优就是要减少full gc发生的次数，减少STW出现次数；还有就是在发生了Full gc时，所有的线程停掉等待垃圾回收，所以，减少垃圾回收时间。</p><p>下面举一个调优例子：</p><p><img src="https://img-blog.csdnimg.cn/20191122013401603.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>首先分析：</p><p>我们设置了堆的大小为3G，老年区就有2G，eden有800M，from和to各占100M</p><p>线程在运行时，每秒产生60M的对象，用后直接干掉变垃圾，这样大概每13秒就会把Eden占满，触发minor gc。</p><p>但是一个问题，在第13s产生的进程，会被直接移到survivor区，但是它的大小超过了survivor中一个区的一半，这时候触发==对象动态年龄判断机制==，直接进入老年代。</p><p>但是着60M的对象，在下一秒就又变成了垃圾。。。这样算下来，大概5、6分钟就会使老年代触发full gc，这样的效率是极其低下的。</p><p>所以呢，我们可以对这个系统进行一个调优：</p><p>把这些参数改一下，我们可以尽量让垃圾在年轻代就被干掉。</p><p><strong>总共3G，把old区设置为1G（因为没那么多要放的），这样，新生代就有了2G，这样，Eden分了1.6G，from和to分别200M</strong></p><p>这时候，每秒60M的对象过来，最后那一秒在Eden区中传入survivor中，但是根据==对象动态年龄判断机制==，60M小于from或to的一半，不会被传进老年代，变成垃圾，直接被minor gc干死，所以基本上，老年代就不可能被放满，所以就极大的改善了性能！！！</p><p>今晚收获巨大。jvm内容还多，路途且长，继续奋斗！</p><p>图片来源：诸葛老师</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> JVM结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM底层结构</title>
      <link href="/2020/01/28/JVM/JVM%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/"/>
      <url>/2020/01/28/JVM/JVM%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>Java的优点：一次编写，处处执行，即跨平台。<br>Java如何做到跨平台呢？<br><img src="https://img-blog.csdnimg.cn/20191127121557147.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70#pic_center=400x500" alt=""></p><p>首先看这张图片，我们写的Java代码，通过Javac编译成字节码文件，然后通过Java命令进入jvm。但是在不同的平台上机器码不一样，所以jvm一个宏观上的理解就是：从软件层面屏蔽不同操作系统在底层硬件与指令上的区别。</p><h2 id="JVM虚拟机结构图"><a href="#JVM虚拟机结构图" class="headerlink" title="JVM虚拟机结构图"></a>JVM虚拟机结构图</h2><p><img src="https://img-blog.csdnimg.cn/20191127123045440.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="JVM各组成部分："><a href="#JVM各组成部分：" class="headerlink" title="JVM各组成部分："></a>JVM各组成部分：</h2><ul><li>运行时数据区（内存模型）</li><li>类转载子系统</li><li>字节码执行引擎</li></ul><h2 id="运行时数据区（内存模型"><a href="#运行时数据区（内存模型" class="headerlink" title="运行时数据区（内存模型):"></a>运行时数据区（内存模型):</h2><ul><li>堆</li><li>栈（线程栈）</li><li>本地方法栈</li><li>方法区（元空间）</li><li>程序计数器</li></ul><h4 id="栈（线程栈）"><a href="#栈（线程栈）" class="headerlink" title="栈（线程栈）"></a>栈（线程栈）</h4><p>程序在运行时会有很多个线程，每产生一个新的线程，Java的线程栈就会给线程分配一段栈内存区。<br>栈帧：Java中方法在运行时，栈会给每一个方法分配一段栈帧内存区，里面放各自方法的局部变量。栈帧内存区存放在栈中。</p><p>当方法执行完，栈帧中相应的内存区就被干掉</p><p>栈的结构：FILO，在JVM中亦然，先调用的方法，分配栈帧内存区之后压栈，后调用的方法，先被干掉。</p><p>以代码为例：</p><p><img src="https://img-blog.csdnimg.cn/20191127130535518.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>在代码运行时，首先线程栈会给它分配一个栈内存区，然后执行main方法，这个栈内存区给main方法分配一个栈帧内存区，并压入栈底。main方法中调用Computer时，给computer方法分配一个栈帧内存区，当Computer方法执行完成之后，将为该方法分配的栈帧从栈中干掉。最后，main方法执行完成之后，将main方法对应的栈帧干掉。（FILO）</p><p><strong>栈帧内部</strong></p><ul><li>局部变量表</li><li>操作数栈</li><li>动态链接</li><li>方法出口</li></ul><h4 id="方法区（元空间"><a href="#方法区（元空间" class="headerlink" title="方法区（元空间)"></a>方法区（元空间)</h4><ul><li>常量</li><li>静态变量(new出来的对象放在堆里面)</li><li>类元信息</li></ul><p>堆和方法区：</p><p>堆中的对象的头会存放类的信息指针，指向方法区</p><h4 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h4><p>记录程序执行的位置。行数。</p><h4 id="本地方法"><a href="#本地方法" class="headerlink" title="本地方法"></a>本地方法</h4><p>每个线程独有，底层C语言与Java交互调用</p><p><em>可能有些地方说的不清楚，若有疑问，下面留言。</em> </p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> JVM结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>方法调用（解析、动态分派、静态分派）</title>
      <link href="/2020/01/28/JVM/JVM%E5%AD%97%E8%8A%82%E7%A0%81%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8%EF%BC%88%E8%A7%A3%E6%9E%90%E3%80%81%E5%8A%A8%E6%80%81%E5%88%86%E6%B4%BE%E3%80%81%E9%9D%99%E6%80%81%E5%88%86%E6%B4%BE%EF%BC%89/"/>
      <url>/2020/01/28/JVM/JVM%E5%AD%97%E8%8A%82%E7%A0%81%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8%EF%BC%88%E8%A7%A3%E6%9E%90%E3%80%81%E5%8A%A8%E6%80%81%E5%88%86%E6%B4%BE%E3%80%81%E9%9D%99%E6%80%81%E5%88%86%E6%B4%BE%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>参考文章：</p><p><a href="https://blog.csdn.net/wangdongli_1993/article/details/81428848" target="_blank" rel="noopener">JVM（十四）方法调用</a></p><p><a href="https://blog.csdn.net/fan2012huan/article/details/51004615" target="_blank" rel="noopener" title="java方法调用之单分派与多分派（二）">java方法调用之单分派与多分派（二）</a></p><p>方法调用阶段就是确定被调用方法的版本，即调用哪一个方法。</p><h1 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h1><p>我们已经知道，class文件中需要调用的方法都是一个符号引用，而在方法调用中的解析阶段，就是要把一部分符号引用转化为直接引用。</p><p>能在解析阶段将方法的符号引用转化成直接引用的的方法，必须在方法运行前就确定一个可调用的版本，并且这个版本在运行阶段是不可改变的。</p><p>“编译期可知，运行期不可变”，符合这个规则的方法有静态方法和私有方法两大类。前者与所属的类直接关联，后者在外部不可以被访问。这两种方法都适合在解析调用，也就是把这些方法的符号引用转化成直接引用。</p><p>与之对应5条调用方法的字节码指令：</p><ul><li><strong>invokestatic:调用静态方法</strong></li><li><strong>invokespecial：调用实例构造器<init>方法，私有方法和父类方法</strong></li><li><strong>invokevirtual：调用所有虚方法</strong></li><li><strong>invokeinterface：调用接口方法，运行时确定一个实现此接口的对象</strong></li><li><strong>invokedynamic:先在运行时动态解析出调用点限定符所引用的的方法，然后再执行此方法</strong></li></ul><p>只有用invokestatic和invokespecial指令调用的方法（还有final修饰的方法），都可以在解析阶段确定调用版本，这些方法叫做非虚方法。</p><p>剩下三个字节码指令调用的方法叫做虚方法（invokevirtual指令调用的final修饰的方法除外）</p><p>解析调用是一个静态过程，编译期间就可以确定，解析阶段将符号引用转化为直接引用。</p><p><img src="https://img-blog.csdnimg.cn/20200127104051600.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>将代码转换为字节码文件：</p><p><img src="https://img-blog.csdnimg.cn/20200127104142282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>可以看到，确实是通过invokestatic命令调用sayhello方法。</p><h1 id="分派"><a href="#分派" class="headerlink" title="分派"></a>分派</h1><h2 id="1-静态分派—（重载）"><a href="#1-静态分派—（重载）" class="headerlink" title="1. 静态分派—（重载）"></a>1. 静态分派—（重载）</h2><p>先看一段简单的代码：</p><p><img src="https://img-blog.csdnimg.cn/20200127105004799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>输出结果是什么？</p><p>我们先来分析一下：</p><p>首先引入两个概念：我们把代码中的Human成为变量的<strong>静态类型</strong>，把Man、Woman称为变量的<strong>实际类型</strong>。<br>静态类型和动态类型在程序中都可以发生一些变化，区别是静态类型的变化仅仅在使用时发生，变量本身的静态类型不会改变，并且最终的静态类型是在编译期可知的；而实际类型变化的结果在运行期才可确定，<strong>编译器在编译程序的时候不知道一个对象的实际类型是什么</strong>。</p><p>现在再看代码，对于sd.sayHello(man);直观上看，它似乎传入的是Man类型的参数man，所以应该打印的是man的方法“man is saying hello”，</p><p>但是要注意，man 的静态类型仍然是Human，实际类型才是man，所以，在编译阶段，Javac选择了sayHello(Human)作为调用目标。</p><p>即最终输出：</p><p><img src="https://img-blog.csdnimg.cn/20200127110639837.png" alt=""></p><p>通过这个实例，我们可以看到，这里是通过静态类型来定位执行方法的版本，这样的分派动作称为<strong>静态分派</strong><br>静态分派于重载有很深的关系</p><h2 id="2-动态分派—（重写）"><a href="#2-动态分派—（重写）" class="headerlink" title="2. 动态分派—（重写）"></a>2. 动态分派—（重写）</h2><p>先看代码：</p><p><img src="https://img-blog.csdnimg.cn/20200127112750917.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>生成字节码文件：</p><p><img src="https://img-blog.csdnimg.cn/20200127113823727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>注意代码中的：</p><p><img src="https://img-blog.csdnimg.cn/20200127114045571.png" alt=""></p><p>对应字节码中的：</p><p><img src="https://img-blog.csdnimg.cn/20200127114132633.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>invokevirtual指令的运行时解析过程大致分为：</p><p>（1）找到操作数栈栈顶的第一个元素所指向的对象的实际类型。记作C。</p><p>（2）如果在类型C中找到与常量描述符和简单名称相符的方法，就进行访问权限校验，通过则返回方法的直接引用，没有通过则抛出java.lang.IllegalAccessError异常</p><p>（3）如果在类型中没有找到对应的方法，则按照继承关系从下往上对C的父类依此查找方法</p><p>（4）若始终没有找到合适方法，抛出java.lang.AbstractMethodError异常。</p><p>invokevirtual指令执行的时候先确定方法调用的对象的实际类型，所以会把两次方法调用的符号引用解析到不同的直接引用上，这个过程叫做<strong>动态分派</strong>，是方法重写的本质。</p><p>代码结果为：</p><p><img src="https://img-blog.csdnimg.cn/20200127124154544.png" alt=""></p><h2 id="单分派与多分派"><a href="#单分派与多分派" class="headerlink" title="单分派与多分派"></a>单分派与多分派</h2><p>先看一段代码：</p><pre><code>public class Dispatcher {    static class QQ {}    static class _360 {}    public static class Father {        public void hardChoice(QQ arg) {            System.out.println(&quot;father choose QQ&quot;);        }        public void hardChoice(_360 arg) {            System.out.println(&quot;father choose _360&quot;);        }    }    public static class Son extends Father {        @Override        public void hardChoice(QQ arg) {            System.out.println(&quot;son choose QQ&quot;);        }        public void hardChoice(_360 arg) {            System.out.println(&quot;son choose 360&quot;);        }    }    public static void main(String[] args) {        Father father = new Father();        Father son = new Son();        father.hardChoice(new _360());        son.hardChoice(new QQ());    }}</code></pre><p>运行结果是什么？</p><p><img src="https://img-blog.csdnimg.cn/20200129100434458.png" alt=""></p><p>它的字节码文件：</p><p><img src="https://img-blog.csdnimg.cn/20200129103435680.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>我们分别从<strong>静态分派</strong>和<strong>动态分派</strong>的角度来分析</p><h3 id="1-静态分派"><a href="#1-静态分派" class="headerlink" title="1. 静态分派"></a>1. 静态分派</h3><p>看字节码的</p><pre><code>24: invokevirtual #8                  // Method 单分派多分派/Dispatcher$Father.hardChoice:(L单分派多分派/Dispatcher$_360;)V35: invokevirtual #11                 // Method 单分派多分派/Dispatcher$Father.hardChoice:(L单分派多分派/Dispatcher$QQ;)V</code></pre><p>对应的是代码的：</p><pre><code>father.hardChoice(new _360());son.hardChoice(new QQ());</code></pre><p>可以看到，invokevirtual相同，调用的都是<strong>$Father.hardChoice</strong>方法，只是他们的参数不同。</p><p>这说明他们的静态类型相同，都是Father。在选择目标方法时，根据两个<strong>宗量</strong>，是多分派的，即<strong>静态分派属于多分派类型</strong>。</p><blockquote><p>宗量：方法的接收者和方法的参数统称为方法的宗量。</p></blockquote><blockquote><p>单分派：根据一个宗量对目标方法进行选择</p></blockquote><blockquote><p>多分派：多于一个宗量对目标方法进行选择</p></blockquote><h3 id="2-动态分派"><a href="#2-动态分派" class="headerlink" title="2. 动态分派"></a>2. 动态分派</h3><p>当在执行</p><pre><code>father.hardChoice(new _360());son.hardChoice(new QQ());</code></pre><p>发现son的实际类型是Son，所以转去调用Son的方法。在father中也执行了此过程，只不过，father的实际类型仍然是father。</p><p>目标选择时只依据了一个宗量，是单分派的。因此，<strong>动态分派属于单分派类型</strong>。</p><h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><p>静态分派关注了两个宗量，即<strong>静态类型和参数</strong>，（参数对比重载）</p><p>而动态分派关注<strong>实际类型</strong>，（对比重写）</p><p><strong>java语言是一个静态多分派，动态单分派的语言</strong></p><h2 id="动态分派的实现"><a href="#动态分派的实现" class="headerlink" title="动态分派的实现"></a>动态分派的实现</h2><p>动态分派类似重写，动态分派是非常频繁的动作，运行时需要在元数据中搜索合适的目标方法，最常用的“稳定优化”手段就是建立一个虚方法表，存放各个方法实际入口地址。</p><p>子类重写了方法，就会指向子类的方法地址。</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> 字节码执行引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 字节码执行引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解基于栈的字节码执行引擎及JVM底层结构</title>
      <link href="/2020/01/28/JVM/%E9%80%9A%E8%BF%87Java%E5%AD%97%E8%8A%82%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E5%8F%8AJVM%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/"/>
      <url>/2020/01/28/JVM/%E9%80%9A%E8%BF%87Java%E5%AD%97%E8%8A%82%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E5%8F%8AJVM%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>在读本文时，可以参考我的另外两篇介绍jvm的博客。</p><p><a href="https://blog.csdn.net/qq_44357371/article/details/103330641" target="_blank" rel="noopener">JVM底层结构</a></p><p><a href="https://blog.csdn.net/qq_44357371/article/details/103192906" target="_blank" rel="noopener">Java堆内存介绍及简单性能调优</a></p><h2 id="生成一个字节码文件"><a href="#生成一个字节码文件" class="headerlink" title="生成一个字节码文件"></a>生成一个字节码文件</h2><p>首先我们编写一个简单的Java文件</p><p><img src="https://img-blog.csdnimg.cn/20200121101732904.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>在文件夹中找到这个文件，可以看到只有一个.java 文件</p><p><img src="https://img-blog.csdnimg.cn/20200121101824654.png" alt=""></p><p>在命令行使用Javac命令，生成.class文件</p><p><img src="https://img-blog.csdnimg.cn/2020012110192682.png" alt=""></p><p>使用Javap -c命令，生成字节码文件</p><p><img src="https://img-blog.csdnimg.cn/20200121102035583.png" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200121102059870.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>看到这个乱七八糟的代码，你可能会问，，这tm是什么鬼。</p><p>下面通过<strong>JVM指令手册</strong>从Java底层对字节码进行分析</p><h2 id="字节码分析"><a href="#字节码分析" class="headerlink" title="字节码分析"></a>字节码分析</h2><h3 id="一、computer方法："><a href="#一、computer方法：" class="headerlink" title="一、computer方法："></a>一、computer方法：</h3><p><img src="https://img-blog.csdnimg.cn/20200121103143498.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h4 id="代码-int-a-1"><a href="#代码-int-a-1" class="headerlink" title="==代码 int a=1=="></a>==代码 int a=1==</h4><p>iconst_1:将int型常量①压入操作数栈</p><p>istore_2:将int类型的值存入局部变量①</p><p>结合<strong>JVM虚拟机内存结构图</strong></p><p><img src="https://img-blog.csdnimg.cn/20200121103907248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>首先，给常量a在操作数栈中分配一个内存空间，即<strong>iconst_1</strong>对应<strong>int a</strong></p><p>然后，把常量a的值变成1，并存进局部变量表，即<strong>istore_2</strong>对应<strong>a=1</strong></p><h4 id="代码-int-b-2"><a href="#代码-int-b-2" class="headerlink" title="代码 int b=2"></a>代码 int b=2</h4><p>iconst_2:将int型常量②压入操作数栈</p><p>istore_2:将int类型的值存入局部变量②</p><p>同理，很容易理解</p><h4 id="代码-return-a-b"><a href="#代码-return-a-b" class="headerlink" title="代码 return a+b"></a>代码 return a+b</h4><p>iload_1：从局部变量①中转载int类型值  即<strong>a的值1</strong></p><p>iload_2：从局部变量②中装载int类型值  即<strong>b的值2</strong></p><p>即把变量a的值1给装载出来，放在操作数栈</p><p>把变量b的值2给装载出来，放在操作数栈</p><p><img src="https://img-blog.csdnimg.cn/20200121105426146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>iadd:执行int类型的加法</p><p>即从操作数栈中依次弹出栈顶元素相加，最终生成的结果压回操作数栈    <strong>a+b</strong></p><p>最后 ireturn:从当前方法返回int</p><p>从操作数栈中弹出3</p><p>即对应 <strong>return 3</strong></p><h3 id="二、main方法"><a href="#二、main方法" class="headerlink" title="二、main方法"></a>二、main方法</h3><p><img src="https://img-blog.csdnimg.cn/20200121110948756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h4 id="代码-Test-test-new-Test"><a href="#代码-Test-test-new-Test" class="headerlink" title="代码 Test test = new Test()"></a>代码 Test test = new Test()</h4><p> new：创建一个对象</p><p> 我们对比上面的 int a 可以知道，new出来的test 也是一个局部变量，它被存放在main方法对应的栈帧内存区的局部变量表中，<br> 但在jvm底层，对象创建之后放在堆中，</p><p> 这样我们就可以发现栈和堆之间的一个联系。</p><p> <strong>那么，这两个东西真的就是一样的嘛？</strong></p><p> 其实，局部变量表中存放的是堆中对象对应的内存地址，即可以理解为它存放一个指向堆中对象的指针。</p><p> <img src="https://img-blog.csdnimg.cn/20200121112345695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p> 到这里，我们就可以通过字节码在jvm底层结构理解整个Java代码的执行过程。</p><p> 最后还有一个问题</p><p><img src="https://img-blog.csdnimg.cn/20200121112738874.png" alt="">)<img src="https://img-blog.csdnimg.cn/20200121112648124.png" alt="在这里插入图片描述"></p><p>在执行main方法时，会跳出去转到computer方法中，在执行完computer方法后，它会再回到main方法中，但是，它回到哪了呢？</p><p>方法出口就记录着返回的位置。</p><p>同样，程序计数器？</p><p><img src="https://img-blog.csdnimg.cn/20200121113147728.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>它记录着程序执行的位置，即行数</p><p>设想，在Java代码执行时，经常会有多个线程。学过操作系统的就知道，cpu在运行时，经常会发生线程被抢占，被执行的线程挂起。那么，这个线程被挂起之后，它重新运行时，从哪开始呢？</p><p>程序计数器就解决了这个问题。</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> 字节码执行引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 字节码执行引擎 </tag>
            
            <tag> JVM结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>红酒质量预测</title>
      <link href="/2020/01/28/%E7%BA%A2%E9%85%92%E8%B4%A8%E9%87%8F%E9%A2%84%E6%B5%8B/%E7%BA%A2%E9%85%92%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
      <url>/2020/01/28/%E7%BA%A2%E9%85%92%E8%B4%A8%E9%87%8F%E9%A2%84%E6%B5%8B/%E7%BA%A2%E9%85%92%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wine = pd.read_csv(<span class="string">"C:\\Users\\董润泽\\Desktop\\winequality-red.csv"</span>,sep=<span class="string">";"</span>)</span><br><span class="line">wine.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>fixed acidity</th>      <th>volatile acidity</th>      <th>citric acid</th>      <th>residual sugar</th>      <th>chlorides</th>      <th>free sulfur dioxide</th>      <th>total sulfur dioxide</th>      <th>density</th>      <th>pH</th>      <th>sulphates</th>      <th>alcohol</th>      <th>quality</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>7.4</td>      <td>0.70</td>      <td>0.00</td>      <td>1.9</td>      <td>0.076</td>      <td>11.0</td>      <td>34.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>5</td>    </tr>    <tr>      <th>1</th>      <td>7.8</td>      <td>0.88</td>      <td>0.00</td>      <td>2.6</td>      <td>0.098</td>      <td>25.0</td>      <td>67.0</td>      <td>0.9968</td>      <td>3.20</td>      <td>0.68</td>      <td>9.8</td>      <td>5</td>    </tr>    <tr>      <th>2</th>      <td>7.8</td>      <td>0.76</td>      <td>0.04</td>      <td>2.3</td>      <td>0.092</td>      <td>15.0</td>      <td>54.0</td>      <td>0.9970</td>      <td>3.26</td>      <td>0.65</td>      <td>9.8</td>      <td>5</td>    </tr>    <tr>      <th>3</th>      <td>11.2</td>      <td>0.28</td>      <td>0.56</td>      <td>1.9</td>      <td>0.075</td>      <td>17.0</td>      <td>60.0</td>      <td>0.9980</td>      <td>3.16</td>      <td>0.58</td>      <td>9.8</td>      <td>6</td>    </tr>    <tr>      <th>4</th>      <td>7.4</td>      <td>0.70</td>      <td>0.00</td>      <td>1.9</td>      <td>0.076</td>      <td>11.0</td>      <td>34.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>5</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 1599 entries, 0 to 1598Data columns (total 12 columns):fixed acidity           1599 non-null float64volatile acidity        1599 non-null float64citric acid             1599 non-null float64residual sugar          1599 non-null float64chlorides               1599 non-null float64free sulfur dioxide     1599 non-null float64total sulfur dioxide    1599 non-null float64density                 1599 non-null float64pH                      1599 non-null float64sulphates               1599 non-null float64alcohol                 1599 non-null float64quality                 1599 non-null int64dtypes: float64(11), int64(1)memory usage: 150.0 KB</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine.isnull().sum()</span><br></pre></td></tr></table></figure><pre><code>fixed acidity           0volatile acidity        0citric acid             0residual sugar          0chlorides               0free sulfur dioxide     0total sulfur dioxide    0density                 0pH                      0sulphates               0alcohol                 0quality                 0dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bins = (<span class="number">2</span>, <span class="number">6.5</span>, <span class="number">8</span>)</span><br><span class="line">group_names=[<span class="string">'bad'</span>, <span class="string">'good'</span>]</span><br><span class="line">wine[<span class="string">'quality'</span>] = pd.cut(wine[<span class="string">'quality'</span>], bins = bins, labels = group_names)</span><br><span class="line">wine[<span class="string">'quality'</span>].unique()</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)&lt;ipython-input-13-99255379e85c&gt; in &lt;module&gt;      1 bins = (2, 6.5, 8)      2 group_names=[&apos;bad&apos;, &apos;good&apos;]----&gt; 3 wine[&apos;quality&apos;] = pd.cut(wine[&apos;quality&apos;], bins = bins, labels = group_names)      4 wine[&apos;quality&apos;].unique()C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates)    239                               include_lowest=include_lowest,    240                               dtype=dtype,--&gt; 241                               duplicates=duplicates)    242     243     return _postprocess_for_cut(fac, bins, retbins, x_is_series,C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates)    342     343     side = &apos;left&apos; if right else &apos;right&apos;--&gt; 344     ids = ensure_int64(bins.searchsorted(x, side=side))    345     346     if include_lowest:TypeError: &apos;&lt;&apos; not supported between instances of &apos;float&apos; and &apos;str&apos;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>fixed acidity</th>      <th>volatile acidity</th>      <th>citric acid</th>      <th>residual sugar</th>      <th>chlorides</th>      <th>free sulfur dioxide</th>      <th>total sulfur dioxide</th>      <th>density</th>      <th>pH</th>      <th>sulphates</th>      <th>alcohol</th>      <th>quality</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>7.4</td>      <td>0.70</td>      <td>0.00</td>      <td>1.9</td>      <td>0.076</td>      <td>11.0</td>      <td>34.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>bad</td>    </tr>    <tr>      <th>1</th>      <td>7.8</td>      <td>0.88</td>      <td>0.00</td>      <td>2.6</td>      <td>0.098</td>      <td>25.0</td>      <td>67.0</td>      <td>0.9968</td>      <td>3.20</td>      <td>0.68</td>      <td>9.8</td>      <td>bad</td>    </tr>    <tr>      <th>2</th>      <td>7.8</td>      <td>0.76</td>      <td>0.04</td>      <td>2.3</td>      <td>0.092</td>      <td>15.0</td>      <td>54.0</td>      <td>0.9970</td>      <td>3.26</td>      <td>0.65</td>      <td>9.8</td>      <td>bad</td>    </tr>    <tr>      <th>3</th>      <td>11.2</td>      <td>0.28</td>      <td>0.56</td>      <td>1.9</td>      <td>0.075</td>      <td>17.0</td>      <td>60.0</td>      <td>0.9980</td>      <td>3.16</td>      <td>0.58</td>      <td>9.8</td>      <td>bad</td>    </tr>    <tr>      <th>4</th>      <td>7.4</td>      <td>0.70</td>      <td>0.00</td>      <td>1.9</td>      <td>0.076</td>      <td>11.0</td>      <td>34.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>bad</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">label_quality = LabelEncoder()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine[<span class="string">'quality'</span>] = label_quality.fit_transform(wine[<span class="string">'quality'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>fixed acidity</th>      <th>volatile acidity</th>      <th>citric acid</th>      <th>residual sugar</th>      <th>chlorides</th>      <th>free sulfur dioxide</th>      <th>total sulfur dioxide</th>      <th>density</th>      <th>pH</th>      <th>sulphates</th>      <th>alcohol</th>      <th>quality</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>7.4</td>      <td>0.70</td>      <td>0.00</td>      <td>1.9</td>      <td>0.076</td>      <td>11.0</td>      <td>34.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>7.8</td>      <td>0.88</td>      <td>0.00</td>      <td>2.6</td>      <td>0.098</td>      <td>25.0</td>      <td>67.0</td>      <td>0.9968</td>      <td>3.20</td>      <td>0.68</td>      <td>9.8</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>7.8</td>      <td>0.76</td>      <td>0.04</td>      <td>2.3</td>      <td>0.092</td>      <td>15.0</td>      <td>54.0</td>      <td>0.9970</td>      <td>3.26</td>      <td>0.65</td>      <td>9.8</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>11.2</td>      <td>0.28</td>      <td>0.56</td>      <td>1.9</td>      <td>0.075</td>      <td>17.0</td>      <td>60.0</td>      <td>0.9980</td>      <td>3.16</td>      <td>0.58</td>      <td>9.8</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>7.4</td>      <td>0.70</td>      <td>0.00</td>      <td>1.9</td>      <td>0.076</td>      <td>11.0</td>      <td>34.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>0</td>    </tr>    <tr>      <th>5</th>      <td>7.4</td>      <td>0.66</td>      <td>0.00</td>      <td>1.8</td>      <td>0.075</td>      <td>13.0</td>      <td>40.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>0</td>    </tr>    <tr>      <th>6</th>      <td>7.9</td>      <td>0.60</td>      <td>0.06</td>      <td>1.6</td>      <td>0.069</td>      <td>15.0</td>      <td>59.0</td>      <td>0.9964</td>      <td>3.30</td>      <td>0.46</td>      <td>9.4</td>      <td>0</td>    </tr>    <tr>      <th>7</th>      <td>7.3</td>      <td>0.65</td>      <td>0.00</td>      <td>1.2</td>      <td>0.065</td>      <td>15.0</td>      <td>21.0</td>      <td>0.9946</td>      <td>3.39</td>      <td>0.47</td>      <td>10.0</td>      <td>1</td>    </tr>    <tr>      <th>8</th>      <td>7.8</td>      <td>0.58</td>      <td>0.02</td>      <td>2.0</td>      <td>0.073</td>      <td>9.0</td>      <td>18.0</td>      <td>0.9968</td>      <td>3.36</td>      <td>0.57</td>      <td>9.5</td>      <td>1</td>    </tr>    <tr>      <th>9</th>      <td>7.5</td>      <td>0.50</td>      <td>0.36</td>      <td>6.1</td>      <td>0.071</td>      <td>17.0</td>      <td>102.0</td>      <td>0.9978</td>      <td>3.35</td>      <td>0.80</td>      <td>10.5</td>      <td>0</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine[<span class="string">'quality'</span>].value_counts()</span><br></pre></td></tr></table></figure><pre><code>0    13821     217Name: quality, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.countplot(wine[<span class="string">'quality'</span>])</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2076c534cc0&gt;</code></pre><p><img src="output_10_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#seperate the dataset as response variable and feature variable</span></span><br><span class="line">X = wine.drop(<span class="string">'quality'</span> , axis=<span class="number">1</span>)</span><br><span class="line">y = wine[<span class="string">'quality'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Train and test splitting of data</span></span><br><span class="line">X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state = <span class="number">42</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sc = StandardScaler()</span><br><span class="line">X_train = sc.fit_transform(X_train)</span><br><span class="line">X_test = sc.transform(X_test)</span><br></pre></td></tr></table></figure><h1 id="Random-Forest-Classifier"><a href="#Random-Forest-Classifier" class="headerlink" title="Random Forest Classifier"></a>Random Forest Classifier</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">600</span>)</span><br><span class="line">rfc.fit(X_train,y_train)</span><br><span class="line">pred_rfc = rfc.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(classification_report(y_test, pred_rfc))</span><br><span class="line">print(confusion_matrix(y_test,pred_rfc))</span><br><span class="line"><span class="comment">#对于坏酒，264对，9错</span></span><br><span class="line"><span class="comment">#好酒，13对，24错</span></span><br></pre></td></tr></table></figure><pre><code>              precision    recall  f1-score   support           0       0.92      0.97      0.94       273           1       0.73      0.51      0.60        47    accuracy                           0.90       320   macro avg       0.82      0.74      0.77       320weighted avg       0.89      0.90      0.89       320[[264   9] [ 23  24]]</code></pre><h1 id="SVM-classifier"><a href="#SVM-classifier" class="headerlink" title="SVM classifier"></a>SVM classifier</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clf=svm.SVC()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">pred_clf = clf.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(classification_report(y_test, pred_clf))</span><br><span class="line">print(confusion_matrix(y_test,pred_clf))</span><br></pre></td></tr></table></figure><pre><code>              precision    recall  f1-score   support           0       0.88      0.98      0.93       273           1       0.71      0.26      0.37        47    accuracy                           0.88       320   macro avg       0.80      0.62      0.65       320weighted avg       0.86      0.88      0.85       320[[268   5] [ 35  12]]</code></pre><h1 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mlpc = MLPClassifier(hidden_layer_sizes=(<span class="number">11</span>,<span class="number">11</span>,<span class="number">11</span>),max_iter=<span class="number">500</span>)</span><br><span class="line">mlpc.fit(X_train,y_train)</span><br><span class="line">pred_mlpc = mlpc.predict(X_test)</span><br></pre></td></tr></table></figure><pre><code>C:\ProgramData\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn&apos;t converged yet.  % self.max_iter, ConvergenceWarning)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(classification_report(y_test, pred_clf))</span><br><span class="line">print(confusion_matrix(y_test,pred_clf))</span><br></pre></td></tr></table></figure><pre><code>              precision    recall  f1-score   support           0       0.88      0.98      0.93       273           1       0.71      0.26      0.37        47    accuracy                           0.88       320   macro avg       0.80      0.62      0.65       320weighted avg       0.86      0.88      0.85       320[[268   5] [ 35  12]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">print(accuracy_score(y_test,pred_rfc))</span><br><span class="line">print(accuracy_score(y_test,pred_mlpc))</span><br><span class="line">print(accuracy_score(y_test,pred_clf))</span><br></pre></td></tr></table></figure><pre><code>0.8750.8843750.875</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 入门 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>运行时栈帧结构</title>
      <link href="/2020/01/28/JVM/JVM%E5%AD%97%E8%8A%82%E7%A0%81%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E1%E2%80%94%E2%80%94%E8%BF%90%E8%A1%8C%E6%97%B6%E6%A0%88%E5%B8%A7%E7%BB%93%E6%9E%84/"/>
      <url>/2020/01/28/JVM/JVM%E5%AD%97%E8%8A%82%E7%A0%81%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E1%E2%80%94%E2%80%94%E8%BF%90%E8%A1%8C%E6%97%B6%E6%A0%88%E5%B8%A7%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>栈帧中包括：局部变量表、操作数栈、动态链接、方法出口。</p><p><img src="https://img-blog.csdnimg.cn/20200126175647510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h2 id="动态连接"><a href="#动态连接" class="headerlink" title="动态连接"></a>动态连接</h2><p>在<a href="https://blog.csdn.net/qq_44357371/article/details/104072447" target="_blank" rel="noopener">类加载的过程</a>中已介绍过类加载过程中的解析阶段，是将符号引用转换为直接引用，但是是静态的。<br>在与运行期间转化为直接引用就是动态连接。</p><p>其他部分可参考阅读</p><p><a href="https://blog.csdn.net/qq_44357371/article/details/103330641" target="_blank" rel="noopener">JVM底层结构</a></p><p><a href="https://blog.csdn.net/qq_44357371/article/details/104059180" target="_blank" rel="noopener">通过Java字节码深入理解Java执行过程及JVM底层结构</a></p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> 字节码执行引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 字节码执行引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>类加载器</title>
      <link href="/2020/01/28/JVM/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B63%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/"/>
      <url>/2020/01/28/JVM/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B63%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<blockquote><p>类加载器：实现 “ 通过类的全限定名来获取描述此类的二进制字节流 ” 的模块<br><img src="https://img-blog.csdnimg.cn/20200126141937329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p></blockquote><h2 id="类加载器种类："><a href="#类加载器种类：" class="headerlink" title="类加载器种类："></a>类加载器种类：</h2><ul><li><strong>启动类加载器</strong>：负责加载支撑JVM运行的位于jre/lib目录下的核心类库（例如：String、Object类），在虚拟机启动时就会加载完，以支撑虚拟机的运行。对于hotspot，这个类加载器使用C++实现。</li><li><strong>扩展类加载器</strong>：负责加载支撑JVM运行的位于jre/lib/ext中的JAR包。由Java语言实现，父类加载器为null。</li><li><strong>应用程序类加载器</strong>：负责加载用户路径ClassPath下的类库。由Java语言实现，父类加载器为ExtClassLoader。</li></ul><p>类加载器加载Class大致要经过如下8个步骤：</p><ol><li>检测此Class是否载入过，即在缓冲区中是否有此Class，如果有直接进入第8步，否则进入第2步。</li><li>如果没有父类加载器，则要么Parent是根类加载器，要么本身就是根类加载器，则跳到第4步，如果父类加载器存在，则进入第3步。</li><li>请求使用父类加载器去载入目标类，如果载入成功则跳至第8步，否则接着执行第5步。</li><li>请求使用根类加载器去载入目标类，如果载入成功则跳至第8步，否则跳至第7步。</li><li>当前类加载器尝试寻找Class文件，如果找到则执行第6步，如果找不到则执行第7步。</li><li>从文件中载入Class，成功后跳至第8步。</li><li>抛出ClassNotFountException异常。</li><li>返回对应的java.lang.Class对象。</li></ol><h2 id="类加载机制："><a href="#类加载机制：" class="headerlink" title="类加载机制："></a>类加载机制：</h2><ul><li>全盘负责：所谓全盘负责，就是当一个类加载器负责加载某个Class时，该Class所依赖和引用其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入。</li><li>双亲委派：所谓的双亲委派，则是先让父类加载器试图加载该Class，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类。通俗的讲，就是某个特定的类加载器在接到加载类的请求时，首先将加载任务委托给父加载器，依次递归，如果父加载器可以完成类加载任务，就成功返回；只有父加载器无法完成此加载任务时，才自己去加载。</li><li>缓存机制。缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区中搜寻该Class，只有当缓存区中不存在该Class对象时，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓冲区中。这就是为很么修改了Class后，必须重新启动JVM，程序所做的修改才会生效的原因。</li></ul><p><strong>双亲委派机制</strong></p><p><img src="https://img-blog.csdnimg.cn/2020012615383646.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70#pic_center=300x300" alt=""></p><p>双亲委派机制要求除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器，但这里的父子关系是组合关系</p><blockquote><p>组合关系，是关联关系的一种，是比聚合关系强的关系。它要求普通的聚合关系中代表整体的对象负责代表部分对象的生命周期，组合关系是不能共享的。代表整体的对象需要负责保持部分对象和存活，在一些情况下将负责代表部分的对象湮灭掉。代表整体的对象可以将代表部分的对象传递给另一个对象，由后者负责此对象的生命周期。换言之，代表部分的对象在每一个时刻只能与一个对象发生组合关系，由后者排他地负责生命周期。部分和整体的生命周期一样。</p></blockquote><p>对于我们写出来的.class文件，即对应<strong>应用程序加载器</strong>，它在加载时，首先会向上委托，委托给他的父亲，即<strong>扩展类加载器</strong>，扩展类加载器继续向上委托，给<strong>启动类加载器</strong>，而启动类加载器没有父类，则在启动类加载器查找是否有这个类，有则加载，无则打回；然后在扩展类加载器中找，有则加载，无则打回；最后在应用程序类加载器中加载。</p><p>  <strong>双亲委派机制的优势</strong>：采用双亲委派模式的是好处是Java类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层级关可以避免类的重复加载，当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次。其次是考虑到安全因素，java核心api中定义类型不会被随意替换，假设通过网络传递一个名为java.lang.Integer的类，通过双亲委托模式传递到启动类加载器，而启动类加载器在核心Java API发现这个名字的类，发现该类已被加载，并不会重新加载网络传递的过来的java.lang.Integer，而直接返回已加载过的Integer.class，这样便可以防止核心API库被随意篡改。</p><p>我们以一个简单的代码为例：</p><p><img src="https://img-blog.csdnimg.cn/20200126151932356.png" alt=""></p><p>我们自己定义一个String类，注意==在java.lang包中也有一个系统自带的String类==，而我们定义的这个类，包名也叫做java.lang，运行结果怎么样呢 ？</p><p><img src="https://img-blog.csdnimg.cn/20200126152208405.png" alt=""></p><p>为啥会找不到main方法？<br>这里就要从双亲委托机制进行解释：我们定义的这个类在加载时首先是对应应用程序类加载器，它要向上委托，直到启动类加载器。</p><p>上面已经说过，启动类加载器加载的是位于jre/lib目录下的核心类库，其中就有String类，这个String类就在java.lang 中，所以我们自定义的这个String类在最上层的启动类加载器中被核心类库里的String类替换掉了！</p><p>参考文章：<a href="https://blog.csdn.net/m0_38075425/article/details/81627349" target="_blank" rel="noopener">https://blog.csdn.net/m0_38075425/article/details/81627349</a></p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> 类加载机制 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 类加载机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>类加载的过程</title>
      <link href="/2020/01/28/JVM/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B62%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E8%BF%87%E7%A8%8B/"/>
      <url>/2020/01/28/JVM/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B62%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200122204152483.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h2 id="加载"><a href="#加载" class="headerlink" title="加载"></a>加载</h2><ul><li>将类的.class文件中的二进制数据读入到内存中</li><li>将其放在运行时数据区的方法区内</li><li>然后再内存中创建一个java.lang.Class对象用来封装类在方法区内的数据结构</li></ul><p>简单地说，加载：在硬盘上查找并通过IO读入字节码文件</p><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>目的：确保Class文件字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。<br>主要分为四个检验部分：</p><ol><li>文件格式验证：检验是否为class文件</li><li>元数据验证：验证是否符合Java语言规范</li><li>字节码验证：检验程序是否合法，符合逻辑</li><li>符号引用验证：保证该引用能够被访问到</li></ol><p>要注意，验证阶段是非常重要的，但并不是一定必要的，对于反复使用和验证过的代码，可以通过使用-Xverify：none来关闭大部分类验证，来缩短验证时间。</p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>为类变量分配内存，设置类变量初始值</p><p>要注意的是：</p><ul><li>内存分配仅包括类变量（static），不包括实例变量，实例变量会在对象实例化时随对象分配在Java堆中</li><li>初始值在通常情况下是0，并不是程序里的那个值。真正的赋值是在初始化时完成的。<h2 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h2>将<strong>符号引用</strong>替换为<strong>直接引用</strong>，该阶段会把一些<strong>静态方法</strong>替换为指向数据所存内存的指针或句柄等（<strong>直接引用</strong>），这是所谓的<strong>静态链接</strong>的过程（程序加载期间完成），<strong>动态链接</strong>时在程序运行期间完成的<strong>符号引用</strong>替换为<strong>直接引用</strong>。</li></ul><p><strong>符号引用</strong>：用符号来表示目标，符号可以是任意的字面量，要求无歧义</p><p>我们以字节码文档为例：</p><p>将一个简单的代码用javap命令生成字节码文件，内容如下</p><pre><code>package Test;public class Test {    public int computer(){        int a=1;        int b=2;        return a+b;    }    public static void main(String[] args) {        Test test = new Test();        int c = test.computer();        System.out.println(c);    }}</code></pre><p>```</p><pre><code>Classfile /C:/Users/董润泽/workspace/Java虚拟机/src/Test/Test.class  Last modified 2020-1-21; size 490 bytes  MD5 checksum 1015aa0bd5fd4af85cc23a21ef42a1af  Compiled from &quot;Test.java&quot;public class Test.Test  minor version: 0  major version: 52  flags: ACC_PUBLIC, ACC_SUPERConstant pool:   #1 = Methodref          #7.#18         // java/lang/Object.&quot;&lt;init&gt;&quot;:()V   #2 = Class              #19            // Test/Test   #3 = Methodref          #2.#18         // Test/Test.&quot;&lt;init&gt;&quot;:()V   #4 = Methodref          #2.#20         // Test/Test.computer:()I   #5 = Fieldref           #21.#22        // java/lang/System.out:Ljava/io/PrintStream;   #6 = Methodref          #23.#24        // java/io/PrintStream.println:(I)V   #7 = Class              #25            // java/lang/Object   #8 = Utf8               &lt;init&gt;   #9 = Utf8               ()V  #10 = Utf8               Code  #11 = Utf8               LineNumberTable  #12 = Utf8               computer  #13 = Utf8               ()I  #14 = Utf8               main  #15 = Utf8               ([Ljava/lang/String;)V  #16 = Utf8               SourceFile  #17 = Utf8               Test.java  #18 = NameAndType        #8:#9          // &quot;&lt;init&gt;&quot;:()V  #19 = Utf8               Test/Test  #20 = NameAndType        #12:#13        // computer:()I  #21 = Class              #26            // java/lang/System  #22 = NameAndType        #27:#28        // out:Ljava/io/PrintStream;  #23 = Class              #29            // java/io/PrintStream  #24 = NameAndType        #30:#31        // println:(I)V  #25 = Utf8               java/lang/Object  #26 = Utf8               java/lang/System  #27 = Utf8               out  #28 = Utf8               Ljava/io/PrintStream;  #29 = Utf8               java/io/PrintStream  #30 = Utf8               println  #31 = Utf8               (I)V{  public Test.Test();    descriptor: ()V    flags: ACC_PUBLIC    Code:      stack=1, locals=1, args_size=1         0: aload_0         1: invokespecial #1                  // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V         4: return      LineNumberTable:        line 3: 0  public int computer();    descriptor: ()I    flags: ACC_PUBLIC    Code:      stack=2, locals=3, args_size=1         0: iconst_1         1: istore_1         2: iconst_2         3: istore_2         4: iload_1         5: iload_2         6: iadd         7: ireturn      LineNumberTable:        line 5: 0        line 6: 2        line 7: 4  public static void main(java.lang.String[]);    descriptor: ([Ljava/lang/String;)V    flags: ACC_PUBLIC, ACC_STATIC    Code:      stack=2, locals=3, args_size=1         0: new           #2                  // class Test/Test         3: dup         4: invokespecial #3                  // Method &quot;&lt;init&gt;&quot;:()V         7: astore_1         8: aload_1         9: invokevirtual #4                  // Method computer:()I        12: istore_2        13: getstatic     #5                  // Field java/lang/System.out:Ljava/io/PrintStream;        16: iload_2        17: invokevirtual #6                  // Method java/io/PrintStream.println:(I)V        20: return      LineNumberTable:        line 10: 0        line 11: 8        line 12: 13        line 13: 20}SourceFile: &quot;Test.java&quot;</code></pre><p>可以看到，上面有一些“#数字”：</p><p><img src="https://img-blog.csdnimg.cn/20200122220623936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>这其实就是生成的符号，对于后面的解释，例如“#7：#8”即指“#1”这个符号引用了“#7”和“#8”的内容</p><p>这即为符号引用</p><p><strong>直接引用</strong>：引用的是目标的一个地址</p><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>对类的静态变量初始化为指定的值，执行静态代码块</p><p>同准备阶段的赋初始值不同！</p><p>在博客 <a href="https://drunze.github.io/2020/01/28/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B61%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E6%97%B6%E6%9C%BA/" target="_blank" rel="noopener">类加载的时机</a>已介绍，可参考理解。</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> 类加载机制 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 类加载机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>类加载的时机</title>
      <link href="/2020/01/28/JVM/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B61%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E6%97%B6%E6%9C%BA/"/>
      <url>/2020/01/28/JVM/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B61%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E6%97%B6%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p>虚拟机如何加载Class文件？<br>Class文件里的信息进入虚拟机会发生怎样的变化？</p><blockquote><p>虚拟机把描述类的数据从class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是类加载机制</p></blockquote><h2 id="类加载的时机"><a href="#类加载的时机" class="headerlink" title="类加载的时机"></a>类加载的时机</h2><p>类从被加载到内存到卸载出内存，生命周期：<br><strong>加载、连接</strong>（验证、准备、解析）<strong>、初始化、使用、卸载</strong></p><p><strong>初始化：</strong></p><ol><li>遇到new、getstatic、putstatic或invokestatic,若类未初始化，则触发初始化</li><li>使用java.lang.reflect对类进行反射调用</li><li>父类没有进行过初始化，先初始化父类</li><li>虚拟机启动时，先初始化要执行的主类</li><li>若java.lang.invoke.MethodHandle实例最后的解析结果为REF_getStatic、REF_putStatic、REF_invokeStatic的句柄，若此方法句柄对应的类没有进行过初始化，则要先初始化。</li></ol><p>下面以代码来演示</p><pre><code>//父类superClass    public class SuperClass {        static {            System.out.println(&quot;SuperClass init!&quot;);        }    public static int sup = 1111111;    }//子类subClass    public class subClass extends SuperClass{        static {            System.out.println(&quot;SubClass init!&quot;);        }        public static int sub = 2222222;    }</code></pre><p><img src="https://img-blog.csdnimg.cn/20200122152246616.png" alt=""></p><p>guess结果是啥？</p><p>分析，参考上面规则的第三条： <strong>3. 父类没有进行过初始化，先初始化父类</strong></p><p>首先是subClass.sub:要调用子类中的静态变量sub，那就要初始化子类，但是在子类初始化之前，还要去先初始化父类，最后才打印sub的值</p><p>在运行第二句时，两个类已经经过初始化，那么直接打印值即可。</p><p><img src="https://img-blog.csdnimg.cn/20200122152617903.png" alt=""></p><pre><code>public static void main(String[] args) {    SuperClass[] sca = new SuperClass[10];}</code></pre><p>这样，并没有输出superClass init！<br>它代表了元素类型为superClass的一维数组，它是继承于java.lang.Object的子类</p><pre><code>public class ConstClass {    static{        System.out.println(&quot;ConstClass init!&quot;);    }        public static final String Hello = &quot;Hello!&quot;;}public class NotInitialization {    public static void main(String[] args) {        System.out.println(ConstClass.Hello);    }}</code></pre><p>输出什么？</p><p><img src="https://img-blog.csdnimg.cn/20200122185027629.png" alt=""></p><p>Hello！!</p><p>为啥没有执行static里面的语句呢？？？</p><p>这里用了final，即Hello是常量，它被存放在常量池中，和ConstClass这个类没有了联系，主函数在执行时，不需要对ConstClass类进行初始化，所以最终只打印“ Hello！”</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> 类加载机制 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 类加载机制 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
