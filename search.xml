<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>编程作业——tensorflow入门、手势数字识别</title>
      <link href="/2020/02/21/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E2%80%94%E2%80%94tensorflow%E5%85%A5%E9%97%A8%E3%80%81%E6%89%8B%E5%8A%BF%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"/>
      <url>/2020/02/21/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E2%80%94%E2%80%94tensorflow%E5%85%A5%E9%97%A8%E3%80%81%E6%89%8B%E5%8A%BF%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><h1 id="Tensorflow-基础语法"><a href="#Tensorflow-基础语法" class="headerlink" title="Tensorflow 基础语法"></a>Tensorflow 基础语法</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line"><span class="keyword">from</span> tf_utils <span class="keyword">import</span> load_dataset, random_mini_batches, convert_to_one_hot, predict</span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y_hat = tf.constant(<span class="number">36</span>, name=<span class="string">"y_hat"</span>)</span><br><span class="line">y = tf.constant(<span class="number">39</span>, name=<span class="string">"y"</span>)</span><br><span class="line">loss = tf.Variable((y-y_hat)**<span class="number">2</span>, name=<span class="string">"loss"</span>)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    session.run(init)</span><br><span class="line">    print(session.run(loss))</span><br></pre></td></tr></table></figure><pre><code>9</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=tf.constant(<span class="number">2</span>, name=<span class="string">"a"</span>)</span><br><span class="line">b=tf.constant(<span class="number">10</span>, name=<span class="string">"b"</span>)</span><br><span class="line">c=tf.multiply(a, b)</span><br><span class="line">print(c) <span class="comment">#必须要创建并运行session</span></span><br></pre></td></tr></table></figure><pre><code>Tensor(&quot;Mul:0&quot;, shape=(), dtype=int32)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(c))</span><br></pre></td></tr></table></figure><pre><code>20</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=tf.placeholder(tf.int64, name=<span class="string">"x"</span>)</span><br><span class="line">print(sess.run(<span class="number">2</span>*x, feed_dict=&#123;x: <span class="number">3</span>&#125;))</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><pre><code>6</code></pre><h2 id="linear-function"><a href="#linear-function" class="headerlink" title="linear function"></a>linear function</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_function</span><span class="params">()</span>:</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    X=tf.constant(np.random.randn(<span class="number">3</span>,<span class="number">1</span>), name=<span class="string">"X"</span>)</span><br><span class="line">    W=tf.constant(np.random.randn(<span class="number">4</span>,<span class="number">3</span>), name=<span class="string">"W"</span>)</span><br><span class="line">    b=tf.constant(np.random.randn(<span class="number">4</span>,<span class="number">1</span>), name=<span class="string">"b"</span>)</span><br><span class="line">    Y=tf.add(tf.matmul(W, X), b)</span><br><span class="line">    </span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    result = sess.run(Y)</span><br><span class="line">    </span><br><span class="line">    sess.close()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    x = tf.placeholder(tf.float32, name=<span class="string">"x"</span>)</span><br><span class="line">    </span><br><span class="line">    sigmoid = tf.sigmoid(x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        result = sess.run(sigmoid, feed_dict=&#123;x:z&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(logits, labels)</span>:</span></span><br><span class="line">    z=tf.placeholder(tf.float32, name=<span class="string">"z"</span>)</span><br><span class="line">    y=tf.placeholder(tf.float32, name=<span class="string">"y"</span>)</span><br><span class="line">    </span><br><span class="line">    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)</span><br><span class="line">    </span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    </span><br><span class="line">    cost = sess.run(cost, feed_dict=&#123;z:logits, y:labels&#125;)</span><br><span class="line">    </span><br><span class="line">    sess.close()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h2 id="独热编码One-Hot-encodings"><a href="#独热编码One-Hot-encodings" class="headerlink" title="独热编码One Hot encodings"></a>独热编码One Hot encodings</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot_matrix</span><span class="params">(labels, C)</span>:</span></span><br><span class="line">    C = tf.constant(C, name=<span class="string">"C"</span>)</span><br><span class="line">    one_hot_matrix = tf.one_hot(indices=labels, depth=C, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    当axis=0，返回的是（C*features）的矩阵</span></span><br><span class="line"><span class="string">    例如：indices = [2,3,1,6] #长度为features=4的向量</span></span><br><span class="line"><span class="string">    当axis=0/-1，返回的是（features*C）的矩阵</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    </span><br><span class="line">    one_hot = sess.run(one_hot_matrix)</span><br><span class="line">    </span><br><span class="line">    sess.close()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> one_hot</span><br></pre></td></tr></table></figure><h2 id="初始化参数为0-1"><a href="#初始化参数为0-1" class="headerlink" title="初始化参数为0/1"></a>初始化参数为0/1</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ones</span><span class="params">(shape)</span>:</span></span><br><span class="line">    ones = tf.ones(shape)</span><br><span class="line">    </span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    </span><br><span class="line">    ones = sess.run(ones)</span><br><span class="line">    </span><br><span class="line">    sess.close()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ones</span><br></pre></td></tr></table></figure><h1 id="构建手势数字识别"><a href="#构建手势数字识别" class="headerlink" title="构建手势数字识别"></a>构建手势数字识别</h1><p><img src="https://pic3.zhimg.com/80/v2-253cdd03eb6f027233012d9c992530d3_hd.png" alt=""></p><h2 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index = <span class="number">24</span></span><br><span class="line">plt.imshow(X_train_orig[index])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(np.squeeze(Y_train_orig[:, index])))</span><br></pre></td></tr></table></figure><p><img src="https://pic3.zhimg.com/80/v2-e59ec478d9db37fa76972cc1997a61b1_hd.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X_train_flatten = X_train_orig.reshape(X_train_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">X_test_flatten = X_test_orig.reshape(X_test_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line">X_train = X_train_flatten/<span class="number">255</span></span><br><span class="line">X_test = X_test_flatten/<span class="number">255</span></span><br><span class="line"></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>) <span class="comment">#图片的类别总共有6个（数字0-5）</span></span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>)</span><br></pre></td></tr></table></figure><h2 id="创建字符X，Y"><a href="#创建字符X，Y" class="headerlink" title="创建字符X，Y"></a>创建字符X，Y</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_x, n_y)</span>:</span></span><br><span class="line">    X = tf.placeholder(tf.float32, [n_x, <span class="literal">None</span>], name=<span class="string">"X"</span>)</span><br><span class="line">    Y = tf.placeholder(tf.float32, [n_y, <span class="literal">None</span>], name=<span class="string">"Y"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X,Y</span><br></pre></td></tr></table></figure><h2 id="初始化变量"><a href="#初始化变量" class="headerlink" title="初始化变量"></a>初始化变量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    W1 = tf.get_variable(<span class="string">"W1"</span>, [<span class="number">25</span>, <span class="number">12288</span>], initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">    b1 = tf.get_variable(<span class="string">"b1"</span>, [<span class="number">25</span>, <span class="number">1</span>], initializer=tf.zeros_initializer())</span><br><span class="line">    W2 = tf.get_variable(<span class="string">"W2"</span>, [<span class="number">12</span>, <span class="number">25</span>], initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">    b2 = tf.get_variable(<span class="string">"b2"</span>, [<span class="number">12</span>, <span class="number">1</span>], initializer=tf.zeros_initializer())</span><br><span class="line">    W3 = tf.get_variable(<span class="string">"W3"</span>, [<span class="number">6</span>, <span class="number">12</span>], initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">    b3 = tf.get_variable(<span class="string">"b3"</span>, [<span class="number">6</span>, <span class="number">1</span>], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    parmeters = &#123;<span class="string">"W1"</span>:W1,</span><br><span class="line">                <span class="string">"W2"</span>:W2,</span><br><span class="line">                <span class="string">"W3"</span>:W3,</span><br><span class="line">                <span class="string">"b1"</span>:b1,</span><br><span class="line">                <span class="string">"b2"</span>:b2,</span><br><span class="line">                <span class="string">"b3"</span>:b3&#125;</span><br><span class="line">    <span class="keyword">return</span> parmeters</span><br></pre></td></tr></table></figure><h2 id="向前传播"><a href="#向前传播" class="headerlink" title="向前传播"></a>向前传播</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">    W3 = parameters[<span class="string">'W3'</span>]</span><br><span class="line">    b3 = parameters[<span class="string">'b3'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX</span></span><br><span class="line">    </span><br><span class="line">    Z1 = tf.add(tf.matmul(W1, X), b1)</span><br><span class="line">    A1 = tf.nn.relu(Z1)</span><br><span class="line">    Z2 = tf.add(tf.matmul(W2, A1), b2)</span><br><span class="line">    A2 = tf.nn.relu(Z2)</span><br><span class="line">    Z3 = tf.add(tf.matmul(W3, Z2), b3)</span><br><span class="line">    <span class="comment"># 算到Z3就停止，原因是在tensorflow中，最后一个线性层的输出作为计算损耗的函数的输入</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure><h2 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3, Y)</span>:</span></span><br><span class="line">    logits = tf.transpose(Z3)</span><br><span class="line">    labels = tf.transpose(Y)</span><br><span class="line">    </span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h2 id="向后传播，更新参数"><a href="#向后传播，更新参数" class="headerlink" title="向后传播，更新参数"></a>向后传播，更新参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_ , c = sess.run([optimizer, cost], feed_dict=&#123;X: minibatch_X, Y: minibatch_Y&#125;)</span><br></pre></td></tr></table></figure><h2 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, learning_rate = <span class="number">0.0001</span>, num_epochs = <span class="number">1500</span>, minibatch_size = <span class="number">32</span>, print_cost = True)</span>:</span></span><br><span class="line">    ops.reset_default_graph() <span class="comment">#在不覆盖tf变量的情况下重新运行模型</span></span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">    seed = <span class="number">3</span></span><br><span class="line">    (n_x, m) = X_train.shape</span><br><span class="line">    n_y = Y_train.shape[<span class="number">0</span>]</span><br><span class="line">    costs = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#创建Placeholder</span></span><br><span class="line">    X, Y = creat_placeholders(n_x, n_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#初始化参数</span></span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#向前传播</span></span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#计算损失</span></span><br><span class="line">    cost = compute_cost(Z3, Y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#向后传播</span></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化所有变量</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        </span><br><span class="line">        sess.run(init)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            epoch_cost = <span class="number">0</span></span><br><span class="line">            num_minibatches = int(m/minibatch_size)</span><br><span class="line">            seed = seed+<span class="number">1</span></span><br><span class="line">            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line">                (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line">                </span><br><span class="line">                _, minibatch_cost = sess.run([optimizer, cost], feed_dict=&#123;X:minibatch_X, Y:minibatch_Y&#125;)</span><br><span class="line">                </span><br><span class="line">                epoch_cost = epoch_cost+minibatch_cost/num_minibatches</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="literal">True</span> <span class="keyword">and</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> % (epoch, epoch_cost))</span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="literal">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                costs.append(epoch_cost)</span><br><span class="line">            </span><br><span class="line">        plt.plot(np.squeeze(costs))</span><br><span class="line">        plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">        plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">        plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">        plt.show()</span><br><span class="line">        </span><br><span class="line">        parameters = sess.run(parameters)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Parameters have been trained!"</span>)</span><br><span class="line">        </span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))</span><br><span class="line">        </span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Train Accuracy:"</span>, accuracy.eval(&#123;X: X_train, Y: Y_train&#125;))</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Test Accuracy:"</span>, accuracy.eval(&#123;X: X_test, Y: Y_test&#125;))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="开始训练"><a href="#开始训练" class="headerlink" title="开始训练"></a>开始训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(X_train, Y_train, X_test, Y_test)</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From &lt;ipython-input-64-3b18fa0bc2c0&gt;:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.Instructions for updating:Future major versions of TensorFlow will allow gradients to flowinto the labels input on backprop by default.See `tf.nn.softmax_cross_entropy_with_logits_v2`.Cost after epoch 0: 1.895938Cost after epoch 100: 1.400813Cost after epoch 200: 1.255541Cost after epoch 300: 1.181168Cost after epoch 400: 1.114092Cost after epoch 500: 1.073960Cost after epoch 600: 1.032575Cost after epoch 700: 0.977509Cost after epoch 800: 0.937244Cost after epoch 900: 0.886205Cost after epoch 1000: 0.882042Cost after epoch 1100: 0.824795Cost after epoch 1200: 0.818317Cost after epoch 1300: 0.787285Cost after epoch 1400: 0.770850</code></pre><p><img src="https://pic3.zhimg.com/80/v2-15d36fceec8ec2a9abeeafb06789c9c2_hd.png" alt=""></p><pre><code>Parameters have been trained!Train Accuracy: 0.723148Test Accuracy: 0.516667</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.image <span class="keyword">as</span> mping</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">3</span>):</span><br><span class="line">    my_image = str(i)+<span class="string">".jpg"</span></span><br><span class="line"></span><br><span class="line">    fileName = <span class="string">"C:\\Users\\董润泽\\Desktop\\手势数字\\"</span>+my_image</span><br><span class="line">    </span><br><span class="line">    image = mping.imread(fileName)</span><br><span class="line">    </span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    image = cv2.resize(image, (<span class="number">64</span>,<span class="number">64</span>))</span><br><span class="line"></span><br><span class="line">    my_image = image.reshape((<span class="number">1</span>, <span class="number">64</span>*<span class="number">64</span>*<span class="number">3</span>)).T</span><br><span class="line">    my_image_prediction = predict(my_image, parameters)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"Your algorithm predicts: y = "</span> + str(np.squeeze(my_image_prediction)))</span><br></pre></td></tr></table></figure><p><img src="https://pic4.zhimg.com/80/v2-125ae5626d602a77ee5ab5d63b412296_hd.png" alt=""></p><pre><code>Your algorithm predicts: y = 3</code></pre><p><img src="https://pic2.zhimg.com/80/v2-5a37c4077e8d35d521ef9ea46d823ef0_hd.png" alt=""></p><pre><code>Your algorithm predicts: y = 3</code></pre><p><img src="https://pic1.zhimg.com/80/v2-775ed634e753ca61503680526efa56f0_hd.png" alt=""></p><pre><code>Your algorithm predicts: y = 1</code></pre><p>正确率很低啊！！</p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 02改善深层神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编程作业——优化Optimization</title>
      <link href="/2020/02/18/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E2%80%94%E2%80%94%E4%BC%98%E5%8C%96Optimization/"/>
      <url>/2020/02/18/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E2%80%94%E2%80%94%E4%BC%98%E5%8C%96Optimization/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation</span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure><h1 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h1><p><img src="https://pic2.zhimg.com/80/v2-0372babb0724d9532d76fd74dc307cde_hd.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_gd</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    L = len(parameters) // <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)]-learning_rate*grads[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)]-learning_rate*grads[<span class="string">"db"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="Mini-Batch-Gradient-descent"><a href="#Mini-Batch-Gradient-descent" class="headerlink" title="Mini-Batch Gradient descent"></a>Mini-Batch Gradient descent</h1><p><img src="https://pic3.zhimg.com/80/v2-00f6ca656fb6e19b865c4b0bef565920_hd.png" alt=""></p><p><img src="https://pic1.zhimg.com/80/v2-c0ca0a238d53dcd08a708e0634bd505a_hd.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_mini_batches</span><span class="params">(X, Y, mini_batch_size = <span class="number">64</span>, seed = <span class="number">0</span>)</span>:</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    mini_batches = []</span><br><span class="line">    </span><br><span class="line">    permutation = list(np.random.permutation(m)) <span class="comment">#对m个数随机排列</span></span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>,m))</span><br><span class="line">    </span><br><span class="line">    num_complete_minibatches = math.floor(m/mini_batch_size) <span class="comment"># 向上取整</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, num_complete_minibatches):</span><br><span class="line">        mini_batch_X = shuffled_X[:, <span class="number">0</span>:mini_batch_size]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, <span class="number">0</span>:mini_batch_size]</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> m%mini_batch_size != <span class="number">0</span>:</span><br><span class="line">        mini_batch_X = shuffled_X[:, mini_batch_size:<span class="number">2</span>*mini_batch_size]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, mini_batch_size:<span class="number">2</span>*mini_batch_size]</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mini_batches</span><br></pre></td></tr></table></figure><h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h1><p><img src="https://pic2.zhimg.com/80/v2-1a32c6000f98d67ab5ef11d7f231f517_hd.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_velocity</span><span class="params">(parameters)</span>:</span></span><br><span class="line">    L = len(parameters)//<span class="number">2</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = np.zeros(parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape)</span><br><span class="line">        v[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = np.zeros(parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape)</span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_momentum</span><span class="params">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class="line">    L = len(parameters)//<span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = beta*v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]+(<span class="number">1</span>-beta)*grads[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = beta*v[<span class="string">"db"</span>+str(l+<span class="number">1</span>)]+(<span class="number">1</span>-beta)*grads[<span class="string">"db"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        </span><br><span class="line">        parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)]-learning_rate*v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)]-learning_rate*v[<span class="string">"db"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters, v</span><br></pre></td></tr></table></figure><h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p><img src="https://pic2.zhimg.com/80/v2-35d5633a827215aa8b9fe55aacdb6d21_hd.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_adam</span><span class="params">(parameters)</span>:</span></span><br><span class="line">    L = len(parameters)//<span class="number">2</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = np.zeros(parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape)</span><br><span class="line">        v[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = np.zeros(parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape)</span><br><span class="line">        s[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = np.zeros(parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape)</span><br><span class="line">        s[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = np.zeros(parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> v, s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_adam</span><span class="params">(parameters, grads, v, s, t, learning_rate = <span class="number">0.01</span>, beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>, epsilon = <span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    L = len(parameters)//<span class="number">2</span></span><br><span class="line">    v_corrected = &#123;&#125;</span><br><span class="line">    s_corrected = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = beta1 * v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta1)*grads[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta1 * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta1)*grads[<span class="string">"db"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        </span><br><span class="line">        s[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = beta2 * s[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta2)*grads[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]*grads[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta2 * s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta2)*np.power(grads[<span class="string">"db"</span>+str(l+<span class="number">1</span>)], <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        v_corrected[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]/(<span class="number">1</span>-np.power(beta1, t))</span><br><span class="line">        v_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]/(<span class="number">1</span>-np.power(beta1, t))</span><br><span class="line">        </span><br><span class="line">        s_corrected[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = s[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]/(<span class="number">1</span>-np.power(beta1, t))</span><br><span class="line">        s_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]/(<span class="number">1</span>-np.power(beta2, t))</span><br><span class="line">        </span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate*v_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]/(np.sqrt(s_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)])+epsilon)</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate*v_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]/(np.sqrt(s_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)])+epsilon)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters, v, s</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y = load_dataset()</span><br></pre></td></tr></table></figure><p><img src="https://pic1.zhimg.com/80/v2-52aa8a3b8f709a715276859729270621_hd.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, layers_dims, optimizer, learning_rate = <span class="number">0.0007</span>, mini_batch_size = <span class="number">64</span>, beta = <span class="number">0.9</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">          beta1 = <span class="number">0.9</span>, beta2 = <span class="number">0.999</span>,  epsilon = <span class="number">1e-8</span>, num_epochs = <span class="number">10000</span>, print_cost = True)</span>:</span></span><br><span class="line">   </span><br><span class="line">    L = len(layers_dims)</span><br><span class="line">    costs = []</span><br><span class="line">    t = <span class="number">0</span></span><br><span class="line">    seed = <span class="number">10</span></span><br><span class="line">    </span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># no initialization required for gradient descent</span></span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">        v = initialize_velocity(parameters)</span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">        v, s = initialize_adam(parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        </span><br><span class="line">        seed = seed + <span class="number">1</span></span><br><span class="line">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Select a minibatch</span></span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward propagation</span></span><br><span class="line">            a3, caches = forward_propagation(minibatch_X, parameters)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute cost</span></span><br><span class="line">            cost = compute_cost(a3, minibatch_Y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backward propagation</span></span><br><span class="line">            grads = backward_propagation(minibatch_X, minibatch_Y, caches)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update parameters</span></span><br><span class="line">            <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">                parameters = update_parameters_with_gd(parameters, grads, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">                t = t + <span class="number">1</span> <span class="comment"># Adam counter</span></span><br><span class="line">                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,</span><br><span class="line">                                                               t, learning_rate, beta1, beta2,  epsilon)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 epoch</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">                </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'epochs (per 100)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="Mini-batch-Gradient-descent"><a href="#Mini-batch-Gradient-descent" class="headerlink" title="Mini-batch Gradient descent"></a>Mini-batch Gradient descent</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"gd"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Gradient Descent optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><pre><code>Cost after epoch 0: 0.708983Cost after epoch 1000: 0.659841Cost after epoch 2000: 0.628205Cost after epoch 3000: 0.571343Cost after epoch 4000: 0.593195Cost after epoch 5000: 0.503812Cost after epoch 6000: 0.507677Cost after epoch 7000: 0.494559Cost after epoch 8000: 0.459184Cost after epoch 9000: 0.399013</code></pre><p><img src="https://pic3.zhimg.com/80/v2-18c0b816b7976c11fda0beb34083aa30_hd.png" alt=""></p><pre><code>Accuracy: 0.7966666666666666</code></pre><p><img src="https://pic4.zhimg.com/80/v2-dde014b8736da3c504a4f609ee221114_hd.png" alt=""></p><h1 id="Mini-batch-gradient-descent-with-momentum"><a href="#Mini-batch-gradient-descent-with-momentum" class="headerlink" title="Mini-batch gradient descent with momentum"></a>Mini-batch gradient descent with momentum</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, beta = <span class="number">0.9</span>, optimizer = <span class="string">"momentum"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Momentum optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><pre><code>Cost after epoch 0: 0.709029Cost after epoch 1000: 0.659953Cost after epoch 2000: 0.628344Cost after epoch 3000: 0.571453Cost after epoch 4000: 0.593252Cost after epoch 5000: 0.503935Cost after epoch 6000: 0.507794Cost after epoch 7000: 0.494631Cost after epoch 8000: 0.459387Cost after epoch 9000: 0.399227</code></pre><p><img src="https://pic3.zhimg.com/80/v2-18c0b816b7976c11fda0beb34083aa30_hd.png" alt=""></p><pre><code>Accuracy: 0.7966666666666666</code></pre><p><img src="https://pic4.zhimg.com/80/v2-dde014b8736da3c504a4f609ee221114_hd.png" alt=""></p><h1 id="Mini-batch-with-Adam-mode"><a href="#Mini-batch-with-Adam-mode" class="headerlink" title="Mini-batch with Adam mode"></a>Mini-batch with Adam mode</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"adam"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Adam optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><pre><code>Cost after epoch 0: 0.700736Cost after epoch 1000: 0.171262Cost after epoch 2000: 0.119976Cost after epoch 3000: 0.258917Cost after epoch 4000: 0.079703Cost after epoch 5000: 0.119927Cost after epoch 6000: 0.150617Cost after epoch 7000: 0.131514Cost after epoch 8000: 0.159743Cost after epoch 9000: 0.057505</code></pre><p><img src="https://pic2.zhimg.com/80/v2-6801ff58b728dacee105d72b48ca5c01_hd.png" alt=""></p><pre><code>Accuracy: 0.93</code></pre><p><img src="https://pic1.zhimg.com/80/v2-4013d2ae18277dbcb69e1b1529fecd6d_hd.png" alt=""></p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><table>     <tr>        <td>        **optimization method**        </td>        <td>        **accuracy**        </td>        <td>        **cost shape**        </td>    </tr>        <td>        Gradient descent        </td>        <td>        79.7%        </td>        <td>        oscillations        </td>    <tr>        <td>        Momentum        </td>        <td>        79.7%        </td>        <td>        oscillations        </td>    </tr>    <tr>        <td>        Adam        </td>        <td>        94%        </td>        <td>        smoother        </td>    </tr></table> ]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 02改善深层神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3超参数调试、batch正则化、程序框架</title>
      <link href="/2020/02/18/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/3%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81batch%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%81%E7%A8%8B%E5%BA%8F%E6%A1%86%E6%9E%B6/"/>
      <url>/2020/02/18/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/3%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81batch%E6%AD%A3%E5%88%99%E5%8C%96%E3%80%81%E7%A8%8B%E5%BA%8F%E6%A1%86%E6%9E%B6/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><p>原文：<a href="https://zhuanlan.zhihu.com/p/30146018" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/30146018</a></p><h1 id="1、超参数调试处理"><a href="#1、超参数调试处理" class="headerlink" title="1、超参数调试处理"></a><strong>1、超参数调试处理</strong></h1><ul><li>在机器学习领域，超参数比较少的情况下，我们之前利用设置网格点的方式来调试超参数；</li><li>但在深度学习领域，超参数较多的情况下，不是设置规则的网格点，而是随机选择点进行调试。这样做是因为在我们处理问题的时候，是无法知道哪个超参数是更重要的，所以随机的方式去测试超参数点的性能，更为合理，这样可以探究更超参数的潜在价值。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-e7efb36483524c45c51c451c8b136493_hd.jpg" alt=""></p><h1 id="2、为超参数选择合适范围"><a href="#2、为超参数选择合适范围" class="headerlink" title="2、为超参数选择合适范围"></a><strong>2、为超参数选择合适范围</strong></h1><p><strong>Scale均匀随机</strong></p><p>在超参数选择的时候，一些超参数是在一个范围内进行均匀随机取值，如隐藏层神经元结点的个数、隐藏层的层数等。但是有一些超参数的选择做均匀随机取值是不合适的，这里需要按照一定的比例在不同的小范围内进行均匀随机取值，以学习率 <strong>α</strong> 的选择为例，在 <strong>0.001,…1</strong> 范围内进行选择：</p><p><img src="https://pic1.zhimg.com/80/v2-5f1dced5e43bc1c3e63676a066441374_hd.jpg" alt=""></p><p>如上图所示，如果在 <strong>0.001,…1</strong> 的范围内进行进行均匀随机取值，则有90%的概率 选择范围在 <strong>0.1,…1</strong> 之间，而只有10%的概率才能选择到 <strong>0.001,…1</strong> 之间，显然是不合理的。</p><p>所以在选择的时候，在不同比例范围内进行均匀随机取值，如 <strong>0.001,…0.001</strong> 、 <strong>0.001,…0.01</strong> 、 <strong>0.01,…0.1</strong> 、 <strong>0.1,…1</strong> 范围内选择。</p><p>代码实现</p><pre><code>r = -4 * np.random.rand()     # r in [-4,0]learning_rate = 10 ** r      # 10^{r}</code></pre><p>一般地，如果在 <img src="https://www.zhihu.com/equation?tex=10%5E%7Ba%7D%5Csim10%5E%7Bb%7D" alt=""> 之间的范围内进行按比例的选择，则 r∈[a,b] ， <img src="https://www.zhihu.com/equation?tex=%5Calpha+%3D+10%5E%7Br%7D" alt=""> 。</p><p>同样，在使用指数加权平均的时候，超参数 β 也需要用上面这种方向进行选择。</p><h1 id="3、超参数调试实践–Pandas-vs-Caviar"><a href="#3、超参数调试实践–Pandas-vs-Caviar" class="headerlink" title="3、超参数调试实践–Pandas vs. Caviar"></a><strong>3、超参数调试实践–Pandas vs. Caviar</strong></h1><p>在超参数调试的实际操作中，我们需要根据我们现有的计算资源来决定以什么样的方式去调试超参数，进而对模型进行改进。下面是不同情况下的两种方式：</p><p><img src="https://pic2.zhimg.com/80/v2-9db64788ca9dd9065937217bffba1cd1_hd.jpg" alt=""></p><ul><li>在计算资源有限的情况下，使用第一种，仅调试一个模型，每天不断优化；</li><li>在计算资源充足的情况下，使用第二种，同时并行调试多个模型，选取其中最好的模型。</li></ul><h1 id="网络中激活值的归一化"><a href="#网络中激活值的归一化" class="headerlink" title="网络中激活值的归一化"></a><strong>网络中激活值的归一化</strong></h1><p>在Logistic Regression 中，将输入特征进行归一化，可以加速模型的训练。那么对于更深层次的神经网络，我们是否可以归一化隐藏层的输出 a[l] 或者经过激活函数前的 z[l] ，以便加速神经网络的训练过程？答案是肯定的。</p><p>常用的方式是将隐藏层的经过激活函数前的 z[l] 进行归一化。</p><p><strong>Batch Norm的实现</strong></p><p><img src="https://pic3.zhimg.com/80/v2-b9def12d03c9b858f9c0e48dbfe9422d_hd.png" alt=""></p><h1 id="5、在圣经网络中融入Batch-Norm"><a href="#5、在圣经网络中融入Batch-Norm" class="headerlink" title="5、在圣经网络中融入Batch Norm"></a><strong>5、在圣经网络中融入Batch Norm</strong></h1><p><img src="https://pic3.zhimg.com/80/v2-d8039b96e3fab8d90614b7713e1b7282_hd.jpg" alt=""></p><p><img src="https://pic1.zhimg.com/80/v2-27bcb7a6db75a3d8bd55fd57fc0da793_hd.png" alt=""></p><h1 id="6、Batch-Norm起作用原因"><a href="#6、Batch-Norm起作用原因" class="headerlink" title="6、Batch Norm起作用原因"></a><strong>6、Batch Norm起作用原因</strong></h1><p><strong>First Reason</strong></p><p>首先Batch Norm 可以加速神经网络训练的原因和输入层的输入特征进行归一化，从而改变Cost function的形状，使得每一次梯度下降都可以更快的接近函数的最小值点，从而加速模型训练过程的原理是有相同的道理。</p><p>只是Batch Norm 不是单纯的将输入的特征进行归一化，而是将各个隐藏层的激活函数的激活值进行的归一化，并调整到另外的分布。</p><p><strong>Second Reason</strong></p><p>Batch Norm 可以加速神经网络训练的另外一个原因是它可以使权重比网络更滞后或者更深层。</p><p>下面是一个判别是否是猫的分类问题，假设第一训练样本的集合中的猫均是黑猫，而第二个训练样本集合中的猫是各种颜色的猫。如果我们将第二个训练样本直接输入到用第一个训练样本集合训练出的模型进行分类判别，那么我们在很大程度上是无法保证能够得到很好的判别结果。</p><p>这是因为第一个训练集合中均是黑猫，而第二个训练集合中各色猫均有，虽然都是猫，但是很大程度上样本的分布情况是不同的，所以我们无法保证模型可以仅仅通过黑色猫的样本就可以完美的找到完整的决策边界。第二个样本集合相当于第一个样本的分布的改变，称为：Covariate shift。如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-72b393411080e18a0bcc2b02c74bf6d8_hd.jpg" alt=""></p><p>那么存在Covariate shift的问题如何应用在神经网络中？就是利用Batch Norm来实现。如下面的网络结构：</p><p><img src="https://pic1.zhimg.com/80/v2-2233827f3f5f56c486c763b4203ebf58_hd.jpg" alt=""></p><p>网络的目的是通过不断的训练，最后输出一个更加接近于真实值的 y_hat 。现在以第2个隐藏层为输入来看：</p><p><img src="https://pic2.zhimg.com/80/v2-81ead15d0cbf78c9d037190d7ab118fd_hd.jpg" alt=""></p><p><img src="https://pic4.zhimg.com/80/v2-fd080eef106a757e0dbbbc7e0e19fd76_hd.png" alt=""></p><p><strong>Batch Norm 正则化效果</strong></p><p>Batch Norm还有轻微的正则化效果。</p><p>这是因为在使用Mini-batch梯度下降的时候，每次计算均值和偏差都是在一个Mini-batch上进行计算，而不是在整个数据样集上。这样就在均值和偏差上带来一些比较小的噪声。那么用均值和偏差计算得到的<img src="https://www.zhihu.com/equation?tex=%5Cwidetilde+z%5E%7B%5Bl%5D%7D" alt="">也将会加入一定的噪声。</p><p>所以和Dropout相似，其在每个隐藏层的激活值上加入了一些噪声，（这里因为Dropout以一定的概率给神经元乘上0或者1）。所以和Dropout相似，Batch Norm 也有轻微的正则化效果。</p><p>这里引入一个小的细节就是，如果使用Batch Norm ，那么使用大的Mini-batch如256，相比使用小的Mini-batch如64，会引入跟少的噪声，那么就会减少正则化的效果。</p><h1 id="7、在测试数据上使用batch-norm"><a href="#7、在测试数据上使用batch-norm" class="headerlink" title="7、在测试数据上使用batch norm"></a><strong>7、在测试数据上使用batch norm</strong></h1><p><img src="https://pic2.zhimg.com/80/v2-25c18b55001698e68797cd398f6e8084_hd.png" alt=""></p><h1 id="8、Softmax回归"><a href="#8、Softmax回归" class="headerlink" title="8、Softmax回归"></a><strong>8、Softmax回归</strong></h1><p><img src="https://pic1.zhimg.com/80/v2-0e04c04ae886a0826602715b43269ff5_hd.png" alt=""></p><p>例如要识别这一组图片中的四个类别，即多分类问题，就要用到Softmax回归。</p><p><strong>Softmax回归可以将多分类任务的输出转换为各个类别可能的概率，从而将最大的概率值所对应的类别作为输入样本的输出类别。</strong></p><p>例：</p><p><img src="https://pic4.zhimg.com/80/v2-066432f24abbc42211cd330feb2b3317_hd.jpg" alt=""></p><p>可以看出Softmax通过向量 z[l] 计算出总和为1的四个概率。最后会输出概率最大的那一类</p><p><img src="https://pic2.zhimg.com/80/v2-fa33a0eeac239892d525dfe0b12931ad_hd.jpg" alt=""></p><h1 id="9、训练Softmax分类器"><a href="#9、训练Softmax分类器" class="headerlink" title="9、训练Softmax分类器"></a><strong>9、训练Softmax分类器</strong></h1><p>Loss Function：</p><p><img src="https://pic3.zhimg.com/80/v2-282c33f9aa4c9dad873a0b66c01f17f2_hd.png" alt=""></p><p><img src="https://pic3.zhimg.com/80/v2-fb7d5ab992c6616387db96eb8e6d8da4_hd.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 02改善深层神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2优化算法</title>
      <link href="/2020/02/17/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
      <url>/2020/02/17/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/2%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><p>参考文章： <a href="https://zhuanlan.zhihu.com/p/30034654" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/30034654</a></p><h1 id="1、Mini-batch-梯度下降"><a href="#1、Mini-batch-梯度下降" class="headerlink" title="1、Mini-batch 梯度下降"></a><strong>1、Mini-batch 梯度下降</strong></h1><h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a><strong>理解</strong></h3><p>我们前面已经知道，向量化可以加快我们的模型处理速度。</p><p>所以可以把训练样本放到矩阵中去：<img src="https://pic3.zhimg.com/80/v2-92a544803f558167f738fad3901ec8be_hd.png" alt=""><br><img src="https://pic1.zhimg.com/80/v2-87408265263077e872a8a8e6bff11554_hd.png" alt=""></p><p>但是当训练样本数m非常大，在对整个训练集执行梯度下降算法时，必须处理完整个训练集，然后进行下一步梯度下降。如果在处理完m个训练集之前，就让一部分训练集进行梯度下降，算法的速度就会执行的更快。</p><h3 id="不同size大小比较"><a href="#不同size大小比较" class="headerlink" title="不同size大小比较"></a><strong>不同size大小比较</strong></h3><p><img src="https://pic2.zhimg.com/80/v2-d7374a43267473cd13bfec8764a32471_hd.jpg" alt=""></p><ul><li><p><strong>batch梯度下降</strong></p><ol><li>对所有m个样本执行一次梯度下降，每一次迭代时间长</li><li>Cost function 总是向减小的方向下降。</li></ol></li><li><p><strong>随机梯度下降</strong></p><ol><li>对每一个训练样本执行一次梯度下降，但是丢失了向量化带来的计算加速；</li><li>Cost function总体的趋势向最小值的方向下降，但是无法到达全局最小值点，呈现波动的形式。</li></ol></li><li><p><strong>Mini-batch梯度下降</strong></p><ol><li>选择一个 <img src="https://www.zhihu.com/equation?tex=1%3Csize%3Cm" alt=""> 的合适的size进行Mini-batch梯度下降，可以实现快速学习，也应用了向量化带来的好处；</li><li>Cost function的下降处于前两者之间。</li></ol></li></ul><h3 id="Mini-batch-size选择："><a href="#Mini-batch-size选择：" class="headerlink" title="Mini-batch size选择："></a><strong>Mini-batch size选择：</strong></h3><p><img src="https://pic1.zhimg.com/80/v2-2cdd92c6706eaf29b80694c1213804bc_hd.jpg" alt=""></p><ul><li>训练样本比较小(&lt;2000):<strong>batch梯度下降法</strong></li><li>当训练样本比较大时，一般的mini-batch大小选择为：64-512，且batch大小为2的n次方</li><li>要确保mimi-batch大小符合CPU/GPU内存</li></ul><h1 id="2、指数加权平均"><a href="#2、指数加权平均" class="headerlink" title="2、指数加权平均"></a><strong>2、指数加权平均</strong></h1><p><img src="https://www.zhihu.com/equation?tex=v_%7Bt%7D+%3D+%5Cbeta+v_%7Bt-1%7D%2B%281-%5Cbeta%29%5Ctheta_%7Bt%7D" alt=""></p><p>对于下面这个关于天数和温度的散点图：</p><p><img src="https://pic3.zhimg.com/80/v2-2969f0fa8cfb79021856d26e2c269306_hd.jpg" alt=""></p><ul><li>当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta+%3D0.9" alt=""> 时，指数加权平均最后的结果如图中红色线所示；</li><li>当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta+%3D0.98" alt=""> 时，指数加权平均最后的结果如图中绿色线所示；</li><li>当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta+%3D0.5" alt=""> 时，指数加权平均最后的结果如下图中黄色线所示；</li></ul><p><img src="https://pic4.zhimg.com/80/v2-44d786b0b58dba00c780e398259a58a7_hd.jpg" alt=""></p><p><strong>有个问题，是不是到最后一天的时候，温度还会受到第一天的影响？</strong></p><h3 id="理解指数加权平均"><a href="#理解指数加权平均" class="headerlink" title="理解指数加权平均"></a><strong>理解指数加权平均</strong></h3><ul><li>例子，当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta+%3D0.9" alt=""> 时：</li></ul><p><img src="https://www.zhihu.com/equation?tex=v_%7B100%7D+%3D+0.9v_%7B99%7D%2B0.1%5Ctheta_%7B100%7D%5C%5Cv_%7B99%7D+%3D+0.9v_%7B98%7D%2B0.1%5Ctheta_%7B99%7D%5C%5Cv_%7B98%7D+%3D+0.9v_%7B97%7D%2B0.1%5Ctheta_%7B98%7D%5C%5C+%5Cldots" alt=""></p><p>展开，有：</p><p><img src="https://www.zhihu.com/equation?tex=v_%7B100%7D%3D0.1%5Ctheta_%7B100%7D%2B0.9%280.1%5Ctheta_%7B99%7D%2B0.9%280.1%5Ctheta_%7B98%7D%2B0.9v_%7B97%7D%29%29%5C%5C%3D0.1%5Ctheta_%7B100%7D%2B0.1%5Ctimes0.9%5Ctheta_%7B99%7D%2B0.1%5Ctimes%280.9%29%5E%7B2%7D%5Ctheta_%7B98%7D%2B0.1%5Ctimes%280.9%29%5E%7B3%7D%5Ctheta_%7B97%7D%2B%5Ccdots" alt=""></p><p>上式中所有 θ 前面的系数相加起来为1或者接近于1，称之为偏差修正。</p><p>总体来说存在， <img src="https://www.zhihu.com/equation?tex=%281-%5Cvarepsilon%29%5E%7B1%2F%5Cvarepsilon%7D%3D%5Cdfrac%7B1%7D%7Be%7D" alt=""> ，在我们的例子中， <img src="https://www.zhihu.com/equation?tex=1-%5Cvarepsilon%3D%5Cbeta%3D0.9" alt=""> ，即 <img src="https://www.zhihu.com/equation?tex=0.9%5E%7B10%7D%5Capprox+0.35%5Capprox%5Cdfrac%7B1%7D%7Be%7D" alt=""> 。相当于大约10天后，系数的峰值（这里是0.1）下降到原来的 <strong>1/e</strong> ，只关注了过去10天的天气。</p><ul><li>同样，对于 <img src="https://www.zhihu.com/equation?tex=%5Cbeta+%3D0.98" alt=""> ，0.98的50次方约等于1/e，所以这本质上是一个下降幅度很大的函数，可以看作平均了50天</li></ul><p><img src="https://www.zhihu.com/equation?tex=v_%7B0%7D+%3D0%5C%5C+v_%7B1%7D%3D+%5Cbeta+v_%7B0%7D%2B%281-%5Cbeta%29%5Ctheta_%7B1%7D%5C%5C+v_%7B2%7D%3D+%5Cbeta+v_%7B1%7D%2B%281-%5Cbeta%29%5Ctheta_%7B2%7D%5C%5C+v_%7B3%7D%3D+%5Cbeta+v_%7B2%7D%2B%281-%5Cbeta%29%5Ctheta_%7B3%7D%5C%5C+%5Cldots" alt=""></p><ul><li>在计算当前时刻的平均值，只需要前一天的平均值和当前时刻的值，所以在数据量非常大的情况下，指数加权平均在节约计算成本的方面是一种非常有效的方式，可以很大程度上减少计算机资源存储和内存的占用。</li></ul><h3 id="指数加权平均的偏差修正"><a href="#指数加权平均的偏差修正" class="headerlink" title="指数加权平均的偏差修正"></a><strong>指数加权平均的偏差修正</strong></h3><p>在我们执行指数加权平均的公式时，当 <img src="https://www.zhihu.com/equation?tex=%5Cbeta%3D0.98" alt=""> 时，我们得到的并不是图中的绿色曲线，而是下图中的紫色曲线，其起点比较低。</p><p><img src="https://pic3.zhimg.com/80/v2-e52c3c3cebde2ec15f3f055fcbca7faa_hd.jpg" alt=""></p><p>这是因为受到初始值的影响过大</p><p><img src="https://www.zhihu.com/equation?tex=v_%7B0%7D%3D0%5C%5Cv_%7B1%7D%3D0.98v_%7B0%7D%2B0.02%5Ctheta_%7B1%7D%3D0.02%5Ctheta_%7B1%7D%5C%5Cv_%7B2%7D%3D0.98v_%7B1%7D%2B0.02%5Ctheta_%7B2%7D%3D0.98%5Ctimes0.02%5Ctheta_%7B1%7D%2B0.02%5Ctheta_%7B2%7D%3D0.0196%5Ctheta_%7B1%7D%2B0.02%5Ctheta_%7B2%7D" alt=""></p><p>若第一天的值θ1=40，那么<strong>v1 = 0.02*40 = 8</strong></p><p><strong>偏差修正</strong></p><p>在评估初期，使用：</p><p><img src="https://pic2.zhimg.com/80/v2-4bb00cbd9b7599735a1f9c6734c9ae88_hd.png" alt=""></p><p>例：</p><p>使用 <img src="https://www.zhihu.com/equation?tex=v_%7B1%7D%3D0.02%5Ctimes40%3D8" alt=""></p><p>当 t=2 时：</p><p><img src="https://www.zhihu.com/equation?tex=+1-%5Cbeta%5E%7Bt%7D%3D1-%280.98%29%5E%7B2%7D%3D0.0396" alt=""></p><p><img src="https://www.zhihu.com/equation?tex=+%5Cdfrac%7Bv_%7B2%7D%7D%7B0.0396%7D%3D%5Cdfrac%7B0.0196%5Ctheta_%7B1%7D%2B0.02%5Ctheta_%7B2%7D%7D%7B0.0396%7D" alt=""></p><p>偏差修正得到了绿色的曲线，在开始的时候，能够得到比紫色曲线更好的计算平均的效果。随着 <img src="https://www.zhihu.com/equation?tex=t" alt=""> 逐渐增大， <img src="https://www.zhihu.com/equation?tex=%5Cbeta%5E%7Bt%7D" alt=""> 接近于0，所以后面绿色的曲线和紫色的曲线逐渐重合了。</p><p>但在实际中，可忽略偏差的影响。</p><h1 id="3、动量（Momentum）梯度下降法"><a href="#3、动量（Momentum）梯度下降法" class="headerlink" title="3、动量（Momentum）梯度下降法"></a><strong>3、动量（Momentum）梯度下降法</strong></h1><p>核心就是利用指数加权平均，把b变小，即纵轴波动变小。</p><p><img src="https://pic2.zhimg.com/80/v2-4e37ac10a00135412175a0ba85103035_hd.jpg" alt=""></p><p>如图所示，从外围的蓝色点开始要经过很多次上下波动最终才能到达中心的红色点，就是因为这种上下波动减慢了梯度下降法的速度。</p><p>倘若我们加快学习速率α，就会导致紫色线的出现。如果减小α，虽然会减小波动，但是在横坐标上的移动也会减慢。</p><p>我们希望在图中的纵轴方向梯度下降的缓慢一些，不要有如此大的上下波动，在横轴方向梯度下降的快速一些，使得能够更快的到达最小值点，而这里用动量梯度下降法既可以实现，如红色线所示。</p><p><img src="https://pic4.zhimg.com/80/v2-070324bcdbf16b9104872c2a63b21e3b_hd.jpg" alt=""></p><p><strong>β的常用值为0.9</strong></p><p>在我们进行动量梯度下降算法的时候，由于使用了指数加权平均的方法。原来在<strong>纵轴方向上的上下波动，经过平均以后，接近于0</strong>，纵轴上的波动变得非常的小；但在<strong>横轴方向上，所有的微分都指向横轴方向</strong>，因此其平均值仍然很大。最终实现红色线所示的梯度下降曲线。</p><p><strong>算法本质解释</strong></p><p>在对应上面的计算公式中，将Cost function想象为一个碗状，想象从顶部往下滚球，其中：</p><p>微分项 <img src="https://www.zhihu.com/equation?tex=dw%2Cdb" alt=""> 想象为球提供的加速度；<br>动量项 <img src="https://www.zhihu.com/equation?tex=v_%7Bdw%7D%2Cv_%7Bdb%7D" alt=""> 相当于速度；<br>小球在向下滚动的过程中，因为加速度的存在使得速度会变快，但是由于 β 的存在，其值小于1，可以认为是摩擦力，所以球不会无限加速下去。</p><h1 id="4、RMSprop（root-mean-square-prop）"><a href="#4、RMSprop（root-mean-square-prop）" class="headerlink" title="4、RMSprop（root mean square prop）"></a><strong>4、RMSprop（root mean square prop）</strong></h1><p><img src="https://pic4.zhimg.com/80/v2-d9b0e716021f96c12ec478c8d4629b4f_hd.jpg" alt=""></p><p><img src="https://pic3.zhimg.com/80/v2-b65658c0e12dd65f73743b94eb0ac060_hd.png" alt=""></p><p><img src="https://pic3.zhimg.com/80/v2-3aee4fdd16944225b3552c2053ee48ae_hd.png" alt=""></p><p><img src="https://pic2.zhimg.com/80/v2-bbc08bc4695c041db0e585274b2faf5d_hd.png" alt=""></p><p>先看最后两个计算w、b的公式：我们希望在横轴即w上变化加快，在纵轴即b波动变小，则要 <strong>Sdw</strong> 变小，要 <strong>Sdb</strong> 变大。</p><p>刚好，上面的两个公式db的波动较大，就会导致Sdb变大，进而抑制其在纵轴的波动。</p><p>这时候我们可以加大α提高速率。</p><p>RMSprop将微分项进行平方，然后使用平方根进行梯度更新，同时为了确保算法不会除以0，平方根分母中在实际使用会加入一个很小的值如 <img src="https://www.zhihu.com/equation?tex=%5Cvarepsilon%3D10%5E%7B-8%7D" alt=""> 。</p><h1 id="5、Adam优化算法"><a href="#5、Adam优化算法" class="headerlink" title="5、Adam优化算法"></a><strong>5、Adam优化算法</strong></h1><p>实质就是把动量梯度下降和RMSprop相结合</p><p><img src="https://pic2.zhimg.com/80/v2-d8b2cbfd699148cde0450425b748e059_hd.png" alt=""></p><p><img src="https://pic2.zhimg.com/80/v2-aa51ad83b3950a787ea3aa37733d7d91_hd.png" alt=""></p><h1 id="6、学习率衰减"><a href="#6、学习率衰减" class="headerlink" title="6、学习率衰减"></a><strong>6、学习率衰减</strong></h1><p><img src="https://pic1.zhimg.com/80/v2-0b54853b610c36c030c124606cea1d5c_hd.jpg" alt=""></p><p>如图所示，当学习速率不变时（蓝色），在到达最小值点时，会在一个较大的范围内波动，进而使误差比较大。而若让学习速率衰减（青色），到最小点时，会控制在一个较小的范围内波动</p><p><img src="https://pic2.zhimg.com/80/v2-9e97dce643829f779e107d4c21995675_hd.png" alt=""></p><h1 id="7、局部最优问题"><a href="#7、局部最优问题" class="headerlink" title="7、局部最优问题"></a><strong>7、局部最优问题</strong></h1><p>在低维度的情形下，我们可能会想象到一个Cost function 如左图所示，存在一些局部最小值点，在初始化参数的时候，如果初始值选取的不得当，会存在陷入局部最优点的可能性。</p><p>但是，如果我们建立一个高维度的神经网络。通常梯度为零的点，并不是如左图中的局部最优点，而是右图中的鞍点（叫鞍点是因为其形状像马鞍的形状）。</p><p><img src="https://pic4.zhimg.com/80/v2-ec0c95cf84b7d2ca878bcd51b4485707_hd.jpg" alt=""></p><p>在一个具有高维度空间的函数中，如果梯度为0，那么在每个方向，Cost function可能是凸函数，也有可能是凹函数。但如果参数维度为2万维，想要得到局部最优解，那么所有维度均需要是凹函数，其概率为 <img src="https://www.zhihu.com/equation?tex=2%5E%7B-20000%7D" alt=""> ，可能性非常的小。也就是说，在低维度中的局部最优点的情况，并不适用于高维度，在梯度为0的点更有可能是鞍点，而不是局部最小值点。</p><p>在高纬度的情况下：</p><p>几乎不可能陷入局部最小值点；<br>处于鞍点的停滞区会减缓学习过程，利用如Adam等算法进行改善。</p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 02改善深层神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深层神经网络编程作业2</title>
      <link href="/2020/02/16/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A2/"/>
      <url>/2020/02/16/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A2/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><h1 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils_v3 <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>]=(<span class="number">5.0</span>,<span class="number">4.0</span>)</span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>]=<span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br><span class="line"></span><br><span class="line">m_train = train_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_x_orig.shape[<span class="number">1</span>]</span><br><span class="line">m_test = test_x_orig.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h1 id="改变数据形状"><a href="#改变数据形状" class="headerlink" title="改变数据形状"></a>改变数据形状</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line">train_x = train_x_flatten/<span class="number">255</span></span><br><span class="line">test_x = test_x_flatten/<span class="number">255</span></span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200213203651700.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""><br><img src="https://img-blog.csdnimg.cn/20200213203743489.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""><br><img src="https://img-blog.csdnimg.cn/2020021320380990.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="">)<img src="https://img-blog.csdnimg.cn/20200213203824165.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="初始化每层神经元数"><a href="#初始化每层神经元数" class="headerlink" title="初始化每层神经元数"></a>初始化每层神经元数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n_x = num_px*num_px*<span class="number">3</span></span><br><span class="line">n_h = <span class="number">7</span></span><br><span class="line">n_y = <span class="number">1</span></span><br><span class="line">layers_dims = (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><h1 id="双层神经网络模型"><a href="#双层神经网络模型" class="headerlink" title="双层神经网络模型"></a>双层神经网络模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layer_dims, learning_rate = <span class="number">0.0075</span>, num_iterations=<span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []</span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (n_x, n_h, n_y) = layer_dims</span><br><span class="line">    </span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    </span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        <span class="comment">#向前传播</span></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: "X, W1, b1, W2, b2". Output: "A1, cache1, A2, cache2".</span></span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, <span class="string">"relu"</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, <span class="string">"sigmoid"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#损失函数</span></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#向后传播</span></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line">        </span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, <span class="string">"sigmoid"</span>)</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, <span class="string">"relu"</span>)</span><br><span class="line">        </span><br><span class="line">        grads[<span class="string">"dW1"</span>] = dW1</span><br><span class="line">        grads[<span class="string">"dW2"</span>] = dW2</span><br><span class="line">        grads[<span class="string">"db1"</span>] = db1</span><br><span class="line">        grads[<span class="string">"db2"</span>] = db2</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#更新参数</span></span><br><span class="line">        parameters = update_parameters(parameters,grads,learning_rate)</span><br><span class="line">        </span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#打印损失</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">        </span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = two_layer_model(train_x, train_y, layer_dims = (n_x, n_h, n_y), num_iterations = <span class="number">2500</span>, print_cost=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.6930497356599888Cost after iteration 100: 0.6464320953428849Cost after iteration 200: 0.6325140647912677Cost after iteration 300: 0.6015024920354665Cost after iteration 400: 0.5601966311605747Cost after iteration 500: 0.5158304772764729Cost after iteration 600: 0.47549013139433255Cost after iteration 700: 0.43391631512257495Cost after iteration 800: 0.4007977536203886Cost after iteration 900: 0.3580705011323798Cost after iteration 1000: 0.3394281538366412Cost after iteration 1100: 0.3052753636196264Cost after iteration 1200: 0.2749137728213015Cost after iteration 1300: 0.24681768210614846Cost after iteration 1400: 0.19850735037466108Cost after iteration 1500: 0.17448318112556654Cost after iteration 1600: 0.17080762978096023Cost after iteration 1700: 0.11306524562164728Cost after iteration 1800: 0.09629426845937154Cost after iteration 1900: 0.08342617959726861Cost after iteration 2000: 0.07439078704319084Cost after iteration 2100: 0.06630748132267932Cost after iteration 2200: 0.05919329501038171Cost after iteration 2300: 0.053361403485605564Cost after iteration 2400: 0.04855478562877018</code></pre><p><img src="https://img-blog.csdnimg.cn/2020021320392212.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictions_train = predict(train_x, train_y, parameters)</span><br><span class="line">predictions_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Accuracy: 0.9999999999999998Accuracy: 0.72</code></pre><h1 id="L层神经网络"><a href="#L层神经网络" class="headerlink" title="L层神经网络"></a>L层神经网络</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations = <span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    costs = []</span><br><span class="line">    </span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line">        </span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line">        </span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line">        </span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">"cost"</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"iterations(per tens)"</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span>+str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, learning_rate=<span class="number">0.0075</span>, num_iterations=<span class="number">3000</span>, print_cost=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.771749Cost after iteration 100: 0.672053Cost after iteration 200: 0.648263Cost after iteration 300: 0.611507Cost after iteration 400: 0.567047Cost after iteration 500: 0.540138Cost after iteration 600: 0.527930Cost after iteration 700: 0.465477Cost after iteration 800: 0.369126Cost after iteration 900: 0.391747Cost after iteration 1000: 0.315187Cost after iteration 1100: 0.272700Cost after iteration 1200: 0.237419Cost after iteration 1300: 0.199601Cost after iteration 1400: 0.189263Cost after iteration 1500: 0.161189Cost after iteration 1600: 0.148214Cost after iteration 1700: 0.137775Cost after iteration 1800: 0.129740Cost after iteration 1900: 0.121225Cost after iteration 2000: 0.113821Cost after iteration 2100: 0.107839Cost after iteration 2200: 0.102855Cost after iteration 2300: 0.100897Cost after iteration 2400: 0.092878Cost after iteration 2500: 0.088413Cost after iteration 2600: 0.085951Cost after iteration 2700: 0.081681Cost after iteration 2800: 0.078247Cost after iteration 2900: 0.075444</code></pre><p><img src="https://img-blog.csdnimg.cn/20200213204018501.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict_train = predict(train_x, train_y, parameters)</span><br><span class="line">predict_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Accuracy: 0.9904306220095691Accuracy: 0.8200000000000001</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print_mislabeled_images(classes, test_x, test_y, predict_test)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200213204253460.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="测试自己的照片"><a href="#测试自己的照片" class="headerlink" title="测试自己的照片"></a>测试自己的照片</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">my_image = <span class="string">"cat1.jpg"</span></span><br><span class="line">my_label_y = [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">fname = <span class="string">"C:\\Users\\董润泽\\Desktop\\cat_picture\\"</span> + my_image</span><br><span class="line">image = np.array(ndimage.imread(fname, flatten=<span class="literal">False</span>))</span><br><span class="line">my_image = scipy.misc.imresize(image, size=(num_px, num_px)).reshape((num_px*num_px*<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">my_image = my_image/<span class="number">255</span></span><br><span class="line">my_predicted_image = predict(my_image, my_label_y, parameters)</span><br><span class="line">plt.imshow(image)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="string">", your L-layer model predicts a \""</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200213204753924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200213204847939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200213204930249.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200213205006656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>1深度学习的实用层面</title>
      <link href="/2020/02/16/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2/"/>
      <url>/2020/02/16/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/1%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/29794318" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29794318</a></p><h1 id="1、训练、验证、测试集"><a href="#1、训练、验证、测试集" class="headerlink" title="1、训练、验证、测试集"></a><strong>1、训练、验证、测试集</strong></h1><p>对于一组data，我们通常分为：</p><ul><li>训练集（train set）：用训练集对算法或模型进行训练</li><li>验证集（development set）：用验证集（简单交叉验证集hold-out cross validation set）进行交叉验证，选出最好的模型</li><li>测试集（test set）：对模型进行测试，获取模型运行的无偏估计</li></ul><h4 id="小数据时代："><a href="#小数据时代：" class="headerlink" title="小数据时代："></a>小数据时代：</h4><p>例如：100、1000、10000的数据量，可将data分为：</p><ul><li>无验证集：70%  /  30%</li><li>有验证集：60%  /  20%  /  20%</li></ul><h4 id="大数据时代"><a href="#大数据时代" class="headerlink" title="大数据时代"></a>大数据时代</h4><p>验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大能够验证大约2-10种算法哪种更好就足够了，不需要使用20%的数据作为验证集。如百万数据中抽取1万的数据作为验证集就可以了。</p><p>测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中1000条数据足以评估单个模型的效果。</p><ul><li>100万数据量：98% / 1% / 1%；</li><li>超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）</li></ul><h4 id="Notation"><a href="#Notation" class="headerlink" title="Notation"></a>Notation</h4><ul><li>建议验证和测试集来自同一分布，这样可以使机器学习算法变得更快</li><li>如果不需要无偏估计来评估模型的性能，则可以不需要测试集</li></ul><h1 id="2、偏差、方差"><a href="#2、偏差、方差" class="headerlink" title="2、偏差、方差"></a><strong>2、偏差、方差</strong></h1><p><img src="https://pic2.zhimg.com/80/v2-780fcc9466d6e335241727e0725c61fd_hd.jpg" alt=""></p><p>从图中可以看出，在欠拟合时，会出现高偏差（high bias），在过拟合时，会出现高方差（high variance）</p><p><img src="https://pic2.zhimg.com/80/v2-ef626d89314943c7701c0462fce443a9_hd.jpg" alt=""></p><p>对于1% / 11% 说明过拟合，使训练集误差小，而测试集误差偏大。</p><p>但是这是以人眼的误差为0%为基础，若人眼是 15% ， 则第二种是较好的情况</p><p>对于第三种高偏差、高方差：<br><img src="https://pic2.zhimg.com/80/v2-d31a17414aa07f477be2d1c94fb411ed_hd.jpg" alt=""></p><p>用了线性方程，并且还出现了过拟合</p><h1 id="3、机器学习基础"><a href="#3、机器学习基础" class="headerlink" title="3、机器学习基础"></a><strong>3、机器学习基础</strong></h1><p>如何解决high bias\high variance?</p><p><img src="https://pic3.zhimg.com/80/v2-69ee1d22e22e3509ecd0f87554e7ba02_hd.jpg" alt=""></p><ul><li><p>high bias</p><ul><li>增加网络结构，如增加隐藏层数目；</li><li>训练更长时间；</li><li>寻找合适的网络架构，使用更大的NN结构;</li></ul></li><li><p>high variance</p><ul><li>获取更多的数据；</li><li>正则化（ regularization）；</li><li>寻找合适的网络结构；</li></ul></li></ul><p>通常情况下，减少一方，会使另一方增加。<br>但在大数据时代，我们可以通过上面的方法减少一方，而不影响另一方</p><h1 id="4、正则化"><a href="#4、正则化" class="headerlink" title="4、正则化"></a><strong>4、正则化</strong></h1><p>上面已说，正则化可以降低variance方差</p><ul><li><strong>logistics regression</strong></li></ul><p>加入正则化项的代价函数：<br><img src="https://www.zhihu.com/equation?tex=J%28w%2Cb%29%3D%5Cdfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dl%28%5Chat+y%5E%7B%28i%29%7D%2Cy%5E%7B%28i%29%7D%29%2B%5Cdfrac%7B%5Clambda%7D%7B2m%7D%7C%7Cw%7C%7C_%7B2%7D%5E%7B2%7D" alt=""></p><ol><li><p>L2正则化：<img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Clambda%7D%7B2m%7D%7C%7Cw%7C%7C_%7B2%7D%5E%7B2%7D+%3D+%5Cdfrac%7B%5Clambda%7D%7B2m%7D%5Csum%5Climits_%7Bj%3D1%7D%5E%7Bn_%7Bx%7D%7D+w_%7Bj%7D%5E%7B2%7D%3D%5Cdfrac%7B%5Clambda%7D%7B2m%7Dw%5E%7BT%7Dw" alt=""></p></li><li><p>L1正则化：<img src="https://www.zhihu.com/equation?tex=%5Cdfrac%7B%5Clambda%7D%7B2m%7D%7C%7Cw%7C%7C_%7B1%7D%3D%5Cdfrac%7B%5Clambda%7D%7B2m%7D%5Csum%5Climits_%7Bj%3D1%7D%5E%7Bn_%7Bx%7D%7D%7Cw_%7Bj%7D%7C" alt=""></p></li></ol><p>其中的λ为正则化因子</p><p>注意：lambda 在python中属于保留字，所以在编程的时候，用“lambd”代表这里的正则化因子λ。</p><ul><li><strong>Neural Network</strong></li></ul><p>加入正则化项的代价函数：<img src="https://www.zhihu.com/equation?tex=J%28w%5E%7B%5B1%5D%7D%2Cb%5E%7B%5B1%5D%7D%2C%5Ccdots%2Cw%5E%7B%5BL%5D%7D%2Cb%5E%7B%5BL%5D%7D%29%3D%5Cdfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dl%28%5Chat+y%5E%7B%28i%29%7D%2Cy%5E%7B%28i%29%7D%29%2B%5Cdfrac%7B%5Clambda%7D%7B2m%7D%5Csum%5Climits_%7Bl%3D1%7D%5E%7BL%7D%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C_%7BF%7D%5E%7B2%7D" alt=""></p><p>其中<img src="https://www.zhihu.com/equation?tex=%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C_%7BF%7D%5E%7B2%7D%3D%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bn%5E%7B%5Bl-1%5D%7D%7D%5Csum%5Climits_%7Bj%3D1%7D%5E%7Bn%5E%7B%5Bl%5D%7D%7D%28w_%7Bij%7D%5E%7B%5Bl%5D%7D%29%5E%7B2%7D" alt="">)，因为W的大小为<img src="https://www.zhihu.com/equation?tex=%28n%5E%7B%5Bl-1%5D%7D%2Cn%5E%7B%5Bl%5D%7D%29" alt="">，该矩阵范数被称为“Frobenius norm”。</p><ul><li><strong>Weight decay 权重衰减</strong></li></ul><p>加入正则化后，梯度变为：</p><p><img src="https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D+%3D+%28form%5C_backprop%29%2B%5Cdfrac%7B%5Clambda%7D%7Bm%7DW%5E%7B%5Bl%5D%7D" alt=""></p><p>则梯度更新公式变为：</p><p><img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D%3A%3D+W%5E%7B%5Bl%5D%7D-%5Calpha+dW%5E%7B%5Bl%5D%7D" alt=""></p><p>代入得：</p><p><img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D%3A%3D+W%5E%7B%5Bl%5D%7D-%5Calpha+%5B+%28form%5C_backprop%29%2B%5Cdfrac%7B%5Clambda%7D%7Bm%7DW%5E%7B%5Bl%5D%7D%5D%5C%5C+%3D+W%5E%7B%5Bl%5D%7D-%5Calpha%5Cdfrac%7B%5Clambda%7D%7Bm%7DW%5E%7B%5Bl%5D%7D+-%5Calpha%28form%5C_backprop%29%5C%5C%3D%281-%5Cdfrac%7B%5Calpha%5Clambda%7D%7Bm%7D%29W%5E%7B%5Bl%5D%7D-%5Calpha%28form%5C_backprop%29" alt=""></p><p>其中， <img src="https://www.zhihu.com/equation?tex=%281-%5Cdfrac%7B%5Calpha%5Clambda%7D%7Bm%7D%29" alt=""> 为一个 <img src="https://www.zhihu.com/equation?tex=%3C1" alt=""> 的项，会给原来的 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="">一个衰减的参数，所以L2范数正则化也被称为“权重衰减（Weight decay）”。</p><h1 id="5、为什么正则化可以减少过拟合"><a href="#5、为什么正则化可以减少过拟合" class="headerlink" title="5、为什么正则化可以减少过拟合"></a><strong>5、为什么正则化可以减少过拟合</strong></h1><p>对于神经网络得cost function：</p><p><img src="https://www.zhihu.com/equation?tex=J%28w%5E%7B%5B1%5D%7D%2Cb%5E%7B%5B1%5D%7D%2C%5Ccdots%2Cw%5E%7B%5BL%5D%7D%2Cb%5E%7B%5BL%5D%7D%29%3D%5Cdfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dl%28%5Chat+y%5E%7B%28i%29%7D%2Cy%5E%7B%28i%29%7D%29%2B%5Cdfrac%7B%5Clambda%7D%7B2m%7D%5Csum%5Climits_%7Bl%3D1%7D%5E%7BL%7D%7C%7Cw%5E%7B%5Bl%5D%7D%7C%7C_%7BF%7D%5E%7B2%7D" alt=""></p><p>当正则化因子λ足够大，为了使cost function变小，则会使w变小趋近于0，则相当于消除了很多神经元的影响，那么图中的大的神经网络就会变成一个较小的网络。</p><p>但是实际上隐藏层的神经元依然存在，但是他们的影响变小了，便不会导致过拟合。</p><p><strong>数学解释：</strong></p><p>假设激活函数为tanh</p><p><img src="https://pic1.zhimg.com/80/v2-649a8466901387e1fdb2f5159fa676f4_hd.jpg" alt=""></p><p>加入正则化项后，当 λ 增大，导致 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D" alt="">减小， <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D%3DW%5E%7B%5Bl%5D%7Da%5E%7B%5Bl-1%5D%7D%2Bb%5E%7B%5Bl%5D%7D" alt="">便会减小，由上图可知，在 z 较小的区域里， <img src="https://www.zhihu.com/equation?tex=%5Ctanh%28z%29" alt=""> 函数近似线性，所以每层的函数就近似线性函数，整个网络就成为一个简单的近似线性的网络，从而不会发生过拟合。</p><h1 id="6、Dropout正则化"><a href="#6、Dropout正则化" class="headerlink" title="6、Dropout正则化"></a><strong>6、Dropout正则化</strong></h1><p>Dropout（随机失活）就是在神经网络的Dropout层，为每个神经元结点设置一个随机消除的概率，对于保留下来的神经元，我们得到一个节点较少，规模较小的网络进行训练。</p><p><img src="https://pic4.zhimg.com/80/v2-fa86d4f6c9fd6196859320bbaabba4df_hd.jpg" alt=""></p><p>实现Dropout的方法：反向随机失活（Inverted dropout）</p><p>首先假设对 layer 3 进行dropout：</p><pre><code>keep_prob = 0.8  # 设置神经元保留概率d3 = np.random.rand(a3.shape[0], a3.shape[1]) &lt; keep_proba3 = np.multiply(a3, d3)a3 /= keep_prob</code></pre><p>这里解释下为什么要有最后一步：a3 /= keep_prob</p><p>依照例子中的 keep_prob = 0.8 ，那么就有大约20%的神经元被删除了，也就是说 <img src="https://www.zhihu.com/equation?tex=a%5E%7B%5B3%5D%7D" alt=""> 中有20%的元素被归零了，在下一层的计算中有 <img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5B4%5D%7D%3DW%5E%7B%5B4%5D%7D%5Ccdot+a%5E%7B%5B3%5D%7D%2Bb%5E%7B%5B4%5D%7D" alt=""> ，所以为了不影响<img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5B4%5D%7D" alt="">的期望值，所以需要 <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5B4%5D%7D%5Ccdot+a%5E%7B%5B3%5D%7D" alt=""> 的部分除以一个keep_prob。</p><p>Inverted dropout 通过对“a3 /= keep_prob”,则保证无论 keep_prob 设置为多少，都不会对<img src="https://www.zhihu.com/equation?tex=Z%5E%7B%5B4%5D%7D" alt="">的期望值产生影响。</p><p>Notation：在测试阶段不要用dropout，因为那样会使得预测结果变得随机。</p><h1 id="7、理解Dropout"><a href="#7、理解Dropout" class="headerlink" title="7、理解Dropout"></a><strong>7、理解Dropout</strong></h1><p>我们以单个神经元入手，单个神经元的工作就是接收输入，并产生一些有意义的输出，但是加入了Dropout以后，输入的特征都是有可能会被随机清除的，所以该神经元不会再特别依赖于任何一个输入特征，也就是说不会给任何一个输入设置太大的权重。</p><p>所以通过传播过程，dropout将产生和L2范数相同的收缩权重的效果。</p><p>对于不同的层，设置的keep_prob也不同，一般来说神经元较少的层，会设 keep_prob=1.0，神经元多的层，则会将keep_prob设置的较小。</p><p><strong>Dropout 缺点</strong>：</p><p>dropout的一大缺点就是其使得 Cost function不能再被明确的定义，以为每次迭代都会随机消除一些神经元结点，所以我们无法绘制出每次迭代<img src="https://www.zhihu.com/equation?tex=J%28W%2Cb%29" alt="">下降的图，如下：</p><p><img src="https://pic2.zhimg.com/80/v2-b766c408d994212d8eb7e198440be695_hd.jpg" alt=""></p><p><strong>使用Dropout：</strong></p><p>关闭dropout功能，即设置 keep_prob = 1.0；<br>运行代码，确保<img src="https://www.zhihu.com/equation?tex=J%28W%2Cb%29" alt="">函数单调递减；<br>再打开 dropout 。</p><h1 id="8、其他正则化方法"><a href="#8、其他正则化方法" class="headerlink" title="8、其他正则化方法"></a><strong>8、其他正则化方法</strong></h1><ul><li><strong>数据扩增</strong>：通过变换图片，得到更多的训练集和验证集</li></ul><p><img src="https://pic1.zhimg.com/80/v2-4ee8dd7e2fd0bfd2c5424994a59fcc98_hd.jpg" alt=""></p><ul><li><strong>Early stopping</strong>：在交叉验证集的误差上升之前的点停止迭代，避免过拟合。这种方法的缺点是无法同时解决bias和variance之间的最优。</li></ul><p><img src="https://pic2.zhimg.com/80/v2-cdf222ea7b5b1fc8845f6e815395bd89_hd.jpg" alt=""></p><h1 id="9、归一化输入"><a href="#9、归一化输入" class="headerlink" title="9、归一化输入"></a><strong>9、归一化输入</strong></h1><p>对数据集特征 <img src="https://www.zhihu.com/equation?tex=x_%7B1%7D%2Cx_%7B2%7D" alt=""> 归一化的过程：</p><p><img src="https://pic3.zhimg.com/80/v2-23cc05544f135c29429e7b0198079662_hd.jpg" alt=""></p><ul><li>计算每个特征所有样本数据的均值： <img src="https://www.zhihu.com/equation?tex=%5Cmu+%3D+%5Cdfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dx%5E%7B%28i%29%7D" alt=""> ；</li><li>减去均值得到对称的分布： <img src="https://www.zhihu.com/equation?tex=x+%3A+%3Dx-%5Cmu" alt=""> ；</li><li>归一化方差： <img src="https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D+%3D+%5Cdfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%3D1%7D%5E%7Bm%7Dx%5E%7B%28i%29%5E%7B2%7D%7D" alt="">， <img src="https://www.zhihu.com/equation?tex=x+%3D+x%2F%5Csigma%5E%7B2%7D" alt="">。</li></ul><p><strong>使用归一化原因：</strong></p><p><img src="https://pic1.zhimg.com/80/v2-9b8ae9653968c8b956c4144577e75880_hd.jpg" alt=""></p><p>由图可以看出不使用归一化和使用归一化前后 Cost function 的函数形状会有很大的区别。</p><p>在不使用归一化的代价函数中，如果我们设置一个较小的学习率，那么很可能我们需要很多次迭代才能到达代价函数全局最优解；如果使用了归一化，那么无论从哪个位置开始迭代，我们都能以相对很少的迭代次数找到全局最优解。</p><h1 id="10、梯度消失、梯度爆炸"><a href="#10、梯度消失、梯度爆炸" class="headerlink" title="10、梯度消失、梯度爆炸"></a><strong>10、梯度消失、梯度爆炸</strong></h1><p><img src="https://img-blog.csdnimg.cn/20200215110718285.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>上面的情况对于导数也是同样的道理，所以在计算梯度时，根据情况的不同，梯度函数会以指数级递增或者递减，导致训练导数难度上升，梯度下降算法的步长会变得非常非常小，需要训练的时间将会非常长。</p><p>在梯度函数上出现的以指数级递增或者递减的情况就分别称为梯度爆炸或者梯度消失。</p><h1 id="11、利用初始化缓解梯度消失和爆炸问题"><a href="#11、利用初始化缓解梯度消失和爆炸问题" class="headerlink" title="11、利用初始化缓解梯度消失和爆炸问题"></a><strong>11、利用初始化缓解梯度消失和爆炸问题</strong></h1><p>以一个单神经元为例：</p><p><img src="https://pic1.zhimg.com/80/v2-a1783f8bfc4a2519275903658fdc44e8_hd.jpg" alt=""></p><p>当输入的数量 <img src="https://www.zhihu.com/equation?tex=n" alt=""> 较大时，我们希望每个 <img src="https://www.zhihu.com/equation?tex=w_%7Bi%7D" alt=""> 的值都小一些，这样它们的和得到的! <a href="https://www.zhihu.com/equation?tex=z" target="_blank" rel="noopener"></a> 也较小。</p><p>这里为了得到较小的 <img src="https://www.zhihu.com/equation?tex=w_%7Bi%7D" alt=""> ，设置 <img src="https://www.zhihu.com/equation?tex=Var%28w_%7Bi%7D%29%3D%5Cdfrac%7B1%7D%7Bn%7D" alt=""> ，这里称为Xavier initialization。</p><p>对参数进行初始化：</p><pre><code>WL = np.random.randn(WL.shape[0],WL.shape[1])* np.sqrt(1/n)</code></pre><p>这么做是因为，如果激活函数的输入 x 近似设置成均值为0，标准方差1的情况，输出 z 也会调整到相似的范围内。虽然没有解决梯度消失和爆炸的问题，但其在一定程度上确实减缓了梯度消失和爆炸的速度。</p><p>不同激活函数的 Xavier initialization：</p><ul><li>激活函数使用Relu： <img src="https://www.zhihu.com/equation?tex=Var%28w_%7Bi%7D%29%3D%5Cdfrac%7B2%7D%7Bn%7D" alt=""></li><li>激活函数使用tanh： <img src="https://www.zhihu.com/equation?tex=Var%28w_%7Bi%7D%29%3D%5Cdfrac%7B1%7D%7Bn%7D" alt=""></li></ul><p>其中n是输入的神经元个数，也就是 <img src="https://www.zhihu.com/equation?tex=n%5E%7B%5Bl-1%5D%7D" alt=""> 。</p><h1 id="12、梯度的数值逼近"><a href="#12、梯度的数值逼近" class="headerlink" title="12、梯度的数值逼近"></a><strong>12、梯度的数值逼近</strong></h1><p>使用双边误差的方法去逼近导数：</p><p><img src="https://pic2.zhimg.com/80/v2-2f23a8f5c12577d9dbaa2eb47b226e2d_hd.jpg" alt=""></p><p>由图可以看出，双边误差逼近的误差是0.0001，先比单边逼近的误差0.03，其精度要高了很多。</p><p><img src="https://img-blog.csdnimg.cn/20200215111402791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="13、梯度检验"><a href="#13、梯度检验" class="headerlink" title="13、梯度检验"></a><strong>13、梯度检验</strong></h1><p><strong>连接参数：</strong></p><p>因为我们的神经网络中含有大量的参数： <img src="https://www.zhihu.com/equation?tex=W%5E%7B%5B1%5D%7D%2Cb%5E%7B%5B1%5D%7D%2C%5Ccdots%2CW%5E%7B%5BL%5D%7D%2Cb%5E%7B%5BL%5D%7D" alt=""> ，为了做梯度检验，需要将这些参数全部连接起来，reshape成一个大的向量 θ 。</p><p>同时对 <img src="https://www.zhihu.com/equation?tex=dW%5E%7B%5B1%5D%7D%2Cdb%5E%7B%5B1%5D%7D%2C%5Ccdots%2CdW%5E%7B%5BL%5D%7D%2Cdb%5E%7B%5BL%5D%7D" alt=""> 执行同样的操作：</p><p><img src="https://pic4.zhimg.com/80/v2-4f422f204d09a055d4d4ed78d21a9437_hd.jpg" alt=""></p><p><strong>进行梯度检验：</strong></p><p>进行如下图的梯度检验</p><p><img src="https://pic1.zhimg.com/80/v2-9e1e6192fd6bb32ca0330c29a9d09688_hd.jpg" alt=""></p><p>判断 <img src="https://www.zhihu.com/equation?tex=d%5Ctheta_%7Bapprox%7D%5Capprox+d%5Ctheta" alt=""> 是否接近。</p><p><strong>判断公式：</strong></p><p><img src="https://www.zhihu.com/equation?tex=%5Cdfrac+%7B%7C%7Cd%5Ctheta_%7Bapprox%7D-d%5Ctheta%7C%7C_%7B2%7D%7D%7B%7C%7Cd%5Ctheta_%7Bapprox%7D%7C%7C_%7B2%7D%2B%7C%7Cd%5Ctheta%7C%7C_%7B2%7D%7D" alt=""></p><p>其中，“ <img src="https://www.zhihu.com/equation?tex=%7C%7C%5Ccdot+%7C%7C_%7B2%7D" alt=""> ”表示欧几里得范数，它是误差平方之和，然后求平方根，得到的欧氏距离。</p><h1 id="14、实现梯度检验-Notes"><a href="#14、实现梯度检验-Notes" class="headerlink" title="14、实现梯度检验 Notes"></a><strong>14、实现梯度检验 Notes</strong></h1><ul><li>不要在训练过程中使用梯度检验，只在debug的时候使用，使用完毕关闭梯度检验的功能；</li><li>如果算法的梯度检验出现了错误，要检查每一项，找出错误，也就是说要找出哪个 dθ_approx[i] 与 dθ 的值相差比较大；</li><li>不要忘记了正则化项；</li><li>梯度检验不能与dropout同时使用。因为每次迭代的过程中，dropout会随机消除隐层单元的不同神经元，这时是难以计算dropout在梯度下降上的代价函数J；</li><li>在随机初始化的时候运行梯度检验，或许在训练几次后再进行。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 02改善深层神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编程作业——梯度检测</title>
      <link href="/2020/02/16/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E2%80%94%E2%80%94%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%B5%8B/"/>
      <url>/2020/02/16/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E2%80%94%E2%80%94%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> gc_utils <span class="keyword">import</span> sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector</span><br></pre></td></tr></table></figure><h1 id="一维梯度检测"><a href="#一维梯度检测" class="headerlink" title="一维梯度检测"></a>一维梯度检测</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(x, theta)</span>:</span></span><br><span class="line">    J = theta * x</span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(x, theta)</span>:</span></span><br><span class="line">    dtheta = x</span><br><span class="line">    <span class="keyword">return</span> dtheta</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200216170122323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check</span><span class="params">(x, theta, epsilon = <span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    thetaplus = theta + epsilon</span><br><span class="line">    thetaminus = theta - epsilon</span><br><span class="line">    J_plus = forward_propagation(x, thetaplus)</span><br><span class="line">    J_minus = forward_propagation(x, thetaminus)</span><br><span class="line">    </span><br><span class="line">    gradapprox = (J_plus-J_minus)/(<span class="number">2</span>*epsilon)</span><br><span class="line">    </span><br><span class="line">    grad = backward_propagation(x, theta)</span><br><span class="line">    </span><br><span class="line">    numerator = np.linalg.norm(grad - gradapprox)</span><br><span class="line">    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> difference &lt; <span class="number">1e-7</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The gradient is correct!"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The gradient is wrong!"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure><h1 id="N维梯度检测"><a href="#N维梯度检测" class="headerlink" title="N维梯度检测"></a>N维梯度检测</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_n</span><span class="params">(X, Y, parameters)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line">    </span><br><span class="line">    Z1 = np.dot(W1, X)+b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Cost</span></span><br><span class="line">    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(<span class="number">1</span> - A3), <span class="number">1</span> - Y)</span><br><span class="line">    cost = <span class="number">1.</span>/m * np.sum(logprobs)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_n</span><span class="params">(X, Y, cache)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) * <span class="number">2</span></span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">4.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,</span><br><span class="line">                 <span class="string">"dA2"</span>: dA2, <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2,</span><br><span class="line">                 <span class="string">"dA1"</span>: dA1, <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200216170153682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check_n</span><span class="params">(parameters, gradients, X, Y, epsilon=<span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    parameters_values, _ = dictionary_to_vector(parameters)</span><br><span class="line">    grad = gradients_to_vector(gradients)</span><br><span class="line">    num_parameters = parameters_values.shape[<span class="number">0</span>]</span><br><span class="line">    J_plus = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    J_minus = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    gradapprox = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_parameters):</span><br><span class="line">        thetaplus = np.copy(parameters_values)</span><br><span class="line">        thetaplus[i][<span class="number">0</span>] = thetaplus[i][<span class="number">0</span>] + epsilon</span><br><span class="line">        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))</span><br><span class="line">        </span><br><span class="line">        thetaminus = np.copy(parameters_values)</span><br><span class="line">        thetaminus[i][<span class="number">0</span>] = thetaminus[i][<span class="number">0</span>] - epsilon</span><br><span class="line">        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))</span><br><span class="line">        </span><br><span class="line">        gradapprox[i] = (J_plus[i]-J_minus[i]) / (<span class="number">2</span>*epsilon)</span><br><span class="line">        </span><br><span class="line">    numerator = np.linalg.norm(grad-gradapprox)</span><br><span class="line">    denominator = np.linalg.norm(grad)+np.linalg.norm(gradapprox)</span><br><span class="line">    difference = numerator/denominator</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> difference &gt; <span class="number">2e-7</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[93m"</span> + <span class="string">"There is a mistake in the backward propagation! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[92m"</span> + <span class="string">"Your backward propagation works perfectly fine! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X, Y, parameters = gradient_check_n_test_case()</span><br><span class="line"></span><br><span class="line">cost, cache = forward_propagation_n(X, Y, parameters)</span><br><span class="line">gradients = backward_propagation_n(X, Y, cache)</span><br><span class="line">difference = gradient_check_n(parameters, gradients, X, Y)</span><br></pre></td></tr></table></figure><pre><code>[93mThere is a mistake in the backward propagation! difference = 0.2850931566540251[0m</code></pre>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 02改善深层神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编程作业——正则化</title>
      <link href="/2020/02/16/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96/"/>
      <url>/2020/02/16/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E2%80%94%E2%80%94%E6%AD%A3%E5%88%99%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><h1 id="导包、加载数据"><a href="#导包、加载数据" class="headerlink" title="导包、加载数据"></a>导包、加载数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> compute_cost, predict, forward_propagation, backward_propagation, update_parameters</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_2D_dataset</span><span class="params">()</span>:</span></span><br><span class="line">    data = scipy.io.loadmat(<span class="string">'datasets/data.mat'</span>)</span><br><span class="line">    train_X = data[<span class="string">'X'</span>].T</span><br><span class="line">    train_Y = data[<span class="string">'y'</span>].T</span><br><span class="line">    test_X = data[<span class="string">'Xval'</span>].T</span><br><span class="line">    test_Y = data[<span class="string">'yval'</span>].T</span><br><span class="line"></span><br><span class="line">    plt.scatter(train_X[<span class="number">0</span>, :], train_X[<span class="number">1</span>, :], c=np.squeeze(train_Y), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> train_X, train_Y, test_X, test_Y</span><br></pre></td></tr></table></figure><pre><code>train_X, train_Y, test_X, test_Y = load_2D_dataset()</code></pre><p><strong>这行代码执行时可能会报错，解决方法：在load_2D_dataset中，把 c=train_Y改为 c=np.squeeze(train_Y)，如果仍然无法解决，可以把此函数放在本文件中</strong></p><p><img src="https://img-blog.csdnimg.cn/20200216161157328.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="Non-regularized-model"><a href="#Non-regularized-model" class="headerlink" title="Non-regularized model"></a>Non-regularized model</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.3</span>, num_iterations = <span class="number">30000</span>, print_cost = True, lambd = <span class="number">0</span>, keep_prob = <span class="number">1</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                            <span class="comment"># to keep track of the cost</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                        <span class="comment"># number of examples</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">20</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary.</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        <span class="keyword">if</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost function</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span>:</span><br><span class="line">            cost = compute_cost(a3, Y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        <span class="keyword">assert</span>(lambd==<span class="number">0</span> <span class="keyword">or</span> keep_prob==<span class="number">1</span>)    <span class="comment"># it is possible to use both L2 regularization and dropout, </span></span><br><span class="line">                                            <span class="comment"># but this assignment will only explore one at a time</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span> <span class="keyword">and</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation(X, Y, cache)</span><br><span class="line">        <span class="keyword">elif</span> lambd != <span class="number">0</span>:</span><br><span class="line">            grads = backward_propagation_with_regularization(X, Y, cache, lambd)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the loss every 10000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i,cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (x1,000)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the training set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.6557412523481002Cost after iteration 10000: 0.1632998752572419Cost after iteration 20000: 0.13851642423239133</code></pre><p><img src="https://img-blog.csdnimg.cn/20200216161208614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><pre><code>On the training set:Accuracy: 0.9478672985781991On the test set:Accuracy: 0.915</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model without regularization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>,<span class="number">0.40</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>,<span class="number">0.65</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200216161224852.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h1><p><img src="https://img-blog.csdnimg.cn/20200216161718304.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_with_regularization</span><span class="params">(A3, Y, parameters, lambd)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    </span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y)</span><br><span class="line">    </span><br><span class="line">    L2_regularization_cost = <span class="number">1</span>/m*lambd/<span class="number">2</span>*(np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))</span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200216161816207.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_regularization</span><span class="params">(X, Y, cache, lambd)</span>:</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    </span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T) + lambd/m*W3</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) + lambd/m*W2</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T) + lambd/m*W1</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, lambd = <span class="number">0.7</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.6974484493131264Cost after iteration 10000: 0.2684918873282239Cost after iteration 20000: 0.2680916337127301</code></pre><p><img src="https://img-blog.csdnimg.cn/20200216161236600.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><pre><code>On the train set:Accuracy: 0.9383886255924171On the test set:Accuracy: 0.93</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with L2-regularization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>,<span class="number">0.40</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>,<span class="number">0.65</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200216161246614.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>步骤</p><p>keep_prob = 0.8  # 设置神经元保留概率</p><p>d3 = np.random.rand(a3.shape[0], a3.shape[1]) &lt; keep_prob</p><p>a3 = np.multiply(a3, d3)</p><p>a3 /= keep_prob</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_with_dropout</span><span class="params">(X, parameters, keep_prob=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line">    </span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">######</span></span><br><span class="line">    D1 = np.random.randn(A1.shape[<span class="number">0</span>], A1.shape[<span class="number">1</span>])</span><br><span class="line">    D1 = D1&lt;keep_prob</span><br><span class="line">    A1 = np.multiply(A1, D1)</span><br><span class="line">    A1 = A1/keep_prob</span><br><span class="line">    <span class="comment">######</span></span><br><span class="line">    </span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    <span class="comment">######</span></span><br><span class="line">    D2 = np.random.randn(A2.shape[<span class="number">0</span>], A2.shape[<span class="number">1</span>])</span><br><span class="line">    D2 = D2&lt;keep_prob</span><br><span class="line">    A2 = np.multiply(A2, D2)</span><br><span class="line">    A2 = A2/keep_prob</span><br><span class="line">    <span class="comment">######</span></span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A3, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_dropout</span><span class="params">(X, Y, cache, keep_prob)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3-Y</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">######</span></span><br><span class="line">    dA2 = dA2*D2</span><br><span class="line">    dA2 = dA2/keep_prob</span><br><span class="line">    <span class="comment">######</span></span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    <span class="comment">######</span></span><br><span class="line">    dA1 = dA1 * D1</span><br><span class="line">    dA1 = dA1 / keep_prob</span><br><span class="line">    <span class="comment">######</span></span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, keep_prob = <span class="number">0.86</span>, learning_rate = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.6595130683184598Cost after iteration 10000: 0.07083004396279081Cost after iteration 20000: 0.07030667016479066</code></pre><p><img src="https://img-blog.csdnimg.cn/20200216161254572.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><pre><code>On the train set:Accuracy: 0.8957345971563981On the test set:Accuracy: 0.89</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with dropout"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-0.75</span>,<span class="number">0.40</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-0.75</span>,<span class="number">0.65</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200216161327437.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><table>     <tr>        <td>        **model**        </td>        <td>        **train accuracy**        </td>        <td>        **test accuracy**        </td>    </tr>        <td>        3-layer NN without regularization        </td>        <td>        95%        </td>        <td>        91.5%        </td>    <tr>        <td>        3-layer NN with L2-regularization        </td>        <td>        94%        </td>        <td>        93%        </td>    </tr>    <tr>        <td>        3-layer NN with dropout        </td>        <td>        93%        </td>        <td>        95%        </td>    </tr></table> ]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 02改善深层神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>编程作业——初始化参数w</title>
      <link href="/2020/02/16/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E2%80%94%E2%80%94%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0w/"/>
      <url>/2020/02/16/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81%E6%AD%A3%E5%88%99%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%BC%98%E5%8C%96/%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A%E2%80%94%E2%80%94%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0w/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><p><strong>对w初始化用 np.random.randn(shape) * <img src="https://img-blog.csdnimg.cn/20200216150807581.png" alt=""></strong></p><h1 id="导包、加载数据"><a href="#导包、加载数据" class="headerlink" title="导包、加载数据"></a>导包、加载数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">from</span> init_utils <span class="keyword">import</span> sigmoid, relu, compute_loss, forward_propagation, backward_propagation</span><br><span class="line"><span class="keyword">from</span> init_utils <span class="keyword">import</span> update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">train_X, train_Y, test_X, test_Y = load_dataset()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200216150950475.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.01</span>, num_iterations = <span class="number">15000</span>, print_cost = True, initialization = <span class="string">"he"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate for gradient descent </span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations to run gradient descent</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string">    initialization -- flag to choose which initialization to use ("zeros","random" or "he")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = [] <span class="comment"># to keep track of the loss</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>] <span class="comment"># number of examples</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary.</span></span><br><span class="line">    <span class="keyword">if</span> initialization == <span class="string">"zeros"</span>:</span><br><span class="line">        parameters = initialize_parameters_zeros(layers_dims)</span><br><span class="line">    <span class="keyword">elif</span> initialization == <span class="string">"random"</span>:</span><br><span class="line">        parameters = initialize_parameters_random(layers_dims)</span><br><span class="line">    <span class="keyword">elif</span> initialization == <span class="string">"he"</span>:</span><br><span class="line">        parameters = initialize_parameters_he(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loss</span></span><br><span class="line">        cost = compute_loss(a3, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        grads = backward_propagation(X, Y, cache)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the loss every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, cost))</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># plot the loss</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="Zero-initialization"><a href="#Zero-initialization" class="headerlink" title="Zero initialization"></a>Zero initialization</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_zeros</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.zeros((layers_dims[l], layers_dims[l<span class="number">-1</span>]))</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, initialization = <span class="string">"zeros"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.6931471805599453Cost after iteration 1000: 0.6931471805599453Cost after iteration 2000: 0.6931471805599453Cost after iteration 3000: 0.6931471805599453Cost after iteration 4000: 0.6931471805599453Cost after iteration 5000: 0.6931471805599453Cost after iteration 6000: 0.6931471805599453Cost after iteration 7000: 0.6931471805599453Cost after iteration 8000: 0.6931471805599453Cost after iteration 9000: 0.6931471805599453Cost after iteration 10000: 0.6931471805599455Cost after iteration 11000: 0.6931471805599453Cost after iteration 12000: 0.6931471805599453Cost after iteration 13000: 0.6931471805599453Cost after iteration 14000: 0.6931471805599453</code></pre><p><img src="https://img-blog.csdnimg.cn/2020021615100484.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><pre><code>On the train set:Accuracy: 0.5On the test set:Accuracy: 0.5</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with Zeros initialization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">1.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1.5</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200216151016277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="random-initialization"><a href="#random-initialization" class="headerlink" title="random initialization"></a>random initialization</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_random</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class="number">-1</span>]) * <span class="number">10</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, initialization = <span class="string">"random"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: infCost after iteration 1000: 0.6250982793959966Cost after iteration 2000: 0.5981216596703697Cost after iteration 3000: 0.5638417572298645Cost after iteration 4000: 0.5501703049199763Cost after iteration 5000: 0.5444632909664456Cost after iteration 6000: 0.5374513807000807Cost after iteration 7000: 0.4764042074074983Cost after iteration 8000: 0.39781492295092263Cost after iteration 9000: 0.3934764028765484Cost after iteration 10000: 0.3920295461882659Cost after iteration 11000: 0.38924598135108Cost after iteration 12000: 0.3861547485712325Cost after iteration 13000: 0.384984728909703Cost after iteration 14000: 0.3827828308349524</code></pre><p><img src="https://img-blog.csdnimg.cn/20200216151030134.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><pre><code>On the train set:Accuracy: 0.83On the test set:Accuracy: 0.86</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with large random initialization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">1.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1.5</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200216151041871.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="He-initialization"><a href="#He-initialization" class="headerlink" title="He initialization"></a>He initialization</h1><p>This function is similar to the previous <code>initialize_parameters_random(...)</code>. The only difference is that instead of multiplying <code>np.random.randn(..,..)</code> by 10, you will multiply it by $\sqrt{\frac{2}{\text{dimension of the previous layer}}}$, which is what He initialization recommends for layers with a ReLU activation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_he</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims) - <span class="number">1</span> <span class="comment"># integer representing the number of layers</span></span><br><span class="line">     </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l], layers_dims[l<span class="number">-1</span>])*np.sqrt(<span class="number">2</span>/layers_dims[l<span class="number">-1</span>])</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, initialization = <span class="string">"he"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.8830537463419761Cost after iteration 1000: 0.6879825919728063Cost after iteration 2000: 0.6751286264523371Cost after iteration 3000: 0.6526117768893807Cost after iteration 4000: 0.6082958970572937Cost after iteration 5000: 0.5304944491717495Cost after iteration 6000: 0.4138645817071793Cost after iteration 7000: 0.3117803464844441Cost after iteration 8000: 0.23696215330322556Cost after iteration 9000: 0.18597287209206828Cost after iteration 10000: 0.15015556280371808Cost after iteration 11000: 0.12325079292273548Cost after iteration 12000: 0.09917746546525937Cost after iteration 13000: 0.08457055954024274Cost after iteration 14000: 0.07357895962677366</code></pre><p><img src="https://img-blog.csdnimg.cn/20200216151050979.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><pre><code>On the train set:Accuracy: 0.9933333333333333On the test set:Accuracy: 0.96</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Model with He initialization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">1.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1.5</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200216151102834.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><table>     <tr>        <td>        **Model**        </td>        <td>        **Train accuracy**        </td>        <td>        **Problem/Comment**        </td>    </tr>        <td>        3-layer NN with zeros initialization        </td>        <td>        50%        </td>        <td>        fails to break symmetry        </td>    <tr>        <td>        3-layer NN with large random initialization        </td>        <td>        83%        </td>        <td>        too large weights         </td>    </tr>    <tr>        <td>        3-layer NN with He initialization        </td>        <td>        99%        </td>        <td>        recommended method        </td>    </tr></table> ]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 02改善深层神经网络 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深层神经网络编程作业1</title>
      <link href="/2020/02/13/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A1/"/>
      <url>/2020/02/13/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A1/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><h1 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v4 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>)</span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"> </span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="浅层神经网络参数初始化"><a href="#浅层神经网络参数初始化" class="headerlink" title="浅层神经网络参数初始化"></a>浅层神经网络参数初始化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                 <span class="string">"b1"</span>: b1,</span><br><span class="line">                 <span class="string">"W2"</span>: W2,</span><br><span class="line">                 <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="L层神经网络参数初始化"><a href="#L层神经网络参数初始化" class="headerlink" title="L层神经网络参数初始化"></a>L层神经网络参数初始化</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L): <span class="comment">#从1到L-1，不包括L</span></span><br><span class="line">        parameters[<span class="string">'W'</span>+str(l)] = np.random.randn(layer_dims[l], layer_dims[l<span class="number">-1</span>])*<span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span>+str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span>+str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span>+str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="实现一个单层正向传播"><a href="#实现一个单层正向传播" class="headerlink" title="实现一个单层正向传播"></a>实现一个单层正向传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></span><br><span class="line">    Z = np.dot(W, A) +b</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><h1 id="对一层激活并向前传播（activation-cache存Z，linear-cache存A、W、b）"><a href="#对一层激活并向前传播（activation-cache存Z，linear-cache存A、W、b）" class="headerlink" title="对一层激活并向前传播（activation_cache存Z，linear_cache存A、W、b）"></a>对一层激活并向前传播（activation_cache存Z，linear_cache存A、W、b）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> activation==<span class="string">"sigmoid"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z)</span><br><span class="line">    <span class="keyword">elif</span> activation==<span class="string">"relu"</span>:</span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><h1 id="L层正向传播"><a href="#L层正向传播" class="headerlink" title="L层正向传播"></a>L层正向传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    caches = []</span><br><span class="line">    A = X</span><br><span class="line">    L = len(parameters)//<span class="number">2</span> <span class="comment">#整数除法，返回一个不大于结果的整数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L): <span class="comment">#前L-1层都用relu，只有最后一层用sigmoid</span></span><br><span class="line">        A_prev = A</span><br><span class="line">        A,cache = linear_activation_forward(A_prev, parameters[<span class="string">'W'</span>+str(l)], parameters[<span class="string">'b'</span>+str(l)], <span class="string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">'W'</span>+str(L)], parameters[<span class="string">'b'</span>+str(L)], <span class="string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>,X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    cost = <span class="number">-1</span>/m*np.sum(Y*np.log(AL)+(<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-AL))</span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200213155632392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p><img src="https://img-blog.csdnimg.cn/2020021315582530.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="单层向后传播"><a href="#单层向后传播" class="headerlink" title="单层向后传播"></a>单层向后传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    dW = <span class="number">1</span>/m * np.dot(dZ, A_prev.T)</span><br><span class="line">    db = <span class="number">1</span>/m * np.sum(dZ, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    dA_prev = np.dot(W.T, dZ)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.shape == b.shape)</span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><h1 id="对单层激活并向后传播"><a href="#对单层激活并向后传播" class="headerlink" title="对单层激活并向后传播"></a>对单层激活并向后传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation==<span class="string">"relu"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">    <span class="keyword">elif</span> activation==<span class="string">"sigmoid"</span>:</span><br><span class="line">        dZ = sigmoid_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200213160009273.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="L层向后传播"><a href="#L层向后传播" class="headerlink" title="L层向后传播"></a>L层向后传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    grads=&#123;&#125;</span><br><span class="line">    L = len(caches)</span><br><span class="line">    m = AL.shape[<span class="number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape)</span><br><span class="line">    </span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</span><br><span class="line">    </span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]</span><br><span class="line">    grads[<span class="string">"dA"</span>+str(L<span class="number">-1</span>)], grads[<span class="string">"dW"</span>+str(L)], grads[<span class="string">"db"</span>+str(L)] = linear_activation_backward(dAL, current_cache, <span class="string">"sigmoid"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</span><br><span class="line">        <span class="comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span></span><br><span class="line">        <span class="comment"># Inputs: "grads["dA" + str(l + 1)], current_cache". Outputs: "grads["dA" + str(l)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] </span></span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span> + str(l + <span class="number">1</span>)], current_cache, <span class="string">"relu"</span>)</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l + <span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l + <span class="number">1</span>)] = db_temp</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><h1 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    L = len(parameters) // <span class="number">2</span>  <span class="comment"># 神经网络的层数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L): <span class="comment">#从0到L-1</span></span><br><span class="line">        parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)]-learning_rate*grads[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)]-learning_rate*grads[<span class="string">"db"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">parameters, grads = update_parameters_test_case()</span><br><span class="line">parameters = update_parameters(parameters, grads, <span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"W1 = "</span>+ str(parameters[<span class="string">"W1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b1 = "</span>+ str(parameters[<span class="string">"b1"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"W2 = "</span>+ str(parameters[<span class="string">"W2"</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"b2 = "</span>+ str(parameters[<span class="string">"b2"</span>]))</span><br></pre></td></tr></table></figure><pre><code>W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008] [-1.76569676 -0.80627147  0.51115557 -1.18258802] [-1.0535704  -0.86128581  0.68284052  2.20374577]]b1 = [[-0.04659241] [-1.28888275] [ 0.53405496]]W2 = [[-0.55569196  0.0354055   1.32964895]]b2 = [[-0.84610769]]</code></pre>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅层神经网络编程作业</title>
      <link href="/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><h1 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> sklearn.linear_model</span><br><span class="line"><span class="keyword">from</span> planar_utils <span class="keyword">import</span> plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h1 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X, Y = load_planar_dataset()</span><br></pre></td></tr></table></figure><h1 id="观察数据"><a href="#观察数据" class="headerlink" title="观察数据"></a>观察数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(Y), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">    关于np.squeeze()</span></span><br><span class="line"><span class="string">    删除Y中为空的维度</span></span><br><span class="line"><span class="string">    例如：</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; a = e.reshape(1,1,10)</span></span><br><span class="line"><span class="string">    array([[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]])</span></span><br><span class="line"><span class="string">    &gt;&gt;&gt; np.squeeze(a)</span></span><br><span class="line"><span class="string">    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><pre><code>&apos;\n    关于np.squeeze()\n    删除Y中为空的维度\n    例如：\n    &gt;&gt;&gt; a = e.reshape(1,1,10)\n    array([[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]])\n    &gt;&gt;&gt; np.squeeze(a)\n    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n&apos;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200211224816770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="神经网络每层神经元数"><a href="#神经网络每层神经元数" class="headerlink" title="神经网络每层神经元数"></a>神经网络每层神经元数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment">#输入层</span></span><br><span class="line">    n_h = <span class="number">4</span> <span class="comment">#隐藏层</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment">#输出层</span></span><br><span class="line">    <span class="keyword">return</span>(n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><h1 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    W1 = np.random.randn(n_h, n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros(shape=(n_h, <span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y, n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros(shape=(n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                 <span class="string">"b1"</span>: b1,</span><br><span class="line">                 <span class="string">"W2"</span>: W2,</span><br><span class="line">                 <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="向前传播"><a href="#向前传播" class="headerlink" title="向前传播"></a>向前传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    Z1 = np.dot(W1,X)+b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2,A1)+b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">            <span class="string">"A1"</span>: A1,</span><br><span class="line">            <span class="string">"Z2"</span>: Z2,</span><br><span class="line">            <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    <span class="keyword">return</span> A2,cache</span><br></pre></td></tr></table></figure><h1 id="计算损失函数"><a href="#计算损失函数" class="headerlink" title="计算损失函数"></a>计算损失函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    cost = <span class="number">-1</span>/m * np.sum(Y*np.log(A2)+(<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A2))</span><br><span class="line">    </span><br><span class="line">    cost = float(np.squeeze(cost))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost, float))</span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h1 id="向后传播"><a href="#向后传播" class="headerlink" title="向后传播"></a>向后传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">    </span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = <span class="number">1</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>) <span class="comment">#按行相加并保持二维特性</span></span><br><span class="line">    dZ1 = np.dot(W2.T, dZ2) *(<span class="number">1</span>-np.power(A1, <span class="number">2</span>))</span><br><span class="line">    dW1 = <span class="number">1</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">            <span class="string">"db1"</span>: db1,</span><br><span class="line">            <span class="string">"dW2"</span>: dW2,</span><br><span class="line">            <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><h1 id="更新参数"><a href="#更新参数" class="headerlink" title="更新参数"></a>更新参数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line">    </span><br><span class="line">    W1 = W1-learning_rate*dW1</span><br><span class="line">    b1 = b1-learning_rate*db1</span><br><span class="line">    W2 = W2-learning_rate*dW2</span><br><span class="line">    b2 = b2-learning_rate*db2</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h1 id="构造神经网络模型"><a href="#构造神经网络模型" class="headerlink" title="构造神经网络模型"></a>构造神经网络模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=<span class="number">0.5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    predictions = np.round(A2)</span><br><span class="line">    <span class="comment">#np.round()函数的作用：对给定的数组进行四舍五入，可以指定精度</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br></pre></td></tr></table></figure><h1 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画边界</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">predictions = predict(parameters, X)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">'Accuracy: %d'</span> % float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.693048Cost after iteration 1000: 0.309802Cost after iteration 2000: 0.292433Cost after iteration 3000: 0.283349Cost after iteration 4000: 0.276781Cost after iteration 5000: 0.263472Cost after iteration 6000: 0.242044Cost after iteration 7000: 0.235525Cost after iteration 8000: 0.231410Cost after iteration 9000: 0.228464Accuracy: 90%</code></pre><p><img src="https://img-blog.csdnimg.cn/20200211224850147.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="隐藏层数量大小的影响"><a href="#隐藏层数量大小的影响" class="headerlink" title="隐藏层数量大小的影响"></a>隐藏层数量大小的影响</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">16</span>, <span class="number">32</span>))</span><br><span class="line">hidden_layer_sizes = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">20</span>, <span class="number">50</span>]</span><br><span class="line"><span class="keyword">for</span> i, n_h <span class="keyword">in</span> enumerate(hidden_layer_sizes):</span><br><span class="line">    plt.subplot(<span class="number">5</span>, <span class="number">2</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.title(<span class="string">'Hidden Layer of size %d'</span> % n_h)</span><br><span class="line">    parameters = nn_model(X, Y, n_h, num_iterations = <span class="number">5000</span>)</span><br><span class="line">    plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">    predictions = predict(parameters, X)</span><br><span class="line">    accuracy = float((np.dot(Y,predictions.T) + np.dot(<span class="number">1</span>-Y,<span class="number">1</span>-predictions.T))/float(Y.size)*<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"Accuracy for &#123;&#125; hidden units: &#123;&#125; %"</span>.format(n_h, accuracy))</span><br></pre></td></tr></table></figure><pre><code>Accuracy for 1 hidden units: 67.25 %Accuracy for 2 hidden units: 66.5 %Accuracy for 3 hidden units: 89.25 %Accuracy for 4 hidden units: 90.0 %Accuracy for 5 hidden units: 89.75 %Accuracy for 20 hidden units: 90.0 %Accuracy for 50 hidden units: 89.75 %</code></pre><p><img src="https://img-blog.csdnimg.cn/20200211224940314.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""><br><img src="https://img-blog.csdnimg.cn/20200211225018244.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="测试其他数据集"><a href="#测试其他数据集" class="headerlink" title="测试其他数据集"></a>测试其他数据集</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Datasets</span></span><br><span class="line">noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()</span><br><span class="line"></span><br><span class="line">datasets = &#123;<span class="string">"noisy_circles"</span>: noisy_circles,</span><br><span class="line">            <span class="string">"noisy_moons"</span>: noisy_moons,</span><br><span class="line">            <span class="string">"blobs"</span>: blobs,</span><br><span class="line">            <span class="string">"gaussian_quantiles"</span>: gaussian_quantiles&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">### START CODE HERE ### (choose your dataset)</span></span><br><span class="line">dataset = <span class="string">"noisy_moons"</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">X, Y = datasets[dataset]</span><br><span class="line">X, Y = X.T, Y.reshape(<span class="number">1</span>, Y.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># make blobs binary</span></span><br><span class="line"><span class="keyword">if</span> dataset == <span class="string">"blobs"</span>:</span><br><span class="line">    Y = Y%<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the data</span></span><br><span class="line">plt.scatter(X[<span class="number">0</span>, :], X[<span class="number">1</span>, :], c=np.squeeze(Y), s=<span class="number">40</span>, cmap=plt.cm.Spectral);</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200211225052519.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200211225220649.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""><br><img src="https://img-blog.csdnimg.cn/20200211225246980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="">)<img src="https://img-blog.csdnimg.cn/20200211225306910.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="">)<img src="https://img-blog.csdnimg.cn/20200211225325983.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>神经网络基础编程作业</title>
      <link href="/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/"/>
      <url>/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B%E4%BD%9C%E4%B8%9A/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><h1 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> lr_utils <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h1 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m_train = train_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">m_test = test_set_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = test_set_x_orig.shape[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h1 id="转换数据的形状，转换为：每一列代表一个图片的RGB，行代表所有的图片"><a href="#转换数据的形状，转换为：每一列代表一个图片的RGB，行代表所有的图片" class="headerlink" title="转换数据的形状，转换为：每一列代表一个图片的RGB，行代表所有的图片"></a>转换数据的形状，转换为：每一列代表一个图片的RGB，行代表所有的图片</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">print(train_set_x_flatten.shape)</span><br><span class="line">print(test_set_x_flatten.shape)</span><br></pre></td></tr></table></figure><pre><code>(12288, 209)(12288, 50)</code></pre><h1 id="将数据集标准化（颜色值都是0-255，要把颜色值转化成0-1）"><a href="#将数据集标准化（颜色值都是0-255，要把颜色值转化成0-1）" class="headerlink" title="将数据集标准化（颜色值都是0-255，要把颜色值转化成0-1）"></a>将数据集标准化（颜色值都是0-255，要把颜色值转化成0-1）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_set_x = train_set_x_flatten/<span class="number">255</span></span><br><span class="line">test_set_x = test_set_x_flatten/<span class="number">255</span></span><br></pre></td></tr></table></figure><h1 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="sigmoid 函数"></a>sigmoid 函数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    s = <span class="number">1</span>/(<span class="number">1</span>+np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><h1 id="初始化w、b-这里是logistics-regression"><a href="#初始化w、b-这里是logistics-regression" class="headerlink" title="初始化w、b(这里是logistics regression)"></a>初始化w、b(这里是logistics regression)</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    w = np.zeros(shape=(dim,<span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>)) <span class="comment">#断言确保维度正确</span></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> w,b</span><br></pre></td></tr></table></figure><h1 id="向前向后传播"><a href="#向前向后传播" class="headerlink" title="向前向后传播"></a>向前向后传播</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="comment">#向前传播</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>] <span class="comment">#样本个数</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X)+b)</span><br><span class="line">    cost = (<span class="number">-1</span>/m)*np.sum(Y*np.log(A)+(<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#向后传播</span></span><br><span class="line">    dw = <span class="number">1</span>/m * np.dot(X , (A - Y).T)</span><br><span class="line">    db = <span class="number">1</span>/m * np.sum(A - Y)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,  <span class="comment">#将dw、db存入字典</span></span><br><span class="line">            <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads,cost</span><br></pre></td></tr></table></figure><h1 id="优化w、b"><a href="#优化w、b" class="headerlink" title="优化w、b"></a>优化w、b</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    num_iterations  - 优化循环的迭代次数</span></span><br><span class="line"><span class="string">        learning_rate  - 梯度下降更新规则的学习率</span></span><br><span class="line"><span class="string">        print_cost  - 每100步打印一次损失值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    costs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        grads, cost = propagate(w,b,X,Y)</span><br><span class="line">        </span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line">        </span><br><span class="line">        w = w-learning_rate*dw</span><br><span class="line">        b = b-learning_rate*db</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        </span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">             <span class="string">"b"</span>: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">            <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> params,grads,costs</span><br></pre></td></tr></table></figure><h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m)) <span class="comment">#每一列对应一张图片</span></span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>) <span class="comment">#X.shape[0]代表一章图片的每一个颜色值，要和w权值相乘</span></span><br><span class="line">    </span><br><span class="line">    A = sigmoid(np.dot(w.T, X)+b) <span class="comment">#激活函数将值对应在0-1（logistics regression）</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span>(A[<span class="number">0</span>,i]&gt;<span class="number">0.5</span>):</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i]=<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>,i]=<span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line">    <span class="keyword">return</span> Y_prediction,A</span><br></pre></td></tr></table></figure><h1 id="制作模型"><a href="#制作模型" class="headerlink" title="制作模型"></a>制作模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    num_iterations  - 优化循环的迭代次数</span></span><br><span class="line"><span class="string">        learning_rate  - 梯度下降更新规则的学习率</span></span><br><span class="line"><span class="string">        print_cost  - 每100步打印一次损失值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line">    </span><br><span class="line">    w = parameters[<span class="string">"w"</span>]</span><br><span class="line">    b = parameters[<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    Y_prediction_test, A_test = predict(w, b, X_test)</span><br><span class="line">    Y_prediction_train, A_train = predict(w, b, X_train)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#打印错误预测</span></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line">    </span><br><span class="line">    d = &#123;<span class="string">"costs"</span>: costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </span><br><span class="line">         <span class="string">"w"</span> : w, </span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.004</span>, print_cost = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>Cost after iteration 0: 0.693147Cost after iteration 100: 0.506765Cost after iteration 200: 0.442269Cost after iteration 300: 0.397201Cost after iteration 400: 0.362439Cost after iteration 500: 0.334271Cost after iteration 600: 0.310725Cost after iteration 700: 0.290608Cost after iteration 800: 0.273138Cost after iteration 900: 0.257771Cost after iteration 1000: 0.244114Cost after iteration 1100: 0.231873Cost after iteration 1200: 0.220823Cost after iteration 1300: 0.210787Cost after iteration 1400: 0.201623Cost after iteration 1500: 0.193217Cost after iteration 1600: 0.185474Cost after iteration 1700: 0.178317Cost after iteration 1800: 0.171678Cost after iteration 1900: 0.165504train accuracy: 98.08612440191388 %test accuracy: 70.0 %</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 学习速率对损失函数影响的曲线</span></span><br><span class="line">costs = np.squeeze(d[<span class="string">'costs'</span>])</span><br><span class="line">plt.plot(costs)</span><br><span class="line">plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">plt.title(<span class="string">"Learning rate ="</span> + str(d[<span class="string">"learning_rate"</span>]))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/20200211174752993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="测试自己的图片"><a href="#测试自己的图片" class="headerlink" title="测试自己的图片"></a>测试自己的图片</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">my_image = <span class="string">"cat2.jpg"</span></span><br><span class="line"></span><br><span class="line">fname = <span class="string">"C:\\Users\\董润泽\\Desktop\\cat_picture\\"</span> + my_image</span><br><span class="line">image = np.array(ndimage.imread(fname, flatten=<span class="literal">False</span>))</span><br><span class="line">my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((<span class="number">1</span>, num_px*num_px*<span class="number">3</span>)).T</span><br><span class="line">my_predicted_image,A = predict(d[<span class="string">"w"</span>], d[<span class="string">"b"</span>], my_image)</span><br><span class="line"></span><br><span class="line">plt.imshow(image)</span><br><span class="line">print(<span class="string">"y = "</span> + str(np.squeeze(my_predicted_image)) + <span class="string">", your algorithm predicts a \""</span> + classes[int(np.squeeze(my_predicted_image)),].decode(<span class="string">"utf-8"</span>) +  <span class="string">"\" picture."</span>)</span><br><span class="line">print(A[<span class="number">0</span>][<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>y = 1.0, your algorithm predicts a &quot;cat&quot; picture.1.0C:\ProgramData\Anaconda3\lib\site-packages\ipykernel_launcher.py:4: DeprecationWarning: `imread` is deprecated!`imread` is deprecated in SciPy 1.0.0.Use ``matplotlib.pyplot.imread`` instead.  after removing the cwd from sys.path.C:\ProgramData\Anaconda3\lib\site-packages\ipykernel_launcher.py:5: DeprecationWarning: `imresize` is deprecated!`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.3.0.Use Pillow instead: ``numpy.array(Image.fromarray(arr).resize())``.  &quot;&quot;&quot;</code></pre><p><img src="https://img-blog.csdnimg.cn/20200211174807684.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>03深层神经网络</title>
      <link href="/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/02/12/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/29738823" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29738823</a></p><p>深层神经网络模型：<br><img src="https://img-blog.csdnimg.cn/20200212111650297.png" alt=""></p><p>看完前面的两个部分，这个深层神经网络就比较简单，原理相同。</p><h1 id="1、参数矩阵维度"><a href="#1、参数矩阵维度" class="headerlink" title="1、参数矩阵维度"></a><strong>1、参数矩阵维度</strong></h1><p><img src="https://img-blog.csdnimg.cn/20200212111930249.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="2、前向和反向传播"><a href="#2、前向和反向传播" class="headerlink" title="2、前向和反向传播"></a><strong>2、前向和反向传播</strong></h1><p>给定参数：</p><p><img src="https://img-blog.csdnimg.cn/20200212112503726.png" alt=""></p><p><strong>- 前向传播</strong></p><p><img src="https://img-blog.csdnimg.cn/20200212112614553.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><strong>- 反向传播</strong></p><p><img src="https://img-blog.csdnimg.cn/2020021211271436.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200212112740739.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h1 id="3、为什么要用深层表示"><a href="#3、为什么要用深层表示" class="headerlink" title="3、为什么要用深层表示"></a><strong>3、为什么要用深层表示</strong></h1><p><img src="https://pic2.zhimg.com/v2-07b30fc02b2b2decaa66edcbff69f6b5_r.jpg" alt=""></p><p>对于人脸识别，神经网络的第一层从原始图片中提取人脸的轮廓和边缘，每个神经元学习到不同边缘的信息；网络的第二层将第一层学得的边缘信息组合起来，形成人脸的一些局部的特征，例如眼睛、嘴巴等；后面的几层逐步将上一层的特征组合起来，形成人脸的模样。随着神经网络层数的增加，特征也从原来的边缘逐步扩展为人脸的整体，由整体到局部，由简单到复杂。层数越多，那么模型学习的效果也就越精确。</p><p>对于语音识别，第一层神经网络可以学习到语言发音的一些音调，后面更深层次的网络可以检测到基本的音素，再到单词信息，逐渐加深可以学到短语、句子。</p><p>所以从上面的两个例子可以看出随着神经网络的深度加深，模型能学习到更加复杂的问题，功能也更加强大。</p><p><strong>对于逻辑电路</strong><br><img src="https://pic1.zhimg.com/80/v2-d912f368562a4abdb0cf7b69589fdc64_hd.jpg" alt=""></p><p>假定计算异或逻辑输出：</p><p><img src="https://www.zhihu.com/equation?tex=y%3Dx_%7B1%7D%5Coplus+x_%7B2%7D%5Coplus+x_%7B3%7D%5Coplus+%5Ccdots%5Coplus+x_%7Bn%7D" alt=""></p><p>对于该运算，若果使用深度神经网络，每层将前一层的相邻的两单元进行异或，最后到一个输出，此时整个网络的层数为一个树形的形状，网络的深度为 [公式] ，共使用的神经元的个数为：</p><p><img src="https://www.zhihu.com/equation?tex=1%2B2%2B%5Ccdot%2B2%5E%7B%5Clog_%7B2%7D%28n%29-1%7D%3D1%5Ccdot+%5Cdfrac%7B1-2%5E%7B%5Clog_%7B2%7D%28n%29%7D%7D%7B1-2%7D%3D2%5E%7B%5Clog_%7B2%7D%28n%29%7D-1%3Dn-1" alt=""></p><p>即输入个数为n，输出个数为n-1。</p><p>但是如果不适用深层网络，仅仅使用单隐层的网络（如右图所示），需要的神经元个数为 <img src="https://www.zhihu.com/equation?tex=2%5E%7Bn-1%7D" alt=""> 个 。同样的问题，但是深层网络要比浅层网络所需要的神经元个数要少得多。</p><h1 id="4、参数和超参数"><a href="#4、参数和超参数" class="headerlink" title="4、参数和超参数"></a><strong>4、参数和超参数</strong></h1><p><img src="https://img-blog.csdnimg.cn/20200212113727546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>02浅层神经网络</title>
      <link href="/2020/02/11/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/02/11/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/29706138" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29706138</a></p><h1 id="1、神经网络表示"><a href="#1、神经网络表示" class="headerlink" title="1、神经网络表示"></a><strong>1、神经网络表示</strong></h1><p><img src="https://img-blog.csdnimg.cn/2020021109545357.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>看着这张浅层神经网络的示意图</p><p>第一列为输入层，x1,x2,x3；第二列为隐藏层，第三列为输出层。我们把这个神经网络成为两层神经网络（输入层不计）</p><p>我们要注意的是每两层之间的计算和每一层的参数矩阵大小</p><p><img src="https://pic2.zhimg.com/80/v2-57166f7b5bb26904e62433ffffd01e61_hd.jpg" alt=""></p><ul><li><p>输入层和隐藏层</p><ul><li>w–&gt;(4,3),4代表隐藏层神经元个数，3代表输入层神经元个数</li><li>b–&gt;(4,1)，4指隐藏层神经元数</li></ul></li><li><p>隐藏层和输出层</p><ul><li>w–&gt;(1,4),1指输出层，4指隐藏层</li><li>b–&gt;(1,1),1指输出层</li></ul></li></ul><p>在神经网络中，前一层是输入，后一层作为输出，两层之间，w参数矩阵大小为<strong>（N[out], N[in]）</strong>, b参数矩阵大小为<strong>（N[out], 1）</strong>,这里 <strong>z = wX + b</strong>,神经网络中w[i] = w.T</p><p>在logistic regression 中，用<strong>（N[in], N[out]）</strong>表示参数矩阵大小，公式是 <strong>z = w.T***</strong>X + b**</p><h1 id="2、神经网络的输出"><a href="#2、神经网络的输出" class="headerlink" title="2、神经网络的输出"></a><strong>2、神经网络的输出</strong></h1><p><img src="https://pic4.zhimg.com/80/v2-722991018136c95bff18be8e2932d987_hd.jpg" alt=""></p><p>只需要四个公式来控制这个神经网络的输出</p><p><img src="https://pic3.zhimg.com/80/v2-6eb2bc22d2d7953ab4fb4115a2a33a96_hd.jpg" alt=""></p><h1 id="3、向量化"><a href="#3、向量化" class="headerlink" title="3、向量化"></a><strong>3、向量化</strong></h1><p>对于每一个神经元x，都需要计算上面的四个公式，如果用for循环，速度十分慢。所以向量化就是使用矩阵来代替for循环，提高效率</p><p><img src="https://pic1.zhimg.com/80/v2-1615f9d56a1561b3aef3bc4deb895110_hd.jpg" alt=""></p><p>对于m个样本，它的参数矩阵为： （n_x,m）</p><h1 id="4、激活函数"><a href="#4、激活函数" class="headerlink" title="4、激活函数"></a><strong>4、激活函数</strong></h1><p><img src="https://pic4.zhimg.com/80/v2-58168196b7618a2dfe6b2f02a714fabb_hd.jpg" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200211104536645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>sigmoid函数和tanh函数比较：</p><ul><li><p>隐藏层：tanh函数的表现要好于sigmoid函数，因为tanh取值范围为 【-1，1】 ，输出分布在0值的附近，均值为0，从隐藏层到输出层数据起到了归一化（均值为0）的效果。<br>输出层：对于二分类任务的输出取值为 【0，1】 ，故一般会选择sigmoid函数。<br>然而sigmoid和tanh函数在当 |Z| 很大的时候，梯度会很小，在依据梯度的算法中，更新在后期会变得很慢。在实际应用中，要使 |Z| 尽可能的落在0值附近。</p></li><li><p>ReLU弥补了前两者的缺陷，当 Z&gt;0 时，梯度始终为1，从而提高神经网络基于梯度算法的运算速度。然而当 Z&lt;0 时，梯度一直为0，但是实际的运用中，该缺陷的影响不是很大。</p></li><li><p>Leaky ReLU保证在 Z&lt;0 的时候，梯度仍然不为0。</p></li></ul><p>在选择激活函数的时候，如果在不知道该选什么的时候就选择ReLU，当然也没有固定答案，要依据实际问题在交叉验证集合中进行验证分析。</p><h1 id="5、神经网络的梯度下降"><a href="#5、神经网络的梯度下降" class="headerlink" title="5、神经网络的梯度下降"></a><strong>5、神经网络的梯度下降</strong></h1><p><img src="https://img-blog.csdnimg.cn/2020021110591783.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><img src="https://pic3.zhimg.com/80/v2-88e08b98b1d6ee1acba5f79a326fbb82_hd.jpg" alt=""></p><h1 id="6、随机初始化"><a href="#6、随机初始化" class="headerlink" title="6、随机初始化"></a><strong>6、随机初始化</strong></h1><p><img src="https://img-blog.csdnimg.cn/2020021111032085.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>01神经网络基础</title>
      <link href="/2020/02/06/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
      <url>/2020/02/06/%E5%90%B4%E6%81%A9%E8%BE%BE%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ss0.bdstatic.com/70cFuHSh_Q1YnxGkpoWK1HF6hhy/it/u=2536090967,3947773569&fm=26&gp=0.jpg" alt=""></p><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/29688927" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29688927</a></p><h1 id="整体处理流程："><a href="#整体处理流程：" class="headerlink" title="整体处理流程："></a>整体处理流程：</h1><ul><li>初始化参数w、b</li><li>正向传播，计算损失函数J</li><li>反向传播，计算dw、db</li><li>梯度下降优化w、b</li><li>预测y</li></ul><h1 id="1-二分类问题"><a href="#1-二分类问题" class="headerlink" title="1.二分类问题"></a><strong>1.二分类问题</strong></h1><p>这一节主要讲了定义数据格式</p><p>例：</p><p>对于下面这张图片，把它看作64*64个像素点，那么对每个像素点，又可以分为红黄蓝三色：</p><p><img src="https://img-blog.csdnimg.cn/2020020610562129.png" alt=" "></p><p>我们可以得出需要的</p><p><img src="https://img-blog.csdnimg.cn/20200206110505637.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><p>而y=1/0 可以代表图片中有无猫</p><p><img src="https://img-blog.csdnimg.cn/20200206110730236.png" alt=" "></p><p><img src="https://img-blog.csdnimg.cn/20200206110931586.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><h1 id="2-logistic-regression"><a href="#2-logistic-regression" class="headerlink" title="2.logistic regression"></a><strong>2.logistic regression</strong></h1><p>上面的 Y^ = [0,1]表示的是一种概率。</p><p><img src="https://img-blog.csdnimg.cn/202002061618162.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><p>z趋于无穷大，y趋于1；z变小，y变小</p><p>控制z，即要获得w、b的值</p><p><img src="https://img-blog.csdnimg.cn/20200206162902528.png" alt=" "></p><p>符号惯例：θo 代表 b；其他的代表w（不同神经）</p><h1 id="3-logistic-regression损失函数-Loss-Function"><a href="#3-logistic-regression损失函数-Loss-Function" class="headerlink" title="3.logistic regression损失函数 Loss Function"></a><strong>3.logistic regression损失函数 Loss Function</strong></h1><p>一般经验来说是用平方错误衡量损失函数</p><p><img src="https://img-blog.csdnimg.cn/20200206171116302.png" alt=" "></p><p>但是，对于logistic regression 来说，一般不适用平方错误来作为Loss Function，这是因为上面的平方错误损失函数一般是非凸函数（non-convex），其在使用梯度下降算法的时候，容易得到局部最优解，而不是全局最优解。因此要选择凸函数。</p><p><img src="https://img-blog.csdnimg.cn/20200206171229963.png" alt=" "></p><p><img src="https://img-blog.csdnimg.cn/20200206171332982.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><h1 id="4-梯度下降法"><a href="#4-梯度下降法" class="headerlink" title="4.梯度下降法"></a><strong>4.梯度下降法</strong></h1><p>即以梯度下降的方式最小化代价函数cost function,最终得出w、b</p><p>每次迭代更新的修正表达式：</p><p><img src="https://img-blog.csdnimg.cn/20200206192504836.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><p>程序中分别用dw、db分别表示上面的偏导部分</p><h1 id="5-逻辑回归中的梯度下降法"><a href="#5-逻辑回归中的梯度下降法" class="headerlink" title="5.逻辑回归中的梯度下降法"></a><strong>5.逻辑回归中的梯度下降法</strong></h1><p><img src="https://img-blog.csdnimg.cn/20200207085458345.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><p>最后是通过梯度下降法中的修正表达式，得出w1、w2、b的值</p><h1 id="6-m个样本的梯度下降"><a href="#6-m个样本的梯度下降" class="headerlink" title="6.m个样本的梯度下降"></a><strong>6.m个样本的梯度下降</strong></h1><p><img src="https://img-blog.csdnimg.cn/20200207092426643.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><p>$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}$$</p><p>$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}$$</p><h1 id="7-向量化"><a href="#7-向量化" class="headerlink" title="7.向量化"></a><strong>7.向量化</strong></h1><p>向量化就是把原始的for循环用矩阵来代替，使用numpy中的矩阵运算直接算出来。</p><p>单次迭代梯度下降算法流程：</p><pre><code>Z = np.dot(w.T,X) + bA = sigmoid(Z)dZ = A-Ydw = 1/m*np.dot(X,dZ.T)db = 1/m*np.sum(dZ)w = w - alpha*dwb = b - alpha*db</code></pre><h1 id="8-关于numpy说明"><a href="#8-关于numpy说明" class="headerlink" title="8.关于numpy说明"></a><strong>8.关于numpy说明</strong></h1><p>当不确定矩阵的维度时，可以使用assert保证安全</p><pre><code>assert(a.shape == (5,1))</code></pre>]]></content>
      
      
      <categories>
          
          <category> 吴恩达DeepLearning </category>
          
          <category> 01神经网络和深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JAVA静态代理和动态代理</title>
      <link href="/2020/02/04/JVM/JAVA%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"/>
      <url>/2020/02/04/JVM/JAVA%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>参考文章：<a href="https://www.jianshu.com/p/9bcac608c714" target="_blank" rel="noopener">https://www.jianshu.com/p/9bcac608c714</a></p><h1 id="代理模式"><a href="#代理模式" class="headerlink" title="代理模式"></a>代理模式</h1><p>为其他对象提供一个代理以控制对某个对象的访问。代理类主要负责为委托了（真实对象）预处理消息、过滤消息、传递消息给委托类，代理类不现实具体服务，而是利用委托类来完成服务，并将执行结果封装处理。</p><p>代理类为被代理类预处理消息、过滤消息并在此之后将消息转发给被代理类，之后还能进行消息的后置处理。代理类和被代理类通常会存在关联关系(即上面提到的持有的被代理对象的引用)，代理类本身不实现服务，而是通过调用被代理类中的方法来提供服务。</p><h1 id="静态代理"><a href="#静态代理" class="headerlink" title="静态代理"></a>静态代理</h1><p>静态代理的模式：首先创建一个接口，然后被代理类实现这个接口，再创建一个代理类，代理类中创建一个被代理类对象的引用，然后代理类中完成被代理类的方法。</p><p>有点绕，只要记住一点：代理类主要负责预处理消息、过滤消息、传递消息，所以方法要在代理类中进行加工和调用。</p><p>下面来看代码：</p><p>1、创建接口</p><pre><code>public interface HelloInterface {    void sayHello();}</code></pre><p>2、创建被代理类实现接口：</p><pre><code>public class Hello implements HelloInterface{    public void sayHello() {        System.out.println(&quot;Hello&quot;);    }}</code></pre><p>3、创建代理类，在该类中，实现被代理类的引用以及方法调用和加工</p><pre><code>public class HelloProxy implements HelloInterface{    private HelloInterface helloInterface = (HelloInterface) new Hello();    public void sayHello() {        System.out.println(&quot;before invok&quot;);        helloInterface.sayHello();        System.out.println(&quot;after invok&quot;);    }}</code></pre><p>4、通过代理类对象，对目标方法进行调用</p><pre><code>public static void main(String[] args) {        HelloProxy hello = new HelloProxy();    }</code></pre><p>静态代理存在缺点：每个代理类只能为一个类服务，如果要代理的类很多，就需要编写大量的代理类。</p><p>例如，再来一个Bye类：</p><p>1、接口：</p><pre><code>public interface ByeInterface {    void sayBye();}</code></pre><p>2、接口实现</p><pre><code>public class Bye implements ByeInterface{    public void sayBye() {        System.out.println(&quot;GoodBye!&quot;);    }}</code></pre><p>3、代理类中引用方法</p><p>注意，这时候就会发现，原来的那个Hello代理不可以代理这个，必须要新建一个此方法的专属代理</p><pre><code>public class ByeProxy implements ByeInterface{    private ByeInterface byeInterface = (ByeInterface) new Bye();    public void sayBye(){        System.out.println(&quot;before invok&quot;);        byeInterface.sayBye();        System.out.println(&quot;after invok&quot;);    }}</code></pre><p>4、main方法也要new一个新代理的对象</p><pre><code>ByeProxy bye = new ByeProxy();bye.sayBye();</code></pre><p>结果：</p><p><img src="https://img-blog.csdnimg.cn/20200204111758538.png" alt=" "></p><h1 id="动态代理"><a href="#动态代理" class="headerlink" title="动态代理"></a>动态代理</h1><p>前两步相同：1、创建接口 ， 2、接口实现</p><p>3、代理：</p><pre><code>public class ProxyHandler implements InvocationHandler{    private Object object;    public ProxyHandler(Object object){        this.object = object;    }    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {        System.out.println(&quot;Before invok &quot;+method.getName());        method.invoke(object,args);        System.out.println(&quot;after invok &quot;+method.getName());        return null;    }}</code></pre><p>构建一个handler类来实现InvocationHandler接口</p><p>4、main</p><pre><code>public static void main(String[] args) {        HelloInterface hello = new Hello();        ByeInterface bye = new Bye();        InvocationHandler handler = new ProxyHandler(hello);        InvocationHandler handler2 = new ProxyHandler(bye);        HelloInterface proxyHello = (HelloInterface) Proxy.newProxyInstance(hello.getClass().getClassLoader(), hello.getClass().getInterfaces(), handler);        ByeInterface proxyBye = (ByeInterface) Proxy.newProxyInstance(bye.getClass().getClassLoader(), bye.getClass().getInterfaces(), handler2);        proxyHello.sayHello();        proxyBye.sayBye();    }</code></pre><p>结果：</p><p><img src="https://img-blog.csdnimg.cn/20200204113509359.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=" "></p><p>动态代理的代理不需要新建。</p><p><img src="https://img-blog.csdnimg.cn/20200204141934502.png" alt=" "><br><img src="https://img-blog.csdnimg.cn/20200204141955282.png" alt=" "></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 静态、动态代理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pandas实践-2012美国总统竞选赞助数据分析</title>
      <link href="/2020/02/01/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/pandas%E5%AE%9E%E8%B7%B5-2012%E7%BE%8E%E5%9B%BD%E6%80%BB%E7%BB%9F%E7%AB%9E%E9%80%89%E8%B5%9E%E5%8A%A9%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
      <url>/2020/02/01/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/pandas%E5%AE%9E%E8%B7%B5-2012%E7%BE%8E%E5%9B%BD%E6%80%BB%E7%BB%9F%E7%AB%9E%E9%80%89%E8%B5%9E%E5%8A%A9%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="数据载入和总览"><a href="#数据载入和总览" class="headerlink" title="数据载入和总览"></a>数据载入和总览</h1><pre><code>import numpy as npimport pandas as pdimport matplotlib.pyplot as plt#导入文件data = pd.read_csv(&quot;C:\\Users\\董润泽\\Desktop\\P00000001-ALL.csv&quot;)#打印前五行print(data.head())#查看数据的信息，包括每个字段的名称、非空数量、字段的数据类型print(data.info())#用统计学指标快速描述数据的概要data.describe()</code></pre><p>各个值含义：</p><pre><code>cand_nm – 接受捐赠的候选人姓名contbr_nm – 捐赠人姓名contbr_st – 捐赠人所在州contbr_employer – 捐赠人所在公司contbr_occupation – 捐赠人职业contb_receipt_amt – 捐赠数额（美元）contb_receipt_dt – 收到捐款的日期</code></pre><h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><h2 id="1、缺失值处理"><a href="#1、缺失值处理" class="headerlink" title="1、缺失值处理"></a>1、缺失值处理</h2><pre><code>data[&apos;contbr_employer&apos;].fillna(&apos;Not Provided&apos;)data[&apos;contbr_occupation&apos;].fillna(&apos;Not Provided&apos;)</code></pre><h2 id="2、数据转换"><a href="#2、数据转换" class="headerlink" title="2、数据转换"></a>2、数据转换</h2><pre><code>print(&apos;共有{}位候选人，分别是：&apos;.format(len(data[&apos;cand_nm&apos;].unique())))data[&apos;cand_nm&apos;].unique()</code></pre><p>结果：</p><pre><code>共有13位候选人，分别是：array([&apos;Bachmann, Michelle&apos;, &apos;Romney, Mitt&apos;, &apos;Obama, Barack&apos;,       &quot;Roemer, Charles E. &apos;Buddy&apos; III&quot;, &apos;Pawlenty, Timothy&apos;,       &apos;Johnson, Gary Earl&apos;, &apos;Paul, Ron&apos;, &apos;Santorum, Rick&apos;,       &apos;Cain, Herman&apos;, &apos;Gingrich, Newt&apos;, &apos;McCotter, Thaddeus G&apos;,       &apos;Huntsman, Jon&apos;, &apos;Perry, Rick&apos;], dtype=object)</code></pre><p>-</p><pre><code>#通过搜索引擎等途径，获取到每个总统候选人的所属党派，建立字典parties，候选人名字作为键，所属党派作为对应的值parties = {&apos;Bachmann, Michelle&apos;: &apos;Republican&apos;,           &apos;Cain, Herman&apos;: &apos;Republican&apos;,           &apos;Gingrich, Newt&apos;: &apos;Republican&apos;,           &apos;Huntsman, Jon&apos;: &apos;Republican&apos;,           &apos;Johnson, Gary Earl&apos;: &apos;Republican&apos;,           &apos;McCotter, Thaddeus G&apos;: &apos;Republican&apos;,           &apos;Obama, Barack&apos;: &apos;Democrat&apos;,           &apos;Paul, Ron&apos;: &apos;Republican&apos;,           &apos;Pawlenty, Timothy&apos;: &apos;Republican&apos;,           &apos;Perry, Rick&apos;: &apos;Republican&apos;,           &quot;Roemer, Charles E. &apos;Buddy&apos; III&quot;: &apos;Republican&apos;,           &apos;Romney, Mitt&apos;: &apos;Republican&apos;,           &apos;Santorum, Rick&apos;: &apos;Republican&apos;}#通过map映射函数，增加一列party存储党派信息data[&apos;party&apos;] = data[&apos;cand_nm&apos;].map(parties)#查看两个党派的情况data[&apos;party&apos;].value_counts()#按照职业汇总对赞助总金额进行排序#DataFrame.sort_values(by, ascending=True, inplace=False)#by是根据哪一列进行排序，可以传入多列；ascending=True是升序排序，False为降序；inplace=Ture则是修改原dataframe，默认为Falsedata.groupby(&apos;contbr_occupation&apos;)[&apos;contb_receipt_amt&apos;].sum().sort_values(ascending=False)[:20]</code></pre><p>##3、数据筛选</p><pre><code>#查看各候选人获得的赞助总金额data.groupby(&apos;cand_nm&apos;)[&apos;contb_receipt_amt&apos;].sum().sort_values(ascending=False)#选取候选人为Obama、Romney的子集数据data_vs = data[data[&apos;cand_nm&apos;].isin([&apos;Obama, Barack&apos;,&apos;Romney, Mitt&apos;])].copy()</code></pre><h1 id="数据聚合与分组运算"><a href="#数据聚合与分组运算" class="headerlink" title="数据聚合与分组运算"></a>数据聚合与分组运算</h1><p>##1、透视表(pivot_table)分析党派和职业</p><pre><code>#按照党派、职业对赞助金额进行汇总，类似excel中的透视表操作，聚合函数为sumby_occupation = data.pivot_table(&apos;contb_receipt_amt&apos;,index=&apos;contbr_occupation&apos;,columns=&apos;party&apos;,aggfunc=&apos;sum&apos;)#过滤掉赞助金额小于200W的数据over_2mm = by_occupation[by_occupation.sum(1)&gt;2000000]over_2mm</code></pre><p>-</p><p>#由于职业和雇主的处理非常相似，我们定义函数get_top_amounts()对两个字段进行分析处理</p><pre><code>def get_top_amounts(group,key,n=5):#传入groupby分组后的对象，返回按照key字段汇总的排序前n的数据    totals = group.groupby(key)[&apos;contb_receipt_amt&apos;].sum()    return totals.sort_values(ascending=False)[:n]grouped = data_vs.groupby(&apos;cand_nm&apos;)grouped.apply(get_top_amounts,&apos;contbr_occupation&apos;,n=7)#同样的，使用get_top_amounts()对雇主进行分析处理grouped.apply(get_top_amounts,&apos;contbr_employer&apos;,n=10)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据挖掘 </category>
          
          <category> pandas </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pandas</title>
      <link href="/2020/02/01/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/pandas/"/>
      <url>/2020/02/01/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/pandas/</url>
      
        <content type="html"><![CDATA[<h1 id="数据结构简介"><a href="#数据结构简介" class="headerlink" title="数据结构简介"></a>数据结构简介</h1><h2 id="1、Series"><a href="#1、Series" class="headerlink" title="1、Series"></a>1、Series</h2><pre><code>import numpy as np,pandas as pdarr1 = np.arange(10)print(arr1)print(type(arr1))</code></pre><h3 id="通过一维数组创建"><a href="#通过一维数组创建" class="headerlink" title="- 通过一维数组创建"></a>- 通过一维数组创建</h3><pre><code>s1 = pd.Series(arr1)print(s1)print(type(s1))</code></pre><p>结果：</p><pre><code>0    01    12    23    34    45    56    67    78    89    9</code></pre><h3 id="通过字典创建"><a href="#通过字典创建" class="headerlink" title="- 通过字典创建"></a>- 通过字典创建</h3><pre><code>dic = {&apos;a&apos;:10,&apos;b&apos;:20,&apos;c&apos;:30,&apos;d&apos;:40}s2 = pd.Series(dic)print(s2)</code></pre><p>结果：</p><pre><code>a    10b    20c    30d    40</code></pre><h2 id="2、DataFrame"><a href="#2、DataFrame" class="headerlink" title="2、DataFrame"></a>2、DataFrame</h2><h3 id="二维数组创建数据框"><a href="#二维数组创建数据框" class="headerlink" title="- 二维数组创建数据框"></a>- 二维数组创建数据框</h3><pre><code>arr2 = np.array(np.arange(12)).reshape(4,3)print(arr2)df1 = pd.DataFrame(arr2)print(df1)</code></pre><p>结果：</p><pre><code>   0   1   20  0   1   21  3   4   52  6   7   83  9  10  11</code></pre><h3 id="字典创建数据框"><a href="#字典创建数据框" class="headerlink" title="- 字典创建数据框"></a>- 字典创建数据框</h3><pre><code>dic2 = {&apos;a&apos;:[1,2,3,4],&apos;b&apos;:[5,6,7,8],&apos;c&apos;:[9,0,11,21]}print(dic2)df2 = pd.DataFrame(dic2)print(df2)dic3 = {&apos;one&apos;:{&apos;a&apos;:1,&apos;b&apos;:2,&apos;c&apos;:3,&apos;d&apos;:4},&apos;two&apos;:{&apos;a&apos;:5,&apos;b&apos;:6,&apos;c&apos;:7,&apos;d&apos;:8},&apos;three&apos;:{&apos;a&apos;:9,&apos;b&apos;:10,&apos;c&apos;:11,&apos;d&apos;:12}}df3 = pd.DataFrame(dic3)print(df3)</code></pre><p>结果：</p><pre><code>{&apos;a&apos;: [1, 2, 3, 4], &apos;b&apos;: [5, 6, 7, 8], &apos;c&apos;: [9, 0, 11, 21]}   a  b   c0  1  5   91  2  6   02  3  7  113  4  8  21   one  two  threea    1    5      9b    2    6     10c    3    7     11d    4    8     12</code></pre><h3 id="通过数据框的方式创建数据框"><a href="#通过数据框的方式创建数据框" class="headerlink" title="通过数据框的方式创建数据框"></a>通过数据框的方式创建数据框</h3><pre><code>df4 = df3[[&apos;one&apos;,&apos;two&apos;]]print(df4)</code></pre><p>结果：</p><pre><code>   one  twoa    1    5b    2    6c    3    7d    4    8</code></pre><p>#数据索引 index<br>##通过索引值或索引标签获取数据</p><pre><code>s4 = pd.Series(np.array(np.arange(5)))print(s4)print(s4.index)s4.index = [&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;]print(s4)print(s4[3],s4[&apos;a&apos;])</code></pre><p>结果：</p><pre><code>0    01    12    23    34    4dtype: int32RangeIndex(start=0, stop=5, step=1)a    0b    1c    2d    3e    4dtype: int323 0</code></pre><h2 id="自动化对齐"><a href="#自动化对齐" class="headerlink" title="自动化对齐"></a>自动化对齐</h2><pre><code>s5=pd.Series(np.array([1,2,3,21,12,325]),index=[&apos;a&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;,&apos;f&apos;])print(s5)s6=pd.Series(np.array([1,2,3,21,12,325]),index=[&apos;b&apos;,&apos;b&apos;,&apos;c&apos;,&apos;d&apos;,&apos;e&apos;,&apos;f&apos;])print(s5+s6)</code></pre><p>结果：</p><pre><code>a      1b      2c      3d     21e     12f    325dtype: int32a      NaNb      3.0b      4.0c      6.0d     42.0e     24.0f    650.0dtype: float64</code></pre><h1 id="利用pandas查询数据"><a href="#利用pandas查询数据" class="headerlink" title="利用pandas查询数据"></a>利用pandas查询数据</h1><p>导入数据<br>    import pandas as pd</p><pre><code>stu_dic = {&apos;Age&apos;:[14,13,13,14,14,12,12,15,13,12,11,14,12,15,16,12,15,11,15],&apos;Height&apos;:[69,56.5,65.3,62.8,63.5,57.3,59.8,62.5,62.5,59,51.3,64.3,56.3,66.5,72,64.8,67,57.5,66.5],&apos;Name&apos;:[&apos;Alfred&apos;,&apos;Alice&apos;,&apos;Barbara&apos;,&apos;Carol&apos;,&apos;Henry&apos;,&apos;James&apos;,&apos;Jane&apos;,&apos;Janet&apos;,&apos;Jeffrey&apos;,&apos;John&apos;,&apos;Joyce&apos;,&apos;Judy&apos;,&apos;Louise&apos;,&apos;Marry&apos;,&apos;Philip&apos;,&apos;Robert&apos;,&apos;Ronald&apos;,&apos;Thomas&apos;,&apos;Willam&apos;],&apos;Sex&apos;:[&apos;M&apos;,&apos;F&apos;,&apos;F&apos;,&apos;F&apos;,&apos;M&apos;,&apos;M&apos;,&apos;F&apos;,&apos;F&apos;,&apos;M&apos;,&apos;M&apos;,&apos;F&apos;,&apos;F&apos;,&apos;F&apos;,&apos;F&apos;,&apos;M&apos;,&apos;M&apos;,&apos;M&apos;,&apos;M&apos;,&apos;M&apos;],&apos;Weight&apos;:[112.5,84,98,102.5,102.5,83,84.5,112.5,84,99.5,50.5,90,77,112,150,128,133,85,112]}student = pd.DataFrame(stu_dic)</code></pre><p>-</p><pre><code>print(student.head())print(student.tail())print(student.loc[[1,3,4,5]])print(student[[&apos;Name&apos;]].head())print(student.loc[:3,[&apos;Name&apos;]])print(student[(student[&apos;Sex&apos;]==&apos;M&apos;) &amp; (student[&apos;Age&apos;]&gt;14)])print(student[(student[&apos;Sex&apos;]==&apos;M&apos;) &amp; (student[&apos;Age&apos;]&gt;14)][[&apos;Name&apos;]])</code></pre><p>结果：</p><pre><code>  Age  Height     Name Sex  Weight0   14    69.0   Alfred   M   112.51   13    56.5    Alice   F    84.02   13    65.3  Barbara   F    98.03   14    62.8    Carol   F   102.54   14    63.5    Henry   M   102.5    Age  Height    Name Sex  Weight14   16    72.0  Philip   M   150.015   12    64.8  Robert   M   128.016   15    67.0  Ronald   M   133.017   11    57.5  Thomas   M    85.018   15    66.5  Willam   M   112.0   Age  Height   Name Sex  Weight1   13    56.5  Alice   F    84.03   14    62.8  Carol   F   102.54   14    63.5  Henry   M   102.55   12    57.3  James   M    83.0      Name0   Alfred1    Alice2  Barbara3    Carol4    Henry      Name0   Alfred1    Alice2  Barbara3    Carol    Age  Height    Name Sex  Weight14   16    72.0  Philip   M   150.016   15    67.0  Ronald   M   133.018   15    66.5  Willam   M   112.0      Name14  Philip16  Ronald18  Willam</code></pre><h1 id="利用pandas的DataFrames进行统计分析"><a href="#利用pandas的DataFrames进行统计分析" class="headerlink" title="利用pandas的DataFrames进行统计分析"></a>利用pandas的DataFrames进行统计分析</h1><pre><code>np.random.seed(1234)d1 = pd.Series(2*np.random.normal(size = 100)+3)d2 = np.random.f(2,4,size = 100)d3 = np.random.randint(1,100,size = 100)print(d1)print(d2)print(d3)print(&apos;非空元素计算: &apos;, d1.count()) #非空元素计算print(&apos;最小值: &apos;, d1.min()) #最小值print(&apos;最大值: &apos;, d1.max()) #最大值print(&apos;最小值的位置: &apos;, d1.idxmin()) #最小值的位置，类似于R中的which.min函数print(&apos;最大值的位置: &apos;, d1.idxmax()) #最大值的位置，类似于R中的which.max函数print(&apos;10%分位数: &apos;, d1.quantile(0.1)) #10%分位数print(&apos;求和: &apos;, d1.sum()) #求和print(&apos;均值: &apos;, d1.mean()) #均值print(&apos;中位数: &apos;, d1.median()) #中位数print(&apos;众数: &apos;, d1.mode()) #众数print(&apos;方差: &apos;, d1.var()) #方差print(&apos;标准差: &apos;, d1.std()) #标准差print(&apos;平均绝对偏差: &apos;, d1.mad()) #平均绝对偏差print(&apos;偏度: &apos;, d1.skew()) #偏度print(&apos;峰度: &apos;, d1.kurt()) #峰度print(&apos;描述性统计指标: &apos;, d1.describe()) #一次性输出多个描述性统计指标</code></pre><p>结果：</p><pre><code>非空元素计算:  100最小值:  -4.1270333212494705最大值:  7.781921030926066最小值的位置:  81最大值的位置:  3910%分位数:  0.6870184644069928求和:  307.0224566250873均值:  3.070224566250874中位数:  3.204555266776845众数:  0    -4.1270331    -1.8009072    -1.4853703    -1.1499554    -1.0425105    -0.6340546    -0.0938117     0.1083808     0.1960539     0.61804910    0.69468211    0.71473712    0.86202213    0.94429914    1.05152715    1.14749116    1.20568617    1.42913018    1.55882319    1.68806120    1.72695321    1.83056422    1.86710823    1.90351524    1.97623725    2.06138926    2.13980927    2.20007128    2.20432029    2.310468        ...   70    4.13147771    4.26395972    4.35110873    4.40845674    4.40944175    4.51082876    4.53473777    4.63318878    4.68201879    4.68334980    4.71917781    4.72743582    4.77432683    4.90664884    4.96984085    4.98389286    5.06760187    5.09187788    5.09515789    5.11793890    5.30007191    5.63630392    5.64221193    5.64231694    5.78397295    5.86541496    6.09131897    7.01568698    7.06120799    7.781921Length: 100, dtype: float64方差:  4.005609378535085标准差:  2.0014018533355777平均绝对偏差:  1.5112880411556109偏度:  -0.6494780760484293峰度:  1.2201094052398012描述性统计指标:  count    100.000000mean       3.070225std        2.001402min       -4.12703325%        2.04010150%        3.20455575%        4.434788max        7.781921dtype: float64</code></pre><h1 id="利用pandas实现SQL操作"><a href="#利用pandas实现SQL操作" class="headerlink" title="利用pandas实现SQL操作"></a>利用pandas实现SQL操作</h1><pre><code>dic = {&apos;Name&apos;:[&apos;LiuShunxiang&apos;,&apos;Zhangshan&apos;],&apos;Sex&apos;:[&apos;M&apos;,&apos;F&apos;],&apos;Age&apos;:[27,23],&apos;Height&apos;:[165.7,167.2],&apos;Weight&apos;:[61,63]}student2 = pd.DataFrame(dic)print(student2)</code></pre><h2 id="增"><a href="#增" class="headerlink" title="增"></a>增</h2><pre><code>student3 = pd.concat([student,student2])print(student3)#新增一列print(pd.DataFrame(student2, columns=[&apos;Age&apos;,&apos;Height&apos;,&apos;Name&apos;,&apos;Sex&apos;,&apos;Weight&apos;,&apos;Score&apos;]))</code></pre><h2 id="删"><a href="#删" class="headerlink" title="删"></a>删</h2><pre><code>del(student3)print(student3)#删除指定行print(student.drop([0,3,5,1]))#删除指定列print(student.drop([&apos;Height&apos;,&apos;Weight&apos;],axis=1).head())</code></pre><p>#利用pandas进行缺失值的处理</p><pre><code>df = pd.DataFrame([[1,1,2],[3,5,np.nan],[13,21,34],[55,np.nan,10],[np.nan,np.nan,np.nan],[np.nan,1,2]],columns=(&apos;x1&apos;,&apos;x2&apos;,&apos;x3&apos;))print(df)#直接删除print(df.dropna())#用0填充print(df.fillna(0))########print(&quot;采用前倾填充、向后填充&quot;)##采用前倾填充、向后填充print(df.fillna(method=&apos;ffill&apos;))print(df.fillna(method=&apos;bfill&apos;))#######print(&quot;#用常量填充不同的列&quot;)#用常量填充不同的列x1_median=df[&apos;x1&apos;].median()x2_mean=df[&apos;x2&apos;].mean()x3_mean=df[&apos;x3&apos;].mean()print(x1_median)print(x2_mean)print(x3_mean)print(df.fillna({&apos;x1&apos;:x1_median,&apos;x2&apos;:x2_mean,&apos;x3&apos;:x3_mean}))</code></pre><p>结果：</p><pre><code>     x1    x2    x30   1.0   1.0   2.01   3.0   5.0   NaN2  13.0  21.0  34.03  55.0   NaN  10.04   NaN   NaN   NaN5   NaN   1.0   2.0     x1    x2    x30   1.0   1.0   2.02  13.0  21.0  34.0     x1    x2    x30   1.0   1.0   2.01   3.0   5.0   0.02  13.0  21.0  34.03  55.0   0.0  10.04   0.0   0.0   0.05   0.0   1.0   2.0采用前倾填充、向后填充     x1    x2    x30   1.0   1.0   2.01   3.0   5.0   2.02  13.0  21.0  34.03  55.0  21.0  10.04  55.0  21.0  10.05  55.0   1.0   2.0     x1    x2    x30   1.0   1.0   2.01   3.0   5.0  34.02  13.0  21.0  34.03  55.0   1.0  10.04   NaN   1.0   2.05   NaN   1.0   2.0#用常量填充不同的列8.07.012.0     x1    x2    x30   1.0   1.0   2.01   3.0   5.0  12.02  13.0  21.0  34.03  55.0   7.0  10.04   8.0   7.0  12.05   8.0   1.0   2.0</code></pre><p>#利用pandas实现Excel的数据透视表功能</p><pre><code>Table2 = pd.pivot_table(student, values=[&apos;Height&apos;,&apos;Weight&apos;], columns=[&apos;Sex&apos;,&apos;Age&apos;]).unstack()print(Table2)</code></pre><p>结果：</p><pre><code>Age           11          12    13      14      15     16       Sex                                               Height F    51.3   58.050000  60.9   63.55   64.50    NaN       M    57.5   60.366667  62.5   66.25   66.75   72.0Weight F    50.5   80.750000  91.0   96.25  112.25    NaN       M    85.0  103.500000  84.0  107.50  122.50  150.0</code></pre><p>-</p><pre><code>Table2 = pd.pivot_table(student, values=[&apos;Height&apos;,&apos;Weight&apos;], columns=[&apos;Sex&apos;,&apos;Age&apos;],aggfunc=[np.mean,np.median,np.std]).unstack()print(Table2)</code></pre><p>结果：</p><pre><code>            mean                                          median               \Age           11          12    13      14      15     16     11     12    13          Sex                                                                      Height F    51.3   58.050000  60.9   63.55   64.50    NaN   51.3  58.05  60.9          M    57.5   60.366667  62.5   66.25   66.75   72.0   57.5  59.00  62.5   Weight F    50.5   80.750000  91.0   96.25  112.25    NaN   50.5  80.75  91.0          M    85.0  103.500000  84.0  107.50  122.50  150.0   85.0  99.50  84.0                                     std                                 \Age             14      15     16  11         12        13        14          Sex                                                             Height F     63.55   64.50    NaN NaN   2.474874  6.222540  1.060660          M     66.25   66.75   72.0 NaN   3.932345       NaN  3.889087   Weight F     96.25  112.25    NaN NaN   5.303301  9.899495  8.838835          M    107.50  122.50  150.0 NaN  22.765105       NaN  7.071068   Age                15  16         Sex                 Height F     2.828427 NaN         M     0.353553 NaN  Weight F     0.353553 NaN         M    14.849242 NaN  </code></pre><h1 id="多层索引的使用"><a href="#多层索引的使用" class="headerlink" title="多层索引的使用"></a>多层索引的使用</h1><pre><code>data = pd.DataFrame(np.random.randint(0,150,size=(8,12)),               columns = pd.MultiIndex.from_product([[&apos;模拟考&apos;,&apos;正式考&apos;],                                                   [&apos;数学&apos;,&apos;语文&apos;,&apos;英语&apos;,&apos;物理&apos;,&apos;化学&apos;,&apos;生物&apos;]]),               index = pd.MultiIndex.from_product([[&apos;期中&apos;,&apos;期末&apos;],                                                   [&apos;雷军&apos;,&apos;李斌&apos;],                                                  [&apos;测试一&apos;,&apos;测试二&apos;]]))</code></pre><p><img src="https://img-blog.csdnimg.cn/2020020112342850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200201123504657.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 数据挖掘 </category>
          
          <category> pandas </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NumPy</title>
      <link href="/2020/01/31/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/NumPy/"/>
      <url>/2020/01/31/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/NumPy/</url>
      
        <content type="html"><![CDATA[<h1 id="NumPy性能和python列表对比"><a href="#NumPy性能和python列表对比" class="headerlink" title="NumPy性能和python列表对比"></a>NumPy性能和python列表对比</h1><pre><code>import numpy as npmy_arr = np.arange(1000000)my_list = list(range(1000000))%time for _ in range(10): my_arr2 = my_arr*2%time for _ in range(10): my_list2 = [x*2 for x in my_list]</code></pre><p>结果</p><pre><code>Wall time: 16 msWall time: 636 ms</code></pre><h1 id="创建ndarray"><a href="#创建ndarray" class="headerlink" title="创建ndarray"></a>创建ndarray</h1><pre><code>data1 = [1,23,4.6,2]arr1 = np.array(data1)arr1data2 = [[1,2,3],        [4,5,6]]arr2 = np.array(data2)arr2print(arr2.ndim)print(arr2.shape)np.zeros((2,3))int_array = np.arange(10)calibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)int_array.astype(calibers.dtype)</code></pre><h1 id="numpy数组的运算"><a href="#numpy数组的运算" class="headerlink" title="numpy数组的运算"></a>numpy数组的运算</h1><pre><code>arr = np.array([[1., 2., 3.],               [4., 5., 6.]])print(arr*arr)print(arr*arr == arr)</code></pre><p>结果</p><pre><code>[[ 1.  4.  9.] [16. 25. 36.]][[False  True  True] [ True  True  True]]</code></pre><h1 id="基本索引和切片"><a href="#基本索引和切片" class="headerlink" title="基本索引和切片"></a>基本索引和切片</h1><pre><code>arr = np.arange(10)print(arr)print(arr[5])arr[5:8]=12print(arr[5:8])print(arr)</code></pre><p>创建一个切片后，对切片进行操作，会改变原数组中的值</p><pre><code>arr_slice = arr[5:8]print(arr_slice)arr_slice[1] = 12345print(arr)</code></pre><p>结果：</p><pre><code>[12 12 12][    0     1     2     3     4    12 12345    12     8     9]</code></pre><p>若想要得到的是一个副本而不是视图，需要明确<strong>arr[5:8].copy()</strong></p><h1 id="布尔类型索引"><a href="#布尔类型索引" class="headerlink" title="布尔类型索引"></a>布尔类型索引</h1><pre><code>names = np.array([&apos;Bob&apos;, &apos;Joe&apos;, &apos;Will&apos;, &apos;Joe&apos;, &apos;Joe&apos;])data = np.random.randn(5,4)print(data)print(names == &apos;Bob&apos;)print(data[names == &apos;Bob&apos;])print(names != &apos;Bob&apos;)print(data[~(names == &apos;Bob&apos;)])</code></pre><p>结果：</p><pre><code>array([[-0.72559713,  0.5021692 , -0.40538323,  1.03159213],       [ 0.72144716,  0.9853986 , -1.90579109, -0.42454087],       [-0.35674934,  0.12369675,  1.71308068,  0.61302904],       [ 0.86463033, -0.97788801, -0.07978174, -0.44386613],       [ 1.47681259,  1.09531974, -0.16154207,  1.92472807]])array([ True, False, False, False, False])array([[-0.72559713,  0.5021692 , -0.40538323,  1.03159213]])array([[ 0.72144716,  0.9853986 , -1.90579109, -0.42454087],       [-0.35674934,  0.12369675,  1.71308068,  0.61302904],       [ 0.86463033, -0.97788801, -0.07978174, -0.44386613],       [ 1.47681259,  1.09531974, -0.16154207,  1.92472807]])</code></pre><h1 id="数组转置和轴对换"><a href="#数组转置和轴对换" class="headerlink" title="数组转置和轴对换"></a>数组转置和轴对换</h1><pre><code>arr = np.arange(16).reshape((2,2,4))print(arr)arr.transpose((1, 0, 2))</code></pre><p>结果：</p><pre><code>[[[ 0  1  2  3]  [ 4  5  6  7]] [[ 8  9 10 11]  [12 13 14 15]]]array([[[ 0,  1,  2,  3],        [ 8,  9, 10, 11]],       [[ 4,  5,  6,  7],        [12, 13, 14, 15]]])</code></pre><p>swapaxes返回源数据的视图</p><pre><code>print(arr.swapaxes(1,2))[[[ 0  4]  [ 1  5]  [ 2  6]  [ 3  7]] [[ 8 12]  [ 9 13]  [10 14]  [11 15]]]</code></pre><p>#将条件逻辑表述为数组运算</p><pre><code>x = np.array([1,2,3,4,5])y = np.array([4,6,8,9,0])cond = np.array([True,False,True,False,True])result = np.where(cond,x,y)print(result)</code></pre><p>结果：</p><pre><code>[1 6 3 9 5]</code></pre><p>where：当cond为真则x，否则为y</p><p>#数学和统计方法</p><ul><li>mean 算数平均数</li><li>std、var 标准差、方差</li><li>argmin，argmax 最小最大元素索引</li><li>cumsum 所有元素累计和</li><li>cumprod 所有元素累计积</li></ul><p>#排序</p><pre><code>np.sort(a, axis, kind, order)</code></pre><ol><li>a:要排序的数组</li><li>axis：按照排序数组的轴</li><li>kind默认为“quicksort”</li><li>order：若数组包含字段，则为要排序的字段</li></ol><p>#唯一化以及其他的集合逻辑<br>    names = np.array([‘Bob’, ‘Joe’, ‘Will’, ‘Joe’, ‘Joe’])<br>    print(np.unique(names))<br>结果：</p><pre><code>[&apos;Bob&apos; &apos;Joe&apos; &apos;Will&apos;]</code></pre>]]></content>
      
      
      <categories>
          
          <category> 数据挖掘 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NumPy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java通信——传文件、消息</title>
      <link href="/2020/01/29/Java/Java%E9%80%9A%E4%BF%A1%E2%80%94%E2%80%94%E4%BC%A0%E6%96%87%E4%BB%B6/"/>
      <url>/2020/01/29/Java/Java%E9%80%9A%E4%BF%A1%E2%80%94%E2%80%94%E4%BC%A0%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="服务器Server端"><a href="#服务器Server端" class="headerlink" title="服务器Server端"></a>服务器Server端</h1><p>首先，创建服务器，循环等待接入客户端</p><pre><code>java.net.ServerSocket server = new java.net.ServerSocket(port);//创建服务器java.net.Socket client = server.accept();//客户端接入</code></pre><p>接入客户端之后，开始接受消息。</p><p>在这里，接受的消息不只是一个简单的内容，还含有消息头，消息头中含有有关消息的规范。</p><ol><li>消息长度——一个int（4字节）</li><li>消息类型——一个byte（1字节）</li><li>目标用户号码——一个int（4字节）</li></ol><p>消息类型这里分为两种：</p><ul><li>1代表普通消息</li><li>2代表文件</li></ul><h2 id="接收普通消息"><a href="#接收普通消息" class="headerlink" title="接收普通消息"></a>接收普通消息</h2><pre><code>byte[] data = new byte[totalLen-4-1-4];//从流中读取data.length个字节放入数组dins.readFully(data);String msg = new String(data);System.out.println(&quot;发送文本给：&quot;+destNum+&quot;内容是：&quot;+msg);</code></pre><p>这里注意readfully和read的区别：</p><p><strong>网络通信中，当发送大的数据量时，有这样一种可能：一部<br>分数据已发送到对方，有一部分数据还在本地的网卡缓存中，如果调用 read()方法，可能会提前<br>返回而没有读取到足够的数据，在传送大块数据（如一次传送一个较大文件时）可能会出错，而<br>readFully()方法会一直等待，读取到数组长度的所有数据后，才会返回。</strong></p><p>这里是个大坑，我之前传图片会出现传送之后，接受到的图片只显示一半。原因就在此。</p><p>我用read进行了测试，结果生成的图片虽然大小一模一样，但是生成图片无法显示！！！</p><h2 id="接收文件"><a href="#接收文件" class="headerlink" title="接收文件"></a>接收文件</h2><pre><code>//256个字节作为文件名大小System.out.println(&quot;发送文件给：&quot;+destNum);    //destnum为接收者byte[] data = new byte[256];dins.readFully(data);String fileName = new String (data).trim();System.out.println(&quot;读到文件名字是：&quot;+fileName);//读文件内容data = new byte[totalLen-4-1-4-256];dins.readFully(data);//保存在当前目录下FileOutputStream fous = new FileOutputStream(fileName);fous.write(data);fous.flush();    //保证可靠fous.close();</code></pre><p>关于flush：原文链接：<a href="https://blog.csdn.net/qq_38129062/article/details/87115620" target="_blank" rel="noopener">https://blog.csdn.net/qq_38129062/article/details/87115620</a></p><p><strong>设想要给鱼缸换水，所以需要一个水泵，水泵是连接鱼缸和下水道的，咱们的任务就是将鱼缸里面水全抽干，这时，我们就可以把水管当做缓冲区。如果咱们一见鱼缸里面水抽干了就立马关了水泵，这时会发现水管里还有来不及通过水泵流向下水道的残留水，我们可以把抽水当做读数据，排水当做写数据，水管当做缓冲区，这样就容易明白了。</strong></p><p><strong>那么这样一来我们如果中途调用close()方法，输出区也还是有数据的，就像水缸里有水，只是在缓冲区遗留了一部分，这时如果我们先调用flush()方法，就会强制把数据输出，缓存区就清空了，最后再关闭读写流调用close()就完成了。</strong></p><p>收文件时，用了FileOutputStream，文件输出流，它把文件直接写在了当前目录下。<br>可以直接规定文件输出的位置：</p><pre><code>FileOutputStream fous = new FileOutputStream(&quot;C:\\Users\\董润泽\\Desktop&quot;);</code></pre><h1 id="客户机client端"><a href="#客户机client端" class="headerlink" title="客户机client端"></a>客户机client端</h1><p>首先创建客户端并通过ip和主机端口连接到服务器</p><pre><code>java.net.Socket client = new java.net.Socket(ip,port);</code></pre><p>然后获取输入流和输出流：</p><pre><code>java.io.InputStream ins = client.getInputStream();OutputStream ous = client.getOutputStream();</code></pre><p>在while循环内持续发送信息</p><h2 id="发送普通消息"><a href="#发送普通消息" class="headerlink" title="发送普通消息"></a>发送普通消息</h2><pre><code>byte[] strb = msg.getBytes();//得到消息的字节数int totalLen = 4+4+1+strb.length;System.out.println(&quot;发送总长为：&quot;+totalLen);dous.writeInt(totalLen);dous.writeByte(1);//类型1代表文本dous.writeInt(destNum);//写入目标用户号dous.write(strb);dous.flush();</code></pre><p>其中dous为数据输出流</p><pre><code>private DataOutputStream dous;</code></pre><h2 id="发送文件"><a href="#发送文件" class="headerlink" title="发送文件"></a>发送文件</h2><pre><code>//根据文件名创建文件对象File file = new File(fileName);//根据文件对象构造一个输入流FileInputStream ins = new FileInputStream(file);int fileDataLen = ins.available(); //文件数据总长int totalLen = 4+1+4+256+fileDataLen; //得到了要发送数据包的总长dous.writeInt(totalLen);dous.writeByte(2);dous.writeInt(destNum);String shortFileName = file.getName();//写入文件名writeString(dous,shortFileName,256);byte[] fileData = new byte[fileDataLen];ins.read(fileData);//读入文件数据dous.write(fileData);//写到服务器的流中dous.flush();</code></pre><p>注意：发送文本和文件的正文前，首先要发出文件头！</p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><h3 id="server"><a href="#server" class="headerlink" title="server"></a>server</h3><pre><code>package qq_Server_test;import java.io.DataInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.io.OutputStream;import java.net.ServerSocket;import org.omg.CORBA.portable.InputStream;public class Server {    public void setUpServer(int port) {        try {            java.net.ServerSocket server = new java.net.ServerSocket(port);            System.out.println(&quot;服务器创建完成&quot;);            while (true) {                java.net.Socket client = server.accept();                System.out.println(&quot;客户端已接入&quot;);                process(client);            }        } catch (Exception e) {            e.printStackTrace();        }    }    private void process(java.net.Socket client) {        try {            OutputStream ous = client.getOutputStream();            java.io.InputStream ins = client.getInputStream();            //将输入流包装成DataInputStream以便读取数据            DataInputStream dins = new DataInputStream(ins);            while(true){                //读取消息，消息以int开头                //1、读消息长度                int totalLen = dins.readInt();                System.out.println(&quot;发来消息长度为：&quot;+totalLen);                //2、读消息类型标识，读一个字节                byte flag = dins.readByte();                System.out.println(&quot;接收消息类型为：&quot;+flag);                //3、读取目标客户号码，一个 int                int destNum = dins.readInt();                System.out.println(&quot;目标用户号码为：&quot;+destNum);                //根据消息内容读取消息体                if(flag==1){//文本消息                    byte[] data = new byte[totalLen-4-1-4];                    //从流中读取data.length个字节放入数组                    dins.readFully(data);                    String msg = new String(data);                    System.out.println(&quot;发送文本给：&quot;+destNum+&quot;内容是：&quot;+msg);                }                else if(flag==2){//文件数据                    //256个字节作为文件名                    System.out.println(&quot;发送文件给：&quot;+destNum);                    byte[] data = new byte[256];                    dins.readFully(data);                    String fileName = new String (data).trim();                    System.out.println(&quot;读到文件名字是：&quot;+fileName);                    //读文件内容                    data = new byte[totalLen-4-1-4-256];                    dins.readFully(data);                    //保存在当前目录下                    FileOutputStream fous = new FileOutputStream(fileName+1);                    fous.write(data);                    fous.flush();                    fous.close();                }            }        } catch (Exception e) {            e.printStackTrace();        }    }    public static void main(String[] args) {        Server server = new Server();        server.setUpServer(9900);    }}</code></pre><h3 id="client"><a href="#client" class="headerlink" title="client"></a>client</h3><pre><code>package qq_client_test;import java.io.DataOutputStream;import java.io.File;import java.io.FileInputStream;import java.io.OutputStream;import java.net.Socket;import java.util.Scanner;import org.omg.CORBA.portable.InputStream;public class Client {    private DataOutputStream dous;//输出流对象    private void writeString(DataOutputStream out,String str,int len){    //向流中写入指定长度的字节，如果不足，就补0        try{            byte[] data = str.getBytes();            out.write(data);            while(len&gt;data.length){                out.writeByte(&apos;\0&apos;);                len--;            }        }catch(Exception e){            e.printStackTrace();        }    }    private void sendTextMsg(String msg,int destNum){        //发送消息        try{            byte[] strb = msg.getBytes();//得到消息的字节数            int totalLen = 4+4+1+strb.length;            System.out.println(&quot;发送总长为：&quot;+totalLen);            dous.writeInt(totalLen);            dous.writeByte(1);//类型1代表文本            dous.writeInt(destNum);//写入目标用户号            dous.write(strb);            dous.flush();                    }catch(Exception e){            e.printStackTrace();        }    }    private void sendFileMsg(String fileName, int destNum){        try{            //根据文件名创建文件对象            File file = new File(fileName);            //根据文件对象构造一个输入流            FileInputStream ins = new FileInputStream(file);            int fileDataLen = ins.available(); //文件数据总长            int totalLen = 4+1+4+256+fileDataLen; //得到了要发送数据包的总长            dous.writeInt(totalLen);            dous.writeByte(2);            dous.writeInt(destNum);            String shortFileName = file.getName();            //写入文件名            writeString(dous,shortFileName,256);            byte[] fileData = new byte[fileDataLen];            ins.read(fileData);//读入文件数据            dous.write(fileData);//写到服务器的流中            dous.flush();        }catch(Exception e){            e.printStackTrace();        }    }    public void conn2Server(String ip, int port){        try{            java.net.Socket client = new java.net.Socket(ip,port);            java.io.InputStream ins = client.getInputStream();            OutputStream ous = client.getOutputStream();            dous = new DataOutputStream(ous);            int testCount = 0;            while(true){                System.out.println(&quot;登陆服务器成功，选择要发的类型（1、聊天，2、文件）：&quot;);                java.util.Scanner sc = new Scanner(System.in);                int type = sc.nextInt();                if(type==1){                    sendTextMsg(&quot;abc聊天&quot;+testCount, 8888);                }                if(type==2){                    sendFileMsg(&quot;C:\\Users\\董润泽\\Desktop\\test1.jpg&quot;, 8888);                }                testCount++;            }        }catch(Exception e){            e.printStackTrace();        }    }    public static void main(String[] args) {        Client client = new Client();        client.conn2Server(&quot;localhost&quot;, 9900);    }}</code></pre><p>运行结果：</p><p><img src="https://img-blog.csdnimg.cn/20200129204116198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""><br><img src="https://img-blog.csdnimg.cn/20200129204122572.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
          <category> 通信 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java通信 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>堆内存结构及简单性能调优</title>
      <link href="/2020/01/28/JVM/%E5%A0%86%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84%E5%8F%8A%E7%AE%80%E5%8D%95%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/"/>
      <url>/2020/01/28/JVM/%E5%A0%86%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84%E5%8F%8A%E7%AE%80%E5%8D%95%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<p>Java底层最重要的一部分就是jvm堆内存，它影响着Java的性能。</p><p>这篇博客主要介绍Java堆内存的分区及简单的Java调优。</p><h2 id="一、Java堆内存"><a href="#一、Java堆内存" class="headerlink" title="一、Java堆内存"></a>一、Java堆内存</h2><p>首先看这张图：</p><p><img src="https://img-blog.csdnimg.cn/20191121235622158.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ul><li>堆中的分区</li><li>Java堆内存分为两部分：<strong>年轻代、老年代</strong></li></ul><p>其中，年轻代分两个部分：<strong>Eden、Survivor</strong></p><ul><li>内存分配</li></ul><pre><code>老年代的内存占堆内总内存的2/3年轻代占1/3，在年轻代中，Eden分8/10，From和To都是1/10例：堆中总共300M空间，那么老年代有200M，年轻代有100M</code></pre><ul><li>运行时到底是什么个情况呢？</li></ul><p>当我们new一个对象之后，首先，这个对象就会被放入Eden区，直到Eden区内存被占满</p><ul><li>Eden满了之后：</li><li>Eden满了后，就会触发垃圾回收机制gc，但是呢，在这个地方是minor gc。minor gc会回收Eden区的垃圾对象（没有任何指针引用程序的对象）</li></ul><p><img src="https://img-blog.csdnimg.cn/201911220014486.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="JVM虚拟机内部构造图"></p><ul><li>垃圾对象？</li></ul><p>可以理解为：当main函数结束时，栈帧区域会被销毁掉，然后局部变量也就被释放掉，那么指向堆中对象的指针也就被干掉了，最后，堆中的对象就是个垃圾对象。<strong>这里我们要注意一个点：查找垃圾对象的起始是从栈帧中开始的，然后查到的时一个局部变量</strong></p><ul><li>可达性分析算法？GC Root？</li></ul><p>在垃圾对象里面说了，堆中的对象是由局部变量指出的，那么我们就可以利用它来判断是不是垃圾对象，这里的局部变量就可以理解为GC Root。可达性分析算法就是以GC Root为起点，往下搜索，找到的对象就不是垃圾对象，找不到的就是垃圾对象。</p><ul><li>接着minor gc</li></ul><p>上面已经说了，minor gc只是针对Eden区的。然后，当进行一次minor gc后，Eden中的垃圾对象全部被干掉，剩下的非垃圾对象，要进入Survivor区中的From中，这时候，这些非垃圾对象的分代年龄就会加一。</p><ul><li>分代年龄？对象头？</li></ul><p>分代年龄存放在对象的对象头中，每经历一轮的垃圾回收，分代年龄就会加一<br>对象头：（<a href="https://blog.csdn.net/lkforce/article/details/81128115#1%EF%BC%8CMark%20Word" target="_blank" rel="noopener">https://blog.csdn.net/lkforce/article/details/81128115#1%EF%BC%8CMark%20Word</a>）</p><pre><code>1. Mark Word2. 指向类的指针3. 数组长度（只有数组对象才有）</code></pre><ul><li>然后Eden区再次满的时候，又会触发垃圾回收，再次将垃圾对象（包括刚刚放入from中变成新垃圾的对象）干掉，把Eden和From中非垃圾对象放入To中，并将年龄加一。</li></ul><p>当下次Eden再满的时候，又会把Eden和To中的非垃圾对象放入From中，年龄加一，以此不断循环……</p><ul><li>所以年龄有什么卵用？</li></ul><p>当进行的次数多了，直到年龄达到了15（可以改这个参数），这时候就会被移到老年代</p><ul><li>上面已经说过，老年代也有一定的大小，那么如果老年代满的时候怎么办呢？<br>这时候就会发生full gc</li></ul><p>下面我们以代码为例，查看运行时的各区情况：</p><p>利用上篇微信红包中的main函数，使它进入死循环</p><p>（对微信红包算法感兴趣的可以看：<a href="https://blog.csdn.net/qq_44357371/article/details/103115263" target="_blank" rel="noopener">https://blog.csdn.net/qq_44357371/article/details/103115263</a>）</p><pre><code>public static void main(String[] args) {        while (true) {            test.thirdMethod(5, 20);        }    }</code></pre><p><img src="https://img-blog.csdnimg.cn/20191122011034389.gif" alt=""></p><p>old就是代表老年区，我们可以看到Eden区是最快的，然后就发生垃圾回收……，和上面所述相符。</p><p><img src="https://img-blog.csdnimg.cn/20191122011342596.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>说实话，我想看old区被塞满的情况，可惜它增长的太慢了。。。</p><ul><li>接着full gc。STW？</li></ul><p>当发生full gc时，就会产生STW（stop world，，毁掉整个世界），（哈哈哈，没这么牛逼）。这时候呢，就会暂停所有的线程，让垃圾回收机制专心的回收垃圾。这样的话，用户端会卡掉。</p><ul><li>为什么要把线程停掉呢？</li></ul><p>考虑一种情况，如果在找垃圾时，刚好已经在某个链条上面了，这时候，如果线程把这条链给干掉了，那么后面的本来应该全部是垃圾的，但是gc没有把它们给找出来，所以停掉线程。</p><h2 id="二、性能调优"><a href="#二、性能调优" class="headerlink" title="二、性能调优"></a>二、性能调优</h2><ul><li>Jvm性能调优到底调的是什么？</li></ul><p>我们上面已经知道，minor gc发生时，对性能的影响不大，但是full gc发生时，对性能的影响是巨大的。所以调优就是要减少full gc发生的次数，减少STW出现次数；还有就是在发生了Full gc时，所有的线程停掉等待垃圾回收，所以，减少垃圾回收时间。</p><p>下面举一个调优例子：</p><p><img src="https://img-blog.csdnimg.cn/20191122013401603.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>首先分析：</p><p>我们设置了堆的大小为3G，老年区就有2G，eden有800M，from和to各占100M</p><p>线程在运行时，每秒产生60M的对象，用后直接干掉变垃圾，这样大概每13秒就会把Eden占满，触发minor gc。</p><p>但是一个问题，在第13s产生的进程，会被直接移到survivor区，但是它的大小超过了survivor中一个区的一半，这时候触发==对象动态年龄判断机制==，直接进入老年代。</p><p>但是着60M的对象，在下一秒就又变成了垃圾。。。这样算下来，大概5、6分钟就会使老年代触发full gc，这样的效率是极其低下的。</p><p>所以呢，我们可以对这个系统进行一个调优：</p><p>把这些参数改一下，我们可以尽量让垃圾在年轻代就被干掉。</p><p><strong>总共3G，把old区设置为1G（因为没那么多要放的），这样，新生代就有了2G，这样，Eden分了1.6G，from和to分别200M</strong></p><p>这时候，每秒60M的对象过来，最后那一秒在Eden区中传入survivor中，但是根据==对象动态年龄判断机制==，60M小于from或to的一半，不会被传进老年代，变成垃圾，直接被minor gc干死，所以基本上，老年代就不可能被放满，所以就极大的改善了性能！！！</p><p>今晚收获巨大。jvm内容还多，路途且长，继续奋斗！</p><p>图片来源：诸葛老师</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> JVM结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM底层结构</title>
      <link href="/2020/01/28/JVM/JVM%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/"/>
      <url>/2020/01/28/JVM/JVM%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>Java的优点：一次编写，处处执行，即跨平台。<br>Java如何做到跨平台呢？<br><img src="https://img-blog.csdnimg.cn/20191127121557147.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70#pic_center=400x500" alt=""></p><p>首先看这张图片，我们写的Java代码，通过Javac编译成字节码文件，然后通过Java命令进入jvm。但是在不同的平台上机器码不一样，所以jvm一个宏观上的理解就是：从软件层面屏蔽不同操作系统在底层硬件与指令上的区别。</p><h2 id="JVM虚拟机结构图"><a href="#JVM虚拟机结构图" class="headerlink" title="JVM虚拟机结构图"></a>JVM虚拟机结构图</h2><p><img src="https://img-blog.csdnimg.cn/20191127123045440.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="JVM各组成部分："><a href="#JVM各组成部分：" class="headerlink" title="JVM各组成部分："></a>JVM各组成部分：</h2><ul><li>运行时数据区（内存模型）</li><li>类转载子系统</li><li>字节码执行引擎</li></ul><h2 id="运行时数据区（内存模型"><a href="#运行时数据区（内存模型" class="headerlink" title="运行时数据区（内存模型):"></a>运行时数据区（内存模型):</h2><ul><li>堆</li><li>栈（线程栈）</li><li>本地方法栈</li><li>方法区（元空间）</li><li>程序计数器</li></ul><h4 id="栈（线程栈）"><a href="#栈（线程栈）" class="headerlink" title="栈（线程栈）"></a>栈（线程栈）</h4><p>程序在运行时会有很多个线程，每产生一个新的线程，Java的线程栈就会给线程分配一段栈内存区。<br>栈帧：Java中方法在运行时，栈会给每一个方法分配一段栈帧内存区，里面放各自方法的局部变量。栈帧内存区存放在栈中。</p><p>当方法执行完，栈帧中相应的内存区就被干掉</p><p>栈的结构：FILO，在JVM中亦然，先调用的方法，分配栈帧内存区之后压栈，后调用的方法，先被干掉。</p><p>以代码为例：</p><p><img src="https://img-blog.csdnimg.cn/20191127130535518.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>在代码运行时，首先线程栈会给它分配一个栈内存区，然后执行main方法，这个栈内存区给main方法分配一个栈帧内存区，并压入栈底。main方法中调用Computer时，给computer方法分配一个栈帧内存区，当Computer方法执行完成之后，将为该方法分配的栈帧从栈中干掉。最后，main方法执行完成之后，将main方法对应的栈帧干掉。（FILO）</p><p><strong>栈帧内部</strong></p><ul><li>局部变量表</li><li>操作数栈</li><li>动态链接</li><li>方法出口</li></ul><h4 id="方法区（元空间"><a href="#方法区（元空间" class="headerlink" title="方法区（元空间)"></a>方法区（元空间)</h4><ul><li>常量</li><li>静态变量(new出来的对象放在堆里面)</li><li>类元信息</li></ul><p>堆和方法区：</p><p>堆中的对象的头会存放类的信息指针，指向方法区</p><h4 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h4><p>记录程序执行的位置。行数。</p><h4 id="本地方法"><a href="#本地方法" class="headerlink" title="本地方法"></a>本地方法</h4><p>每个线程独有，底层C语言与Java交互调用</p><p><em>可能有些地方说的不清楚，若有疑问，下面留言。</em> </p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> JVM结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM结构 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>红酒质量预测</title>
      <link href="/2020/01/28/%E7%BA%A2%E9%85%92%E8%B4%A8%E9%87%8F%E9%A2%84%E6%B5%8B/%E7%BA%A2%E9%85%92%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
      <url>/2020/01/28/%E7%BA%A2%E9%85%92%E8%B4%A8%E9%87%8F%E9%A2%84%E6%B5%8B/%E7%BA%A2%E9%85%92%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix,classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wine = pd.read_csv(<span class="string">"C:\\Users\\董润泽\\Desktop\\winequality-red.csv"</span>,sep=<span class="string">";"</span>)</span><br><span class="line">wine.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>fixed acidity</th>      <th>volatile acidity</th>      <th>citric acid</th>      <th>residual sugar</th>      <th>chlorides</th>      <th>free sulfur dioxide</th>      <th>total sulfur dioxide</th>      <th>density</th>      <th>pH</th>      <th>sulphates</th>      <th>alcohol</th>      <th>quality</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>7.4</td>      <td>0.70</td>      <td>0.00</td>      <td>1.9</td>      <td>0.076</td>      <td>11.0</td>      <td>34.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>5</td>    </tr>    <tr>      <th>1</th>      <td>7.8</td>      <td>0.88</td>      <td>0.00</td>      <td>2.6</td>      <td>0.098</td>      <td>25.0</td>      <td>67.0</td>      <td>0.9968</td>      <td>3.20</td>      <td>0.68</td>      <td>9.8</td>      <td>5</td>    </tr>    <tr>      <th>2</th>      <td>7.8</td>      <td>0.76</td>      <td>0.04</td>      <td>2.3</td>      <td>0.092</td>      <td>15.0</td>      <td>54.0</td>      <td>0.9970</td>      <td>3.26</td>      <td>0.65</td>      <td>9.8</td>      <td>5</td>    </tr>    <tr>      <th>3</th>      <td>11.2</td>      <td>0.28</td>      <td>0.56</td>      <td>1.9</td>      <td>0.075</td>      <td>17.0</td>      <td>60.0</td>      <td>0.9980</td>      <td>3.16</td>      <td>0.58</td>      <td>9.8</td>      <td>6</td>    </tr>    <tr>      <th>4</th>      <td>7.4</td>      <td>0.70</td>      <td>0.00</td>      <td>1.9</td>      <td>0.076</td>      <td>11.0</td>      <td>34.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>5</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 1599 entries, 0 to 1598Data columns (total 12 columns):fixed acidity           1599 non-null float64volatile acidity        1599 non-null float64citric acid             1599 non-null float64residual sugar          1599 non-null float64chlorides               1599 non-null float64free sulfur dioxide     1599 non-null float64total sulfur dioxide    1599 non-null float64density                 1599 non-null float64pH                      1599 non-null float64sulphates               1599 non-null float64alcohol                 1599 non-null float64quality                 1599 non-null int64dtypes: float64(11), int64(1)memory usage: 150.0 KB</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine.isnull().sum()</span><br></pre></td></tr></table></figure><pre><code>fixed acidity           0volatile acidity        0citric acid             0residual sugar          0chlorides               0free sulfur dioxide     0total sulfur dioxide    0density                 0pH                      0sulphates               0alcohol                 0quality                 0dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bins = (<span class="number">2</span>, <span class="number">6.5</span>, <span class="number">8</span>)</span><br><span class="line">group_names=[<span class="string">'bad'</span>, <span class="string">'good'</span>]</span><br><span class="line">wine[<span class="string">'quality'</span>] = pd.cut(wine[<span class="string">'quality'</span>], bins = bins, labels = group_names)</span><br><span class="line">wine[<span class="string">'quality'</span>].unique()</span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)&lt;ipython-input-13-99255379e85c&gt; in &lt;module&gt;      1 bins = (2, 6.5, 8)      2 group_names=[&apos;bad&apos;, &apos;good&apos;]----&gt; 3 wine[&apos;quality&apos;] = pd.cut(wine[&apos;quality&apos;], bins = bins, labels = group_names)      4 wine[&apos;quality&apos;].unique()C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates)    239                               include_lowest=include_lowest,    240                               dtype=dtype,--&gt; 241                               duplicates=duplicates)    242     243     return _postprocess_for_cut(fac, bins, retbins, x_is_series,C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates)    342     343     side = &apos;left&apos; if right else &apos;right&apos;--&gt; 344     ids = ensure_int64(bins.searchsorted(x, side=side))    345     346     if include_lowest:TypeError: &apos;&lt;&apos; not supported between instances of &apos;float&apos; and &apos;str&apos;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>fixed acidity</th>      <th>volatile acidity</th>      <th>citric acid</th>      <th>residual sugar</th>      <th>chlorides</th>      <th>free sulfur dioxide</th>      <th>total sulfur dioxide</th>      <th>density</th>      <th>pH</th>      <th>sulphates</th>      <th>alcohol</th>      <th>quality</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>7.4</td>      <td>0.70</td>      <td>0.00</td>      <td>1.9</td>      <td>0.076</td>      <td>11.0</td>      <td>34.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>bad</td>    </tr>    <tr>      <th>1</th>      <td>7.8</td>      <td>0.88</td>      <td>0.00</td>      <td>2.6</td>      <td>0.098</td>      <td>25.0</td>      <td>67.0</td>      <td>0.9968</td>      <td>3.20</td>      <td>0.68</td>      <td>9.8</td>      <td>bad</td>    </tr>    <tr>      <th>2</th>      <td>7.8</td>      <td>0.76</td>      <td>0.04</td>      <td>2.3</td>      <td>0.092</td>      <td>15.0</td>      <td>54.0</td>      <td>0.9970</td>      <td>3.26</td>      <td>0.65</td>      <td>9.8</td>      <td>bad</td>    </tr>    <tr>      <th>3</th>      <td>11.2</td>      <td>0.28</td>      <td>0.56</td>      <td>1.9</td>      <td>0.075</td>      <td>17.0</td>      <td>60.0</td>      <td>0.9980</td>      <td>3.16</td>      <td>0.58</td>      <td>9.8</td>      <td>bad</td>    </tr>    <tr>      <th>4</th>      <td>7.4</td>      <td>0.70</td>      <td>0.00</td>      <td>1.9</td>      <td>0.076</td>      <td>11.0</td>      <td>34.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>bad</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">label_quality = LabelEncoder()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine[<span class="string">'quality'</span>] = label_quality.fit_transform(wine[<span class="string">'quality'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine.head(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>fixed acidity</th>      <th>volatile acidity</th>      <th>citric acid</th>      <th>residual sugar</th>      <th>chlorides</th>      <th>free sulfur dioxide</th>      <th>total sulfur dioxide</th>      <th>density</th>      <th>pH</th>      <th>sulphates</th>      <th>alcohol</th>      <th>quality</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>7.4</td>      <td>0.70</td>      <td>0.00</td>      <td>1.9</td>      <td>0.076</td>      <td>11.0</td>      <td>34.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>7.8</td>      <td>0.88</td>      <td>0.00</td>      <td>2.6</td>      <td>0.098</td>      <td>25.0</td>      <td>67.0</td>      <td>0.9968</td>      <td>3.20</td>      <td>0.68</td>      <td>9.8</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>7.8</td>      <td>0.76</td>      <td>0.04</td>      <td>2.3</td>      <td>0.092</td>      <td>15.0</td>      <td>54.0</td>      <td>0.9970</td>      <td>3.26</td>      <td>0.65</td>      <td>9.8</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>11.2</td>      <td>0.28</td>      <td>0.56</td>      <td>1.9</td>      <td>0.075</td>      <td>17.0</td>      <td>60.0</td>      <td>0.9980</td>      <td>3.16</td>      <td>0.58</td>      <td>9.8</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>7.4</td>      <td>0.70</td>      <td>0.00</td>      <td>1.9</td>      <td>0.076</td>      <td>11.0</td>      <td>34.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>0</td>    </tr>    <tr>      <th>5</th>      <td>7.4</td>      <td>0.66</td>      <td>0.00</td>      <td>1.8</td>      <td>0.075</td>      <td>13.0</td>      <td>40.0</td>      <td>0.9978</td>      <td>3.51</td>      <td>0.56</td>      <td>9.4</td>      <td>0</td>    </tr>    <tr>      <th>6</th>      <td>7.9</td>      <td>0.60</td>      <td>0.06</td>      <td>1.6</td>      <td>0.069</td>      <td>15.0</td>      <td>59.0</td>      <td>0.9964</td>      <td>3.30</td>      <td>0.46</td>      <td>9.4</td>      <td>0</td>    </tr>    <tr>      <th>7</th>      <td>7.3</td>      <td>0.65</td>      <td>0.00</td>      <td>1.2</td>      <td>0.065</td>      <td>15.0</td>      <td>21.0</td>      <td>0.9946</td>      <td>3.39</td>      <td>0.47</td>      <td>10.0</td>      <td>1</td>    </tr>    <tr>      <th>8</th>      <td>7.8</td>      <td>0.58</td>      <td>0.02</td>      <td>2.0</td>      <td>0.073</td>      <td>9.0</td>      <td>18.0</td>      <td>0.9968</td>      <td>3.36</td>      <td>0.57</td>      <td>9.5</td>      <td>1</td>    </tr>    <tr>      <th>9</th>      <td>7.5</td>      <td>0.50</td>      <td>0.36</td>      <td>6.1</td>      <td>0.071</td>      <td>17.0</td>      <td>102.0</td>      <td>0.9978</td>      <td>3.35</td>      <td>0.80</td>      <td>10.5</td>      <td>0</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine[<span class="string">'quality'</span>].value_counts()</span><br></pre></td></tr></table></figure><pre><code>0    13821     217Name: quality, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.countplot(wine[<span class="string">'quality'</span>])</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x2076c534cc0&gt;</code></pre><p><img src="output_10_1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#seperate the dataset as response variable and feature variable</span></span><br><span class="line">X = wine.drop(<span class="string">'quality'</span> , axis=<span class="number">1</span>)</span><br><span class="line">y = wine[<span class="string">'quality'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Train and test splitting of data</span></span><br><span class="line">X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state = <span class="number">42</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sc = StandardScaler()</span><br><span class="line">X_train = sc.fit_transform(X_train)</span><br><span class="line">X_test = sc.transform(X_test)</span><br></pre></td></tr></table></figure><h1 id="Random-Forest-Classifier"><a href="#Random-Forest-Classifier" class="headerlink" title="Random Forest Classifier"></a>Random Forest Classifier</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rfc = RandomForestClassifier(n_estimators=<span class="number">600</span>)</span><br><span class="line">rfc.fit(X_train,y_train)</span><br><span class="line">pred_rfc = rfc.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(classification_report(y_test, pred_rfc))</span><br><span class="line">print(confusion_matrix(y_test,pred_rfc))</span><br><span class="line"><span class="comment">#对于坏酒，264对，9错</span></span><br><span class="line"><span class="comment">#好酒，13对，24错</span></span><br></pre></td></tr></table></figure><pre><code>              precision    recall  f1-score   support           0       0.92      0.97      0.94       273           1       0.73      0.51      0.60        47    accuracy                           0.90       320   macro avg       0.82      0.74      0.77       320weighted avg       0.89      0.90      0.89       320[[264   9] [ 23  24]]</code></pre><h1 id="SVM-classifier"><a href="#SVM-classifier" class="headerlink" title="SVM classifier"></a>SVM classifier</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clf=svm.SVC()</span><br><span class="line">clf.fit(X_train, y_train)</span><br><span class="line">pred_clf = clf.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(classification_report(y_test, pred_clf))</span><br><span class="line">print(confusion_matrix(y_test,pred_clf))</span><br></pre></td></tr></table></figure><pre><code>              precision    recall  f1-score   support           0       0.88      0.98      0.93       273           1       0.71      0.26      0.37        47    accuracy                           0.88       320   macro avg       0.80      0.62      0.65       320weighted avg       0.86      0.88      0.85       320[[268   5] [ 35  12]]</code></pre><h1 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mlpc = MLPClassifier(hidden_layer_sizes=(<span class="number">11</span>,<span class="number">11</span>,<span class="number">11</span>),max_iter=<span class="number">500</span>)</span><br><span class="line">mlpc.fit(X_train,y_train)</span><br><span class="line">pred_mlpc = mlpc.predict(X_test)</span><br></pre></td></tr></table></figure><pre><code>C:\ProgramData\Anaconda3\lib\site-packages\sklearn\neural_network\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn&apos;t converged yet.  % self.max_iter, ConvergenceWarning)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(classification_report(y_test, pred_clf))</span><br><span class="line">print(confusion_matrix(y_test,pred_clf))</span><br></pre></td></tr></table></figure><pre><code>              precision    recall  f1-score   support           0       0.88      0.98      0.93       273           1       0.71      0.26      0.37        47    accuracy                           0.88       320   macro avg       0.80      0.62      0.65       320weighted avg       0.86      0.88      0.85       320[[268   5] [ 35  12]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">print(accuracy_score(y_test,pred_rfc))</span><br><span class="line">print(accuracy_score(y_test,pred_mlpc))</span><br><span class="line">print(accuracy_score(y_test,pred_clf))</span><br></pre></td></tr></table></figure><pre><code>0.8750.8843750.875</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
          <category> 入门 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>方法调用（解析、动态分派、静态分派）</title>
      <link href="/2020/01/28/JVM/JVM%E5%AD%97%E8%8A%82%E7%A0%81%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8%EF%BC%88%E8%A7%A3%E6%9E%90%E3%80%81%E5%8A%A8%E6%80%81%E5%88%86%E6%B4%BE%E3%80%81%E9%9D%99%E6%80%81%E5%88%86%E6%B4%BE%EF%BC%89/"/>
      <url>/2020/01/28/JVM/JVM%E5%AD%97%E8%8A%82%E7%A0%81%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E6%96%B9%E6%B3%95%E8%B0%83%E7%94%A8%EF%BC%88%E8%A7%A3%E6%9E%90%E3%80%81%E5%8A%A8%E6%80%81%E5%88%86%E6%B4%BE%E3%80%81%E9%9D%99%E6%80%81%E5%88%86%E6%B4%BE%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>参考文章：</p><p><a href="https://blog.csdn.net/wangdongli_1993/article/details/81428848" target="_blank" rel="noopener">JVM（十四）方法调用</a></p><p><a href="https://blog.csdn.net/fan2012huan/article/details/51004615" target="_blank" rel="noopener" title="java方法调用之单分派与多分派（二）">java方法调用之单分派与多分派（二）</a></p><p>方法调用阶段就是确定被调用方法的版本，即调用哪一个方法。</p><h1 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h1><p>我们已经知道，class文件中需要调用的方法都是一个符号引用，而在方法调用中的解析阶段，就是要把一部分符号引用转化为直接引用。</p><p>能在解析阶段将方法的符号引用转化成直接引用的的方法，必须在方法运行前就确定一个可调用的版本，并且这个版本在运行阶段是不可改变的。</p><p>“编译期可知，运行期不可变”，符合这个规则的方法有静态方法和私有方法两大类。前者与所属的类直接关联，后者在外部不可以被访问。这两种方法都适合在解析调用，也就是把这些方法的符号引用转化成直接引用。</p><p>与之对应5条调用方法的字节码指令：</p><ul><li><strong>invokestatic:调用静态方法</strong></li><li><strong>invokespecial：调用实例构造器<init>方法，私有方法和父类方法</strong></li><li><strong>invokevirtual：调用所有虚方法</strong></li><li><strong>invokeinterface：调用接口方法，运行时确定一个实现此接口的对象</strong></li><li><strong>invokedynamic:先在运行时动态解析出调用点限定符所引用的的方法，然后再执行此方法</strong></li></ul><p>只有用invokestatic和invokespecial指令调用的方法（还有final修饰的方法），都可以在解析阶段确定调用版本，这些方法叫做非虚方法。</p><p>剩下三个字节码指令调用的方法叫做虚方法（invokevirtual指令调用的final修饰的方法除外）</p><p>解析调用是一个静态过程，编译期间就可以确定，解析阶段将符号引用转化为直接引用。</p><p><img src="https://img-blog.csdnimg.cn/20200127104051600.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>将代码转换为字节码文件：</p><p><img src="https://img-blog.csdnimg.cn/20200127104142282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>可以看到，确实是通过invokestatic命令调用sayhello方法。</p><h1 id="分派"><a href="#分派" class="headerlink" title="分派"></a>分派</h1><h2 id="1-静态分派—（重载）"><a href="#1-静态分派—（重载）" class="headerlink" title="1. 静态分派—（重载）"></a>1. 静态分派—（重载）</h2><p>先看一段简单的代码：</p><p><img src="https://img-blog.csdnimg.cn/20200127105004799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>输出结果是什么？</p><p>我们先来分析一下：</p><p>首先引入两个概念：我们把代码中的Human成为变量的<strong>静态类型</strong>，把Man、Woman称为变量的<strong>实际类型</strong>。<br>静态类型和动态类型在程序中都可以发生一些变化，区别是静态类型的变化仅仅在使用时发生，变量本身的静态类型不会改变，并且最终的静态类型是在编译期可知的；而实际类型变化的结果在运行期才可确定，<strong>编译器在编译程序的时候不知道一个对象的实际类型是什么</strong>。</p><p>现在再看代码，对于sd.sayHello(man);直观上看，它似乎传入的是Man类型的参数man，所以应该打印的是man的方法“man is saying hello”，</p><p>但是要注意，man 的静态类型仍然是Human，实际类型才是man，所以，在编译阶段，Javac选择了sayHello(Human)作为调用目标。</p><p>即最终输出：</p><p><img src="https://img-blog.csdnimg.cn/20200127110639837.png" alt=""></p><p>通过这个实例，我们可以看到，这里是通过静态类型来定位执行方法的版本，这样的分派动作称为<strong>静态分派</strong><br>静态分派于重载有很深的关系</p><h2 id="2-动态分派—（重写）"><a href="#2-动态分派—（重写）" class="headerlink" title="2. 动态分派—（重写）"></a>2. 动态分派—（重写）</h2><p>先看代码：</p><p><img src="https://img-blog.csdnimg.cn/20200127112750917.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>生成字节码文件：</p><p><img src="https://img-blog.csdnimg.cn/20200127113823727.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>注意代码中的：</p><p><img src="https://img-blog.csdnimg.cn/20200127114045571.png" alt=""></p><p>对应字节码中的：</p><p><img src="https://img-blog.csdnimg.cn/20200127114132633.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>invokevirtual指令的运行时解析过程大致分为：</p><p>（1）找到操作数栈栈顶的第一个元素所指向的对象的实际类型。记作C。</p><p>（2）如果在类型C中找到与常量描述符和简单名称相符的方法，就进行访问权限校验，通过则返回方法的直接引用，没有通过则抛出java.lang.IllegalAccessError异常</p><p>（3）如果在类型中没有找到对应的方法，则按照继承关系从下往上对C的父类依此查找方法</p><p>（4）若始终没有找到合适方法，抛出java.lang.AbstractMethodError异常。</p><p>invokevirtual指令执行的时候先确定方法调用的对象的实际类型，所以会把两次方法调用的符号引用解析到不同的直接引用上，这个过程叫做<strong>动态分派</strong>，是方法重写的本质。</p><p>代码结果为：</p><p><img src="https://img-blog.csdnimg.cn/20200127124154544.png" alt=""></p><h2 id="单分派与多分派"><a href="#单分派与多分派" class="headerlink" title="单分派与多分派"></a>单分派与多分派</h2><p>先看一段代码：</p><pre><code>public class Dispatcher {    static class QQ {}    static class _360 {}    public static class Father {        public void hardChoice(QQ arg) {            System.out.println(&quot;father choose QQ&quot;);        }        public void hardChoice(_360 arg) {            System.out.println(&quot;father choose _360&quot;);        }    }    public static class Son extends Father {        @Override        public void hardChoice(QQ arg) {            System.out.println(&quot;son choose QQ&quot;);        }        public void hardChoice(_360 arg) {            System.out.println(&quot;son choose 360&quot;);        }    }    public static void main(String[] args) {        Father father = new Father();        Father son = new Son();        father.hardChoice(new _360());        son.hardChoice(new QQ());    }}</code></pre><p>运行结果是什么？</p><p><img src="https://img-blog.csdnimg.cn/20200129100434458.png" alt=""></p><p>它的字节码文件：</p><p><img src="https://img-blog.csdnimg.cn/20200129103435680.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>我们分别从<strong>静态分派</strong>和<strong>动态分派</strong>的角度来分析</p><h3 id="1-静态分派"><a href="#1-静态分派" class="headerlink" title="1. 静态分派"></a>1. 静态分派</h3><p>看字节码的</p><pre><code>24: invokevirtual #8                  // Method 单分派多分派/Dispatcher$Father.hardChoice:(L单分派多分派/Dispatcher$_360;)V35: invokevirtual #11                 // Method 单分派多分派/Dispatcher$Father.hardChoice:(L单分派多分派/Dispatcher$QQ;)V</code></pre><p>对应的是代码的：</p><pre><code>father.hardChoice(new _360());son.hardChoice(new QQ());</code></pre><p>可以看到，invokevirtual相同，调用的都是<strong>$Father.hardChoice</strong>方法，只是他们的参数不同。</p><p>这说明他们的静态类型相同，都是Father。在选择目标方法时，根据两个<strong>宗量</strong>，是多分派的，即<strong>静态分派属于多分派类型</strong>。</p><blockquote><p>宗量：方法的接收者和方法的参数统称为方法的宗量。</p></blockquote><blockquote><p>单分派：根据一个宗量对目标方法进行选择</p></blockquote><blockquote><p>多分派：多于一个宗量对目标方法进行选择</p></blockquote><h3 id="2-动态分派"><a href="#2-动态分派" class="headerlink" title="2. 动态分派"></a>2. 动态分派</h3><p>当在执行</p><pre><code>father.hardChoice(new _360());son.hardChoice(new QQ());</code></pre><p>发现son的实际类型是Son，所以转去调用Son的方法。在father中也执行了此过程，只不过，father的实际类型仍然是father。</p><p>目标选择时只依据了一个宗量，是单分派的。因此，<strong>动态分派属于单分派类型</strong>。</p><h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><p>静态分派关注了两个宗量，即<strong>静态类型和参数</strong>，（参数对比重载）</p><p>而动态分派关注<strong>实际类型</strong>，（对比重写）</p><p><strong>java语言是一个静态多分派，动态单分派的语言</strong></p><h2 id="动态分派的实现"><a href="#动态分派的实现" class="headerlink" title="动态分派的实现"></a>动态分派的实现</h2><p>动态分派类似重写，动态分派是非常频繁的动作，运行时需要在元数据中搜索合适的目标方法，最常用的“稳定优化”手段就是建立一个虚方法表，存放各个方法实际入口地址。</p><p>子类重写了方法，就会指向子类的方法地址。</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> 字节码执行引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 字节码执行引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解基于栈的字节码执行引擎及JVM底层结构</title>
      <link href="/2020/01/28/JVM/%E9%80%9A%E8%BF%87Java%E5%AD%97%E8%8A%82%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E5%8F%8AJVM%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/"/>
      <url>/2020/01/28/JVM/%E9%80%9A%E8%BF%87Java%E5%AD%97%E8%8A%82%E7%A0%81%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E5%8F%8AJVM%E5%BA%95%E5%B1%82%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>在读本文时，可以参考我的另外两篇介绍jvm的博客。</p><p><a href="https://blog.csdn.net/qq_44357371/article/details/103330641" target="_blank" rel="noopener">JVM底层结构</a></p><p><a href="https://blog.csdn.net/qq_44357371/article/details/103192906" target="_blank" rel="noopener">Java堆内存介绍及简单性能调优</a></p><h2 id="生成一个字节码文件"><a href="#生成一个字节码文件" class="headerlink" title="生成一个字节码文件"></a>生成一个字节码文件</h2><p>首先我们编写一个简单的Java文件</p><p><img src="https://img-blog.csdnimg.cn/20200121101732904.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>在文件夹中找到这个文件，可以看到只有一个.java 文件</p><p><img src="https://img-blog.csdnimg.cn/20200121101824654.png" alt=""></p><p>在命令行使用Javac命令，生成.class文件</p><p><img src="https://img-blog.csdnimg.cn/2020012110192682.png" alt=""></p><p>使用Javap -c命令，生成字节码文件</p><p><img src="https://img-blog.csdnimg.cn/20200121102035583.png" alt=""></p><p><img src="https://img-blog.csdnimg.cn/20200121102059870.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>看到这个乱七八糟的代码，你可能会问，，这tm是什么鬼。</p><p>下面通过<strong>JVM指令手册</strong>从Java底层对字节码进行分析</p><h2 id="字节码分析"><a href="#字节码分析" class="headerlink" title="字节码分析"></a>字节码分析</h2><h3 id="一、computer方法："><a href="#一、computer方法：" class="headerlink" title="一、computer方法："></a>一、computer方法：</h3><p><img src="https://img-blog.csdnimg.cn/20200121103143498.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h4 id="代码-int-a-1"><a href="#代码-int-a-1" class="headerlink" title="==代码 int a=1=="></a>==代码 int a=1==</h4><p>iconst_1:将int型常量①压入操作数栈</p><p>istore_2:将int类型的值存入局部变量①</p><p>结合<strong>JVM虚拟机内存结构图</strong></p><p><img src="https://img-blog.csdnimg.cn/20200121103907248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>首先，给常量a在操作数栈中分配一个内存空间，即<strong>iconst_1</strong>对应<strong>int a</strong></p><p>然后，把常量a的值变成1，并存进局部变量表，即<strong>istore_2</strong>对应<strong>a=1</strong></p><h4 id="代码-int-b-2"><a href="#代码-int-b-2" class="headerlink" title="代码 int b=2"></a>代码 int b=2</h4><p>iconst_2:将int型常量②压入操作数栈</p><p>istore_2:将int类型的值存入局部变量②</p><p>同理，很容易理解</p><h4 id="代码-return-a-b"><a href="#代码-return-a-b" class="headerlink" title="代码 return a+b"></a>代码 return a+b</h4><p>iload_1：从局部变量①中转载int类型值  即<strong>a的值1</strong></p><p>iload_2：从局部变量②中装载int类型值  即<strong>b的值2</strong></p><p>即把变量a的值1给装载出来，放在操作数栈</p><p>把变量b的值2给装载出来，放在操作数栈</p><p><img src="https://img-blog.csdnimg.cn/20200121105426146.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>iadd:执行int类型的加法</p><p>即从操作数栈中依次弹出栈顶元素相加，最终生成的结果压回操作数栈    <strong>a+b</strong></p><p>最后 ireturn:从当前方法返回int</p><p>从操作数栈中弹出3</p><p>即对应 <strong>return 3</strong></p><h3 id="二、main方法"><a href="#二、main方法" class="headerlink" title="二、main方法"></a>二、main方法</h3><p><img src="https://img-blog.csdnimg.cn/20200121110948756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h4 id="代码-Test-test-new-Test"><a href="#代码-Test-test-new-Test" class="headerlink" title="代码 Test test = new Test()"></a>代码 Test test = new Test()</h4><p> new：创建一个对象</p><p> 我们对比上面的 int a 可以知道，new出来的test 也是一个局部变量，它被存放在main方法对应的栈帧内存区的局部变量表中，<br> 但在jvm底层，对象创建之后放在堆中，</p><p> 这样我们就可以发现栈和堆之间的一个联系。</p><p> <strong>那么，这两个东西真的就是一样的嘛？</strong></p><p> 其实，局部变量表中存放的是堆中对象对应的内存地址，即可以理解为它存放一个指向堆中对象的指针。</p><p> <img src="https://img-blog.csdnimg.cn/20200121112345695.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p> 到这里，我们就可以通过字节码在jvm底层结构理解整个Java代码的执行过程。</p><p> 最后还有一个问题</p><p><img src="https://img-blog.csdnimg.cn/20200121112738874.png" alt="">)<img src="https://img-blog.csdnimg.cn/20200121112648124.png" alt="在这里插入图片描述"></p><p>在执行main方法时，会跳出去转到computer方法中，在执行完computer方法后，它会再回到main方法中，但是，它回到哪了呢？</p><p>方法出口就记录着返回的位置。</p><p>同样，程序计数器？</p><p><img src="https://img-blog.csdnimg.cn/20200121113147728.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>它记录着程序执行的位置，即行数</p><p>设想，在Java代码执行时，经常会有多个线程。学过操作系统的就知道，cpu在运行时，经常会发生线程被抢占，被执行的线程挂起。那么，这个线程被挂起之后，它重新运行时，从哪开始呢？</p><p>程序计数器就解决了这个问题。</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> 字节码执行引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JVM结构 </tag>
            
            <tag> 字节码执行引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>运行时栈帧结构</title>
      <link href="/2020/01/28/JVM/JVM%E5%AD%97%E8%8A%82%E7%A0%81%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E1%E2%80%94%E2%80%94%E8%BF%90%E8%A1%8C%E6%97%B6%E6%A0%88%E5%B8%A7%E7%BB%93%E6%9E%84/"/>
      <url>/2020/01/28/JVM/JVM%E5%AD%97%E8%8A%82%E7%A0%81%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E1%E2%80%94%E2%80%94%E8%BF%90%E8%A1%8C%E6%97%B6%E6%A0%88%E5%B8%A7%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>栈帧中包括：局部变量表、操作数栈、动态链接、方法出口。</p><p><img src="https://img-blog.csdnimg.cn/20200126175647510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h2 id="动态连接"><a href="#动态连接" class="headerlink" title="动态连接"></a>动态连接</h2><p>在<a href="https://blog.csdn.net/qq_44357371/article/details/104072447" target="_blank" rel="noopener">类加载的过程</a>中已介绍过类加载过程中的解析阶段，是将符号引用转换为直接引用，但是是静态的。<br>在与运行期间转化为直接引用就是动态连接。</p><p>其他部分可参考阅读</p><p><a href="https://blog.csdn.net/qq_44357371/article/details/103330641" target="_blank" rel="noopener">JVM底层结构</a></p><p><a href="https://blog.csdn.net/qq_44357371/article/details/104059180" target="_blank" rel="noopener">通过Java字节码深入理解Java执行过程及JVM底层结构</a></p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> 字节码执行引擎 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 字节码执行引擎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>类加载器</title>
      <link href="/2020/01/28/JVM/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B63%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/"/>
      <url>/2020/01/28/JVM/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B63%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<blockquote><p>类加载器：实现 “ 通过类的全限定名来获取描述此类的二进制字节流 ” 的模块<br><img src="https://img-blog.csdnimg.cn/20200126141937329.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p></blockquote><h2 id="类加载器种类："><a href="#类加载器种类：" class="headerlink" title="类加载器种类："></a>类加载器种类：</h2><ul><li><strong>启动类加载器</strong>：负责加载支撑JVM运行的位于jre/lib目录下的核心类库（例如：String、Object类），在虚拟机启动时就会加载完，以支撑虚拟机的运行。对于hotspot，这个类加载器使用C++实现。</li><li><strong>扩展类加载器</strong>：负责加载支撑JVM运行的位于jre/lib/ext中的JAR包。由Java语言实现，父类加载器为null。</li><li><strong>应用程序类加载器</strong>：负责加载用户路径ClassPath下的类库。由Java语言实现，父类加载器为ExtClassLoader。</li></ul><p>类加载器加载Class大致要经过如下8个步骤：</p><ol><li>检测此Class是否载入过，即在缓冲区中是否有此Class，如果有直接进入第8步，否则进入第2步。</li><li>如果没有父类加载器，则要么Parent是根类加载器，要么本身就是根类加载器，则跳到第4步，如果父类加载器存在，则进入第3步。</li><li>请求使用父类加载器去载入目标类，如果载入成功则跳至第8步，否则接着执行第5步。</li><li>请求使用根类加载器去载入目标类，如果载入成功则跳至第8步，否则跳至第7步。</li><li>当前类加载器尝试寻找Class文件，如果找到则执行第6步，如果找不到则执行第7步。</li><li>从文件中载入Class，成功后跳至第8步。</li><li>抛出ClassNotFountException异常。</li><li>返回对应的java.lang.Class对象。</li></ol><h2 id="类加载机制："><a href="#类加载机制：" class="headerlink" title="类加载机制："></a>类加载机制：</h2><ul><li>全盘负责：所谓全盘负责，就是当一个类加载器负责加载某个Class时，该Class所依赖和引用其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入。</li><li>双亲委派：所谓的双亲委派，则是先让父类加载器试图加载该Class，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类。通俗的讲，就是某个特定的类加载器在接到加载类的请求时，首先将加载任务委托给父加载器，依次递归，如果父加载器可以完成类加载任务，就成功返回；只有父加载器无法完成此加载任务时，才自己去加载。</li><li>缓存机制。缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区中搜寻该Class，只有当缓存区中不存在该Class对象时，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓冲区中。这就是为很么修改了Class后，必须重新启动JVM，程序所做的修改才会生效的原因。</li></ul><p><strong>双亲委派机制</strong></p><p><img src="https://img-blog.csdnimg.cn/2020012615383646.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70#pic_center=300x300" alt=""></p><p>双亲委派机制要求除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器，但这里的父子关系是组合关系</p><blockquote><p>组合关系，是关联关系的一种，是比聚合关系强的关系。它要求普通的聚合关系中代表整体的对象负责代表部分对象的生命周期，组合关系是不能共享的。代表整体的对象需要负责保持部分对象和存活，在一些情况下将负责代表部分的对象湮灭掉。代表整体的对象可以将代表部分的对象传递给另一个对象，由后者负责此对象的生命周期。换言之，代表部分的对象在每一个时刻只能与一个对象发生组合关系，由后者排他地负责生命周期。部分和整体的生命周期一样。</p></blockquote><p>对于我们写出来的.class文件，即对应<strong>应用程序加载器</strong>，它在加载时，首先会向上委托，委托给他的父亲，即<strong>扩展类加载器</strong>，扩展类加载器继续向上委托，给<strong>启动类加载器</strong>，而启动类加载器没有父类，则在启动类加载器查找是否有这个类，有则加载，无则打回；然后在扩展类加载器中找，有则加载，无则打回；最后在应用程序类加载器中加载。</p><p>  <strong>双亲委派机制的优势</strong>：采用双亲委派模式的是好处是Java类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层级关可以避免类的重复加载，当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次。其次是考虑到安全因素，java核心api中定义类型不会被随意替换，假设通过网络传递一个名为java.lang.Integer的类，通过双亲委托模式传递到启动类加载器，而启动类加载器在核心Java API发现这个名字的类，发现该类已被加载，并不会重新加载网络传递的过来的java.lang.Integer，而直接返回已加载过的Integer.class，这样便可以防止核心API库被随意篡改。</p><p>我们以一个简单的代码为例：</p><p><img src="https://img-blog.csdnimg.cn/20200126151932356.png" alt=""></p><p>我们自己定义一个String类，注意==在java.lang包中也有一个系统自带的String类==，而我们定义的这个类，包名也叫做java.lang，运行结果怎么样呢 ？</p><p><img src="https://img-blog.csdnimg.cn/20200126152208405.png" alt=""></p><p>为啥会找不到main方法？<br>这里就要从双亲委托机制进行解释：我们定义的这个类在加载时首先是对应应用程序类加载器，它要向上委托，直到启动类加载器。</p><p>上面已经说过，启动类加载器加载的是位于jre/lib目录下的核心类库，其中就有String类，这个String类就在java.lang 中，所以我们自定义的这个String类在最上层的启动类加载器中被核心类库里的String类替换掉了！</p><p>参考文章：<a href="https://blog.csdn.net/m0_38075425/article/details/81627349" target="_blank" rel="noopener">https://blog.csdn.net/m0_38075425/article/details/81627349</a></p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> 类加载机制 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 类加载机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>类加载的过程</title>
      <link href="/2020/01/28/JVM/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B62%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E8%BF%87%E7%A8%8B/"/>
      <url>/2020/01/28/JVM/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B62%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200122204152483.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><h2 id="加载"><a href="#加载" class="headerlink" title="加载"></a>加载</h2><ul><li>将类的.class文件中的二进制数据读入到内存中</li><li>将其放在运行时数据区的方法区内</li><li>然后再内存中创建一个java.lang.Class对象用来封装类在方法区内的数据结构</li></ul><p>简单地说，加载：在硬盘上查找并通过IO读入字节码文件</p><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>目的：确保Class文件字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。<br>主要分为四个检验部分：</p><ol><li>文件格式验证：检验是否为class文件</li><li>元数据验证：验证是否符合Java语言规范</li><li>字节码验证：检验程序是否合法，符合逻辑</li><li>符号引用验证：保证该引用能够被访问到</li></ol><p>要注意，验证阶段是非常重要的，但并不是一定必要的，对于反复使用和验证过的代码，可以通过使用-Xverify：none来关闭大部分类验证，来缩短验证时间。</p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>为类变量分配内存，设置类变量初始值</p><p>要注意的是：</p><ul><li>内存分配仅包括类变量（static），不包括实例变量，实例变量会在对象实例化时随对象分配在Java堆中</li><li>初始值在通常情况下是0，并不是程序里的那个值。真正的赋值是在初始化时完成的。<h2 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h2>将<strong>符号引用</strong>替换为<strong>直接引用</strong>，该阶段会把一些<strong>静态方法</strong>替换为指向数据所存内存的指针或句柄等（<strong>直接引用</strong>），这是所谓的<strong>静态链接</strong>的过程（程序加载期间完成），<strong>动态链接</strong>时在程序运行期间完成的<strong>符号引用</strong>替换为<strong>直接引用</strong>。</li></ul><p><strong>符号引用</strong>：用符号来表示目标，符号可以是任意的字面量，要求无歧义</p><p>我们以字节码文档为例：</p><p>将一个简单的代码用javap命令生成字节码文件，内容如下</p><pre><code>package Test;public class Test {    public int computer(){        int a=1;        int b=2;        return a+b;    }    public static void main(String[] args) {        Test test = new Test();        int c = test.computer();        System.out.println(c);    }}</code></pre><p>```</p><pre><code>Classfile /C:/Users/董润泽/workspace/Java虚拟机/src/Test/Test.class  Last modified 2020-1-21; size 490 bytes  MD5 checksum 1015aa0bd5fd4af85cc23a21ef42a1af  Compiled from &quot;Test.java&quot;public class Test.Test  minor version: 0  major version: 52  flags: ACC_PUBLIC, ACC_SUPERConstant pool:   #1 = Methodref          #7.#18         // java/lang/Object.&quot;&lt;init&gt;&quot;:()V   #2 = Class              #19            // Test/Test   #3 = Methodref          #2.#18         // Test/Test.&quot;&lt;init&gt;&quot;:()V   #4 = Methodref          #2.#20         // Test/Test.computer:()I   #5 = Fieldref           #21.#22        // java/lang/System.out:Ljava/io/PrintStream;   #6 = Methodref          #23.#24        // java/io/PrintStream.println:(I)V   #7 = Class              #25            // java/lang/Object   #8 = Utf8               &lt;init&gt;   #9 = Utf8               ()V  #10 = Utf8               Code  #11 = Utf8               LineNumberTable  #12 = Utf8               computer  #13 = Utf8               ()I  #14 = Utf8               main  #15 = Utf8               ([Ljava/lang/String;)V  #16 = Utf8               SourceFile  #17 = Utf8               Test.java  #18 = NameAndType        #8:#9          // &quot;&lt;init&gt;&quot;:()V  #19 = Utf8               Test/Test  #20 = NameAndType        #12:#13        // computer:()I  #21 = Class              #26            // java/lang/System  #22 = NameAndType        #27:#28        // out:Ljava/io/PrintStream;  #23 = Class              #29            // java/io/PrintStream  #24 = NameAndType        #30:#31        // println:(I)V  #25 = Utf8               java/lang/Object  #26 = Utf8               java/lang/System  #27 = Utf8               out  #28 = Utf8               Ljava/io/PrintStream;  #29 = Utf8               java/io/PrintStream  #30 = Utf8               println  #31 = Utf8               (I)V{  public Test.Test();    descriptor: ()V    flags: ACC_PUBLIC    Code:      stack=1, locals=1, args_size=1         0: aload_0         1: invokespecial #1                  // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V         4: return      LineNumberTable:        line 3: 0  public int computer();    descriptor: ()I    flags: ACC_PUBLIC    Code:      stack=2, locals=3, args_size=1         0: iconst_1         1: istore_1         2: iconst_2         3: istore_2         4: iload_1         5: iload_2         6: iadd         7: ireturn      LineNumberTable:        line 5: 0        line 6: 2        line 7: 4  public static void main(java.lang.String[]);    descriptor: ([Ljava/lang/String;)V    flags: ACC_PUBLIC, ACC_STATIC    Code:      stack=2, locals=3, args_size=1         0: new           #2                  // class Test/Test         3: dup         4: invokespecial #3                  // Method &quot;&lt;init&gt;&quot;:()V         7: astore_1         8: aload_1         9: invokevirtual #4                  // Method computer:()I        12: istore_2        13: getstatic     #5                  // Field java/lang/System.out:Ljava/io/PrintStream;        16: iload_2        17: invokevirtual #6                  // Method java/io/PrintStream.println:(I)V        20: return      LineNumberTable:        line 10: 0        line 11: 8        line 12: 13        line 13: 20}SourceFile: &quot;Test.java&quot;</code></pre><p>可以看到，上面有一些“#数字”：</p><p><img src="https://img-blog.csdnimg.cn/20200122220623936.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ0MzU3Mzcx,size_16,color_FFFFFF,t_70" alt=""></p><p>这其实就是生成的符号，对于后面的解释，例如“#7：#8”即指“#1”这个符号引用了“#7”和“#8”的内容</p><p>这即为符号引用</p><p><strong>直接引用</strong>：引用的是目标的一个地址</p><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><p>对类的静态变量初始化为指定的值，执行静态代码块</p><p>同准备阶段的赋初始值不同！</p><p>在博客 <a href="https://drunze.github.io/2020/01/28/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B61%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E6%97%B6%E6%9C%BA/" target="_blank" rel="noopener">类加载的时机</a>已介绍，可参考理解。</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> 类加载机制 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 类加载机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>类加载的时机</title>
      <link href="/2020/01/28/JVM/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B61%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E6%97%B6%E6%9C%BA/"/>
      <url>/2020/01/28/JVM/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B61%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E6%97%B6%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p>虚拟机如何加载Class文件？<br>Class文件里的信息进入虚拟机会发生怎样的变化？</p><blockquote><p>虚拟机把描述类的数据从class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是类加载机制</p></blockquote><h2 id="类加载的时机"><a href="#类加载的时机" class="headerlink" title="类加载的时机"></a>类加载的时机</h2><p>类从被加载到内存到卸载出内存，生命周期：<br><strong>加载、连接</strong>（验证、准备、解析）<strong>、初始化、使用、卸载</strong></p><p><strong>初始化：</strong></p><ol><li>遇到new、getstatic、putstatic或invokestatic,若类未初始化，则触发初始化</li><li>使用java.lang.reflect对类进行反射调用</li><li>父类没有进行过初始化，先初始化父类</li><li>虚拟机启动时，先初始化要执行的主类</li><li>若java.lang.invoke.MethodHandle实例最后的解析结果为REF_getStatic、REF_putStatic、REF_invokeStatic的句柄，若此方法句柄对应的类没有进行过初始化，则要先初始化。</li></ol><p>下面以代码来演示</p><pre><code>//父类superClass    public class SuperClass {        static {            System.out.println(&quot;SuperClass init!&quot;);        }    public static int sup = 1111111;    }//子类subClass    public class subClass extends SuperClass{        static {            System.out.println(&quot;SubClass init!&quot;);        }        public static int sub = 2222222;    }</code></pre><p><img src="https://img-blog.csdnimg.cn/20200122152246616.png" alt=""></p><p>guess结果是啥？</p><p>分析，参考上面规则的第三条： <strong>3. 父类没有进行过初始化，先初始化父类</strong></p><p>首先是subClass.sub:要调用子类中的静态变量sub，那就要初始化子类，但是在子类初始化之前，还要去先初始化父类，最后才打印sub的值</p><p>在运行第二句时，两个类已经经过初始化，那么直接打印值即可。</p><p><img src="https://img-blog.csdnimg.cn/20200122152617903.png" alt=""></p><pre><code>public static void main(String[] args) {    SuperClass[] sca = new SuperClass[10];}</code></pre><p>这样，并没有输出superClass init！<br>它代表了元素类型为superClass的一维数组，它是继承于java.lang.Object的子类</p><pre><code>public class ConstClass {    static{        System.out.println(&quot;ConstClass init!&quot;);    }        public static final String Hello = &quot;Hello!&quot;;}public class NotInitialization {    public static void main(String[] args) {        System.out.println(ConstClass.Hello);    }}</code></pre><p>输出什么？</p><p><img src="https://img-blog.csdnimg.cn/20200122185027629.png" alt=""></p><p>Hello！!</p><p>为啥没有执行static里面的语句呢？？？</p><p>这里用了final，即Hello是常量，它被存放在常量池中，和ConstClass这个类没有了联系，主函数在执行时，不需要对ConstClass类进行初始化，所以最终只打印“ Hello！”</p>]]></content>
      
      
      <categories>
          
          <category> JVM </category>
          
          <category> 类加载机制 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 类加载机制 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
